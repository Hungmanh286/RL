{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "774916ab",
   "metadata": {
    "id": "KKkRMmTdoBwm",
    "papermill": {
     "duration": 0.005529,
     "end_time": "2025-05-17T17:48:12.119043",
     "exception": false,
     "start_time": "2025-05-17T17:48:12.113514",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1dbba69b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T17:48:12.129405Z",
     "iopub.status.busy": "2025-05-17T17:48:12.129128Z",
     "iopub.status.idle": "2025-05-17T17:48:12.929715Z",
     "shell.execute_reply": "2025-05-17T17:48:12.928820Z"
    },
    "id": "XCzA2o7AoBwo",
    "outputId": "4d1c2e1d-9eed-4f87-ffe6-d190db0ad095",
    "papermill": {
     "duration": 0.807396,
     "end_time": "2025-05-17T17:48:12.931062",
     "exception": false,
     "start_time": "2025-05-17T17:48:12.123666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hungmanh/home_work/RL/marl-delivery\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/cuongtv312/marl-delivery.git marl_delivery\n",
    "%cd /home/hungmanh/home_work/RL/marl-delivery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9cfa87e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T17:48:12.941366Z",
     "iopub.status.busy": "2025-05-17T17:48:12.941135Z",
     "iopub.status.idle": "2025-05-17T17:48:19.026767Z",
     "shell.execute_reply": "2025-05-17T17:48:19.025913Z"
    },
    "id": "309nvG-V8Otr",
    "papermill": {
     "duration": 6.092405,
     "end_time": "2025-05-17T17:48:19.028286",
     "exception": false,
     "start_time": "2025-05-17T17:48:12.935881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from env import Environment\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from sklearn.calibration import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 2025\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2339d8cb",
   "metadata": {
    "id": "uvs9SYSkoBwq",
    "papermill": {
     "duration": 0.004486,
     "end_time": "2025-05-17T17:48:19.037716",
     "exception": false,
     "start_time": "2025-05-17T17:48:19.033230",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Convert map_state and robot_package_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "23fb464b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T17:48:19.047750Z",
     "iopub.status.busy": "2025-05-17T17:48:19.047440Z",
     "iopub.status.idle": "2025-05-17T17:48:19.056875Z",
     "shell.execute_reply": "2025-05-17T17:48:19.056305Z"
    },
    "id": "rq1hlk4b8Q37",
    "papermill": {
     "duration": 0.015826,
     "end_time": "2025-05-17T17:48:19.057893",
     "exception": false,
     "start_time": "2025-05-17T17:48:19.042067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_map(env_state_dict, persistent_packages_for_env, current_robot_idx):\n",
    "    \"\"\"\n",
    "    Create map sate for robot\n",
    "    \"\"\"\n",
    "    num_channels = 6\n",
    "    grid = np.array(env_state_dict['map'], dtype=np.float32)\n",
    "    n_rows, n_cols = grid.shape\n",
    "    obs = np.zeros((num_channels, n_rows, n_cols), dtype=np.float32)\n",
    "\n",
    "    # Channel 0 : map\n",
    "    obs[0] = grid\n",
    "\n",
    "    if not (0 <= current_robot_idx < len(env_state_dict['robots'])):\n",
    "        return obs\n",
    "\n",
    "    robots = env_state_dict['robots']\n",
    "    my_r, my_c, my_pkg = [int(x) for x in robots[current_robot_idx]]\n",
    "    my_r -= 1; my_c -= 1  # 0-indexed\n",
    "\n",
    "    # Channel 1: Current position robot\n",
    "    if 0 <= my_r < n_rows and 0 <= my_c < n_cols:\n",
    "        obs[1, my_r, my_c] = 1.0\n",
    "\n",
    "    # Channel 2: position other robot\n",
    "    for i, (r, c, _) in enumerate(robots):\n",
    "        if i == current_robot_idx: continue\n",
    "        r, c = int(r)-1, int(c)-1\n",
    "        if 0 <= r < n_rows and 0 <= c < n_cols:\n",
    "            obs[2, r, c] = 1.0\n",
    "\n",
    "    # Channel 3, 4, 5:  package information\n",
    "    t = env_state_dict['time_step']\n",
    "    for pkg_id, pkg in persistent_packages_for_env.items():\n",
    "        # Channel 3: Start pos of package 'waiting'\n",
    "        if pkg['status'] == 'waiting' and pkg['start_time'] <= t:\n",
    "            sr, sc = pkg['start_pos']\n",
    "            if 0 <= sr < n_rows and 0 <= sc < n_cols:\n",
    "                obs[3, sr, sc] = 1.0\n",
    "        # Channel 4: Target pos of package 'active'\n",
    "        if (pkg['status'] == 'waiting' and pkg['start_time'] <= t) or pkg['status'] == 'in_transit':\n",
    "            tr, tc = pkg['target_pos']\n",
    "            if 0 <= tr < n_rows and 0 <= tc < n_cols:\n",
    "                obs[4, tr, tc] = 1.0\n",
    "\n",
    "    # Channel 5: target position of the robot package holding\n",
    "    if my_pkg != 0 and my_pkg in persistent_packages_for_env:\n",
    "        pkg = persistent_packages_for_env[my_pkg]\n",
    "        if pkg['status'] == 'in_transit':\n",
    "            tr, tc = pkg['target_pos']\n",
    "            if 0 <= tr < n_rows and 0 <= tc < n_cols:\n",
    "                obs[5, tr, tc] = 1.0\n",
    "\n",
    "    return obs\n",
    "def convert_features(env_state_dict, persistent_packages_for_env, current_robot_idx,\n",
    "                            max_time_steps,\n",
    "                            max_other_robots_to_observe=100, max_packages_to_observe=100):\n",
    "    \"\"\"\n",
    "    State contains relevant information about the robot and the order\n",
    "    \"\"\"\n",
    "    n_rows, n_cols = np.array(env_state_dict['map'], dtype=np.float32).shape\n",
    "    robots = env_state_dict['robots']\n",
    "    t = env_state_dict['time_step']\n",
    "\n",
    "    my_feat = 6\n",
    "    other_feat = 5\n",
    "    pkg_feat = 5\n",
    "    time_feat = 1\n",
    "\n",
    "    total_len = my_feat + max_other_robots_to_observe * other_feat + max_packages_to_observe * pkg_feat + time_feat\n",
    "    if not (0 <= current_robot_idx < len(robots)):\n",
    "        return np.zeros(total_len, dtype=np.float32)\n",
    "\n",
    "    # 1. information about robots\n",
    "    my_r, my_c, my_pkg = [int(x) for x in robots[current_robot_idx]]\n",
    "    my_r -= 1; my_c -= 1\n",
    "    is_carrying = 1.0 if my_pkg != 0 else 0.0\n",
    "    feat = [\n",
    "        my_r / n_rows,\n",
    "        my_c / n_cols,\n",
    "        is_carrying\n",
    "    ]\n",
    "    # if hold package\n",
    "    if is_carrying and my_pkg in persistent_packages_for_env:\n",
    "        pkg = persistent_packages_for_env[my_pkg]\n",
    "        if pkg['status'] == 'in_transit':\n",
    "            tr, tc = pkg['target_pos']\n",
    "            deadline = pkg['deadline']\n",
    "            feat += [\n",
    "                (tr - my_r) / n_rows,\n",
    "                (tc - my_c) / n_cols,\n",
    "                max(0, deadline - t) / max_time_steps if max_time_steps > 0 else 0.0\n",
    "            ]\n",
    "        else:\n",
    "            feat += [0.0, 0.0, 0.0]\n",
    "    else:\n",
    "        feat += [0.0, 0.0, 0.0]\n",
    "\n",
    "    # 2. information other robots\n",
    "    others = []\n",
    "    for i, (r, c, pkg_id) in enumerate(robots):\n",
    "        if i == current_robot_idx: continue\n",
    "        r, c, pkg_id = int(r)-1, int(c)-1, int(pkg_id)\n",
    "        is_c = 1.0 if pkg_id != 0 else 0.0\n",
    "        other = [\n",
    "            (r - my_r) / n_rows,\n",
    "            (c - my_c) / n_cols,\n",
    "            is_c\n",
    "        ]\n",
    "        if is_c and pkg_id in persistent_packages_for_env:\n",
    "            pkg = persistent_packages_for_env[pkg_id]\n",
    "            if pkg['status'] == 'in_transit':\n",
    "                tr, tc = pkg['target_pos']\n",
    "                other += [\n",
    "                    (tr - r) / n_rows,\n",
    "                    (tc - c) / n_cols\n",
    "                ]\n",
    "            else:\n",
    "                other += [0.0, 0.0]\n",
    "        else:\n",
    "            other += [0.0, 0.0]\n",
    "        others.append(other)\n",
    "    \n",
    "    # sort distance (from robot to other robots)\n",
    "    others.sort(key=lambda x: x[0]**2 + x[1]**2)\n",
    "    for i in range(max_other_robots_to_observe):\n",
    "        feat += others[i] if i < len(others) else [0.0]*other_feat\n",
    "\n",
    "    # 3. info package 'waiting'\n",
    "    pkgs = []\n",
    "    for pkg_id, pkg in persistent_packages_for_env.items():\n",
    "        if pkg['status'] == 'waiting' and pkg['start_time'] <= t:\n",
    "            sr, sc = pkg['start_pos']\n",
    "            tr, tc = pkg['target_pos']\n",
    "            deadline = pkg['deadline']\n",
    "            pkgs.append([\n",
    "                (sr - my_r) / n_rows,\n",
    "                (sc - my_c) / n_cols,\n",
    "                (tr - my_r) / n_rows,\n",
    "                (tc - my_c) / n_cols,\n",
    "                max(0, deadline - t) / max_time_steps if max_time_steps > 0 else 0.0\n",
    "            ])\n",
    "\n",
    "    # sort for deadline and distance\n",
    "    pkgs.sort(key=lambda x: (x[4], x[0]**2 + x[1]**2))\n",
    "    for i in range(max_packages_to_observe):\n",
    "        feat += pkgs[i] if i < len(pkgs) else [0.0]*pkg_feat\n",
    "\n",
    "    # 4. normalize time_step\n",
    "    feat.append(t / max_time_steps if max_time_steps > 0 else 0.0)\n",
    "\n",
    "    return np.array(feat, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa1d465",
   "metadata": {
    "id": "72OfNiZEoBws",
    "papermill": {
     "duration": 0.004137,
     "end_time": "2025-05-17T17:48:19.096425",
     "exception": false,
     "start_time": "2025-05-17T17:48:19.092288",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Convert State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "029b04bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T17:48:19.106481Z",
     "iopub.status.busy": "2025-05-17T17:48:19.106054Z",
     "iopub.status.idle": "2025-05-17T17:48:19.117243Z",
     "shell.execute_reply": "2025-05-17T17:48:19.116716Z"
    },
    "id": "eZyKJS7_oBws",
    "papermill": {
     "duration": 0.017367,
     "end_time": "2025-05-17T17:48:19.118213",
     "exception": false,
     "start_time": "2025-05-17T17:48:19.100846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_global_state(env_state_dict, persistent_packages_for_env,\n",
    "                                max_time_steps,\n",
    "                                max_robots_in_state=100, max_packages_in_state=100):\n",
    "    \"\"\"\n",
    "    Sinh global state (spatial + vector) cho Critic.\n",
    "    \"\"\"\n",
    "    # --- Spatial ---\n",
    "    num_map_channels = 4\n",
    "    n_rows, n_cols = np.array(env_state_dict['map'], dtype=np.float32).shape\n",
    "\n",
    "    global_map = np.zeros((num_map_channels, n_rows, n_cols), dtype=np.float32)\n",
    "    global_map[0] = np.array(env_state_dict['map'], dtype=np.float32)  # Obstacles\n",
    "\n",
    "    # Channel 1: All robot positions\n",
    "    for r, c, _ in env_state_dict['robots']:\n",
    "        r0, c0 = int(r)-1, int(c)-1\n",
    "        if 0 <= r0 < n_rows and 0 <= c0 < n_cols:\n",
    "            global_map[1, r0, c0] = 1.0\n",
    "\n",
    "    t = env_state_dict['time_step']\n",
    "    for pkg in persistent_packages_for_env.values():\n",
    "        # Channel 2: waiting package start\n",
    "        if pkg['status'] == 'waiting' and pkg['start_time'] <= t:\n",
    "            sr, sc = pkg['start_pos']\n",
    "            if 0 <= sr < n_rows and 0 <= sc < n_cols:\n",
    "                global_map[2, sr, sc] = 1.0\n",
    "        # Channel 3: active package target\n",
    "        if (pkg['status'] == 'waiting' and pkg['start_time'] <= t) or pkg['status'] == 'in_transit':\n",
    "            tr, tc = pkg['target_pos']\n",
    "            if 0 <= tr < n_rows and 0 <= tc < n_cols:\n",
    "                global_map[3, tr, tc] = 1.0\n",
    "\n",
    "    # --- Vector ---\n",
    "    vec = []\n",
    "    # 1. Robots (padded)\n",
    "    for i in range(max_robots_in_state):\n",
    "        if i < len(env_state_dict['robots']):\n",
    "            r, c, carried = env_state_dict['robots'][i]\n",
    "            r0, c0 = int(r)-1, int(c)-1\n",
    "            is_carrying = 1.0 if carried != 0 else 0.0\n",
    "            vec += [r0/n_rows, c0/n_cols, is_carrying]\n",
    "            if is_carrying and carried in persistent_packages_for_env:\n",
    "                pkg = persistent_packages_for_env[carried]\n",
    "                if pkg['status'] == 'in_transit':\n",
    "                    tr, tc = pkg['target_pos']\n",
    "                    deadline = pkg['deadline']\n",
    "                    vec += [tr/n_rows, tc/n_cols, max(0, deadline-t)/max_time_steps if max_time_steps > 0 else 0.0]\n",
    "                else:\n",
    "                    vec += [0.0, 0.0, 0.0]\n",
    "            else:\n",
    "                vec += [0.0, 0.0, 0.0]\n",
    "        else:\n",
    "            vec += [0.0]*6\n",
    "\n",
    "    # 2. Active packages (padded)\n",
    "    pkgs = []\n",
    "    for pkg in persistent_packages_for_env.values():\n",
    "        is_active = (pkg['status'] == 'waiting' and pkg['start_time'] <= t) or pkg['status'] == 'in_transit'\n",
    "        if is_active:\n",
    "            pkgs.append(pkg)\n",
    "    pkgs = sorted(pkgs, key=lambda p: p['id'])\n",
    "    for i in range(max_packages_in_state):\n",
    "        if i < len(pkgs):\n",
    "            pkg = pkgs[i]\n",
    "            sr, sc = pkg['start_pos']\n",
    "            tr, tc = pkg['target_pos']\n",
    "            deadline = pkg['deadline']\n",
    "            status = pkg['status']\n",
    "            # start pos (nếu waiting), target pos, deadline, status, carrier_id_norm\n",
    "            if status == 'waiting':\n",
    "                vec += [sr/n_rows, sc/n_cols]\n",
    "            else:\n",
    "                vec += [0.0, 0.0]\n",
    "            vec += [tr/n_rows, tc/n_cols]\n",
    "            vec += [max(0, deadline-t)/max_time_steps if max_time_steps > 0 else 0.0]\n",
    "            vec += [0.0 if status == 'waiting' else 1.0]\n",
    "            carrier_id_norm = -1.0\n",
    "            if status == 'in_transit':\n",
    "                for ridx, rdata in enumerate(env_state_dict['robots']):\n",
    "                    if rdata[2] == pkg['id']:\n",
    "                        carrier_id_norm = ridx/(max_robots_in_state-1) if max_robots_in_state > 1 else 0.0\n",
    "                        break\n",
    "            vec += [carrier_id_norm]\n",
    "        else:\n",
    "            vec += [0.0]*7\n",
    "\n",
    "    # 3. Global time\n",
    "    vec.append(t/max_time_steps if max_time_steps > 0 else 0.0)\n",
    "    return global_map, np.array(vec, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ab5c90",
   "metadata": {
    "id": "fck_AprjoBwt",
    "papermill": {
     "duration": 0.009053,
     "end_time": "2025-05-17T17:48:22.011237",
     "exception": false,
     "start_time": "2025-05-17T17:48:22.002184",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "27d449e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T17:48:22.028825Z",
     "iopub.status.busy": "2025-05-17T17:48:22.028561Z",
     "iopub.status.idle": "2025-05-17T17:48:22.033155Z",
     "shell.execute_reply": "2025-05-17T17:48:22.032642Z"
    },
    "id": "s73yD1l4oBwu",
    "papermill": {
     "duration": 0.014667,
     "end_time": "2025-05-17T17:48:22.034175",
     "exception": false,
     "start_time": "2025-05-17T17:48:22.019508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- MAPPO Hyperparameters ---\n",
    "ACTION_DIM = 15  # Total discrete actions for an agent\n",
    "NUM_AGENTS = 5\n",
    "MAP_FILE = \"map1.txt\"\n",
    "N_PACKAGES = 50\n",
    "MOVE_COST = -0.01 # Adjusted for PPO, rewards should be reasonably scaled\n",
    "DELIVERY_REWARD = 10\n",
    "DELAY_REWARD = 1 # Or 0, depending on reward shaping strategy\n",
    "MAX_TIME_STEPS_PER_EPISODE = 500 # Max steps for one episode in one env\n",
    "\n",
    "NUM_ENVS = 5  # Number of parallel environments\n",
    "ROLLOUT_STEPS = 500 # Number of steps to collect data for before an update\n",
    "TOTAL_TIMESTEPS = 1_000_000 # Total timesteps for training\n",
    "\n",
    "# PPO specific\n",
    "LR_ACTOR = 1e-5\n",
    "LR_CRITIC = 1e-5\n",
    "GAMMA = 0.99\n",
    "GAE_LAMBDA = 0.95\n",
    "CLIP_EPS = 0.2\n",
    "NUM_EPOCHS = 5 # Number of epochs to train on collected data\n",
    "MINIBATCH_SIZE = 64 # Minibatch size for PPO updates\n",
    "ENTROPY_COEF = 0.01\n",
    "VALUE_LOSS_COEF = 0.5\n",
    "MAX_GRAD_NORM = 0.5\n",
    "WEIGHT_DECAY = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2d6ccc",
   "metadata": {
    "id": "wpR6HleKoBwu",
    "papermill": {
     "duration": 0.008558,
     "end_time": "2025-05-17T17:48:22.051565",
     "exception": false,
     "start_time": "2025-05-17T17:48:22.043007",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Actor Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "62fc851c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T17:48:22.070444Z",
     "iopub.status.busy": "2025-05-17T17:48:22.070233Z",
     "iopub.status.idle": "2025-05-17T17:48:22.084068Z",
     "shell.execute_reply": "2025-05-17T17:48:22.083522Z"
    },
    "id": "mRcpMBMWoBwu",
    "papermill": {
     "duration": 0.024433,
     "end_time": "2025-05-17T17:48:22.085070",
     "exception": false,
     "start_time": "2025-05-17T17:48:22.060637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, spatial_obs_shape, vector_obs_dim, action_dim = ACTION_DIM,\n",
    "                 cnn_channels_out=64, mlp_hidden_dim=256, combined_hidden_dim=256):\n",
    "        \"\"\"\n",
    "        Actor Network that processes both spatial and vector observations.\n",
    "\n",
    "        Args:\n",
    "            spatial_obs_shape (tuple): Shape of the spatial observation (C_map, H, W).\n",
    "                                       e.g., (6, n_rows, n_cols) from convert_observation.\n",
    "            vector_obs_dim (int): Dimension of the vector observation.\n",
    "                                  e.g., output size of generate_vector_features.\n",
    "            action_dim (int): Total number of discrete actions.\n",
    "            cnn_channels_out (int): Number of output channels from the last CNN layer.\n",
    "            mlp_hidden_dim (int): Hidden dimension for the vector processing MLP.\n",
    "            combined_hidden_dim (int): Hidden dimension for the combined MLP.\n",
    "        \"\"\"\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.spatial_channels_in = spatial_obs_shape[0]\n",
    "        self.map_h = spatial_obs_shape[1]\n",
    "        self.map_w = spatial_obs_shape[2]\n",
    "\n",
    "        # --- CNN Branch for Spatial Observations ---\n",
    "        self.conv1 = nn.Conv2d(self.spatial_channels_in, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, cnn_channels_out, kernel_size=3, stride=1, padding=1) # Last conv layer\n",
    "        self.bn3 = nn.BatchNorm2d(cnn_channels_out)\n",
    "\n",
    "        # Adaptive pooling to get a fixed size output from CNN, regardless of map_h, map_w (within reason)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((4, 4))\n",
    "        self.cnn_flattened_dim = cnn_channels_out * 4 * 4 # Output from adaptive_pool\n",
    "\n",
    "        # --- MLP Branch for Vector Observations ---\n",
    "        self.vector_fc1 = nn.Linear(vector_obs_dim, mlp_hidden_dim)\n",
    "        self.vector_fc2 = nn.Linear(mlp_hidden_dim, mlp_hidden_dim // 2) # Reduce dim slightly\n",
    "\n",
    "        # --- Combined MLP ---\n",
    "        # Input to this MLP is the concatenation of CNN output and Vector MLP output\n",
    "        combined_input_dim = self.cnn_flattened_dim + (mlp_hidden_dim // 2)\n",
    "        self.combined_fc1 = nn.Linear(combined_input_dim, combined_hidden_dim)\n",
    "        self.actor_head = nn.Linear(combined_hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, spatial_obs, vector_obs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            spatial_obs (torch.Tensor): (batch_size, C_map, H, W)\n",
    "            vector_obs (torch.Tensor): (batch_size, vector_obs_dim)\n",
    "        Returns:\n",
    "            action_logits (torch.Tensor): (batch_size, action_dim)\n",
    "        \"\"\"\n",
    "        # CNN path\n",
    "        x_spatial = F.relu(self.bn1(self.conv1(spatial_obs)))\n",
    "        x_spatial = F.relu(self.bn2(self.conv2(x_spatial)))\n",
    "        x_spatial = F.relu(self.bn3(self.conv3(x_spatial)))\n",
    "        x_spatial = self.adaptive_pool(x_spatial)\n",
    "        x_spatial_flat = x_spatial.reshape(x_spatial.size(0), -1) # Flatten\n",
    "\n",
    "        # Vector MLP path\n",
    "        x_vector = F.relu(self.vector_fc1(vector_obs))\n",
    "        x_vector_processed = F.relu(self.vector_fc2(x_vector))\n",
    "\n",
    "        # Concatenate processed features\n",
    "        combined_features = torch.cat((x_spatial_flat, x_vector_processed), dim=1)\n",
    "\n",
    "        # Combined MLP path\n",
    "        x_combined = F.relu(self.combined_fc1(combined_features))\n",
    "        action_logits = self.actor_head(x_combined)\n",
    "\n",
    "        return action_logits\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, global_spatial_state_shape, global_vector_state_dim,\n",
    "                 cnn_channels_out=64, mlp_hidden_dim=256, combined_hidden_dim=256):\n",
    "        \"\"\"\n",
    "        Critic Network that processes both global spatial and global vector states.\n",
    "\n",
    "        Args:\n",
    "            global_spatial_state_shape (tuple): Shape of the global spatial state (C_global_map, H, W).\n",
    "                                                e.g., (4, n_rows, n_cols) from get_global_state_for_critic.\n",
    "            global_vector_state_dim (int): Dimension of the global vector state.\n",
    "            cnn_channels_out (int): Number of output channels from the last CNN layer.\n",
    "            mlp_hidden_dim (int): Hidden dimension for the vector processing MLP.\n",
    "            combined_hidden_dim (int): Hidden dimension for the combined MLP.\n",
    "        \"\"\"\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.global_spatial_channels_in = global_spatial_state_shape[0]\n",
    "        self.map_h = global_spatial_state_shape[1]\n",
    "        self.map_w = global_spatial_state_shape[2]\n",
    "\n",
    "        # --- CNN Branch for Global Spatial State ---\n",
    "        self.conv1 = nn.Conv2d(self.global_spatial_channels_in, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, cnn_channels_out, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(cnn_channels_out)\n",
    "\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((4, 4)) # Fixed output size\n",
    "        self.cnn_flattened_dim = cnn_channels_out * 4 * 4\n",
    "\n",
    "        # --- MLP Branch for Global Vector State ---\n",
    "        self.vector_fc1 = nn.Linear(global_vector_state_dim, mlp_hidden_dim)\n",
    "        self.vector_fc2 = nn.Linear(mlp_hidden_dim, mlp_hidden_dim // 2)\n",
    "\n",
    "        # --- Combined MLP ---\n",
    "        combined_input_dim = self.cnn_flattened_dim + (mlp_hidden_dim // 2)\n",
    "        self.combined_fc1 = nn.Linear(combined_input_dim, combined_hidden_dim)\n",
    "        self.critic_head = nn.Linear(combined_hidden_dim, 1) # Outputs a single state value\n",
    "\n",
    "    def forward(self, global_spatial_state, global_vector_state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            global_spatial_state (torch.Tensor): (batch_size, C_global_map, H, W)\n",
    "            global_vector_state (torch.Tensor): (batch_size, global_vector_state_dim)\n",
    "        Returns:\n",
    "            value (torch.Tensor): (batch_size, 1)\n",
    "        \"\"\"\n",
    "        # CNN path\n",
    "        x_spatial = F.relu(self.bn1(self.conv1(global_spatial_state)))\n",
    "        x_spatial = F.relu(self.bn2(self.conv2(x_spatial)))\n",
    "        x_spatial = F.relu(self.bn3(self.conv3(x_spatial)))\n",
    "        x_spatial = self.adaptive_pool(x_spatial)\n",
    "        x_spatial_flat = x_spatial.reshape(x_spatial.size(0), -1)\n",
    "\n",
    "        # Vector MLP path\n",
    "        x_vector = F.relu(self.vector_fc1(global_vector_state))\n",
    "        x_vector_processed = F.relu(self.vector_fc2(x_vector))\n",
    "\n",
    "        # Concatenate processed features\n",
    "        combined_features = torch.cat((x_spatial_flat, x_vector_processed), dim=1)\n",
    "\n",
    "        # Combined MLP path\n",
    "        x_combined = F.relu(self.combined_fc1(combined_features))\n",
    "        value = self.critic_head(x_combined)\n",
    "\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491df744",
   "metadata": {},
   "source": [
    "# Save and Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b34a8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T17:48:22.102915Z",
     "iopub.status.busy": "2025-05-17T17:48:22.102708Z",
     "iopub.status.idle": "2025-05-17T17:48:22.106420Z",
     "shell.execute_reply": "2025-05-17T17:48:22.105902Z"
    },
    "id": "YXJ8cRpyoBwv",
    "papermill": {
     "duration": 0.014164,
     "end_time": "2025-05-17T17:48:22.107753",
     "exception": false,
     "start_time": "2025-05-17T17:48:22.093589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save model into models with : models/mappo_actor, models/mappo_critic \n",
    "def save_mappo_model(actor, critic, path_prefix=\"models/mappo\"):\n",
    "    if not os.path.exists(os.path.dirname(path_prefix)):\n",
    "        os.makedirs(os.path.dirname(path_prefix))\n",
    "    torch.save(actor.state_dict(), f\"{path_prefix}_actor_1.pt\")\n",
    "    torch.save(critic.state_dict(), f\"{path_prefix}_critic_1.pt\")\n",
    "    print(f\"MAPPO models saved with prefix {path_prefix}\")\n",
    "    \n",
    "# load model mappo :models/mappo_actor and models/mappo_critic\n",
    "def load_mappo_model(actor, critic, path_prefix=\"models/mappo\", device=\"cuda\"):\n",
    "    actor_path = f\"{path_prefix}_actor_1.pt\"\n",
    "    critic_path = f\"{path_prefix}_critic_1.pt\"\n",
    "    if os.path.exists(actor_path) and os.path.exists(critic_path):\n",
    "        actor.load_state_dict(torch.load(actor_path, map_location=device, weights_only=True))\n",
    "        critic.load_state_dict(torch.load(critic_path, map_location=device, weights_only=True))\n",
    "        print(f\"MAPPO models loaded from prefix {path_prefix}\")\n",
    "        return True\n",
    "    print(f\"Could not find MAPPO models at prefix {path_prefix}\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c54c15",
   "metadata": {
    "id": "NMawY_r2oBwv",
    "papermill": {
     "duration": 0.008233,
     "end_time": "2025-05-17T17:48:22.147802",
     "exception": false,
     "start_time": "2025-05-17T17:48:22.139569",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Reward Shaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "10f07970",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T17:48:22.165288Z",
     "iopub.status.busy": "2025-05-17T17:48:22.165093Z",
     "iopub.status.idle": "2025-05-17T17:48:22.176469Z",
     "shell.execute_reply": "2025-05-17T17:48:22.176005Z"
    },
    "id": "fCIw3X3DoBwv",
    "papermill": {
     "duration": 0.021603,
     "end_time": "2025-05-17T17:48:22.177582",
     "exception": false,
     "start_time": "2025-05-17T17:48:22.155979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_shaped_rewards(\n",
    "    global_reward,\n",
    "    prev_env_state_dict,\n",
    "    current_env_state_dict,\n",
    "    actions_taken_for_all_agents,\n",
    "    persistent_packages_at_prev_state,\n",
    "    num_agents,\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes shaped rewards for each agent based on transitions and intended actions.\n",
    "    Returns: tổng shaped reward (float), và shaped reward từng agent (np.array)\n",
    "    \"\"\"\n",
    "    # --- Shaping Constants ---\n",
    "    SHAPING_SUCCESSFUL_PICKUP = 5\n",
    "    SHAPING_SUCCESSFUL_DELIVERY_ON_TIME = 200\n",
    "    SHAPING_SUCCESSFUL_DELIVERY_LATE = 20\n",
    "    SHAPING_MOVED_CLOSER_TO_TARGET = 0.02\n",
    "    SHAPING_WASTED_PICKUP_ATTEMPT = 0\n",
    "    SHAPING_WASTED_DROP_ATTEMPT = 0\n",
    "    SHAPING_COLLISION_OR_STUCK = -0.05\n",
    "    SHAPING_IDLE_WITH_AVAILABLE_TASKS = -0.05\n",
    "    SHAPING_MOVED_AWAY_FROM_TARGET = -0.01\n",
    "\n",
    "    shaped_rewards = np.zeros(num_agents, dtype=np.float32)\n",
    "    current_time = int(current_env_state_dict['time_step'])\n",
    "\n",
    "    def manhattan_distance(pos1, pos2):\n",
    "        return abs(pos1[0] - pos2[0]) + abs(pos1[1] - pos2[1])\n",
    "\n",
    "    for agent_idx in range(num_agents):\n",
    "        prev_r, prev_c, prev_pkg = [int(x) for x in prev_env_state_dict['robots'][agent_idx]]\n",
    "        curr_r, curr_c, curr_pkg = [int(x) for x in current_env_state_dict['robots'][agent_idx]]\n",
    "        prev_r -= 1; prev_c -= 1; curr_r -= 1; curr_c -= 1\n",
    "        move_str, pkg_op_str = actions_taken_for_all_agents[agent_idx]\n",
    "        pkg_op = int(pkg_op_str)\n",
    "\n",
    "        # 1. Nhặt/thả thành công\n",
    "        if prev_pkg == 0 and curr_pkg != 0:\n",
    "            shaped_rewards[agent_idx] += SHAPING_SUCCESSFUL_PICKUP\n",
    "        elif prev_pkg != 0 and curr_pkg == 0:\n",
    "            dropped_pkg = prev_pkg\n",
    "            if dropped_pkg in persistent_packages_at_prev_state:\n",
    "                pkg_info = persistent_packages_at_prev_state[dropped_pkg]\n",
    "                if (curr_r, curr_c) == pkg_info['target_pos']:\n",
    "                    if current_time <= pkg_info['deadline']:\n",
    "                        shaped_rewards[agent_idx] += SHAPING_SUCCESSFUL_DELIVERY_ON_TIME\n",
    "                    else:\n",
    "                        shaped_rewards[agent_idx] += SHAPING_SUCCESSFUL_DELIVERY_LATE\n",
    "\n",
    "        # 2. Phạt hành động lãng phí\n",
    "        if pkg_op == 1:  # Pick\n",
    "            if prev_pkg != 0:\n",
    "                shaped_rewards[agent_idx] += SHAPING_WASTED_PICKUP_ATTEMPT\n",
    "            elif curr_pkg == 0:\n",
    "                can_pickup = any(\n",
    "                    pkg['status'] == 'waiting' and\n",
    "                    pkg['start_time'] <= prev_env_state_dict['time_step'] and\n",
    "                    pkg['start_pos'] == (curr_r, curr_c)\n",
    "                    for pkg in persistent_packages_at_prev_state.values()\n",
    "                )\n",
    "                if not can_pickup:\n",
    "                    shaped_rewards[agent_idx] += SHAPING_WASTED_PICKUP_ATTEMPT\n",
    "        elif pkg_op == 2:  # Drop\n",
    "            if prev_pkg == 0:\n",
    "                shaped_rewards[agent_idx] += SHAPING_WASTED_DROP_ATTEMPT\n",
    "            elif curr_pkg != 0:\n",
    "                if prev_pkg in persistent_packages_at_prev_state:\n",
    "                    pkg_info = persistent_packages_at_prev_state[prev_pkg]\n",
    "                    if (curr_r, curr_c) != pkg_info['target_pos']:\n",
    "                        shaped_rewards[agent_idx] += SHAPING_WASTED_DROP_ATTEMPT\n",
    "\n",
    "        # 3. Di chuyển\n",
    "        moved = (prev_r, prev_c) != (curr_r, curr_c)\n",
    "        intended_move = move_str != 'S'\n",
    "        if intended_move and not moved:\n",
    "            shaped_rewards[agent_idx] += SHAPING_COLLISION_OR_STUCK\n",
    "\n",
    "        # Tính mục tiêu di chuyển\n",
    "        target_pos = None\n",
    "        if prev_pkg != 0 and prev_pkg in persistent_packages_at_prev_state:\n",
    "            target_pos = persistent_packages_at_prev_state[prev_pkg]['target_pos']\n",
    "        else:\n",
    "            # Gói waiting gần nhất\n",
    "            waiting_pkgs = [\n",
    "                pkg for pkg in persistent_packages_at_prev_state.values()\n",
    "                if pkg['status'] == 'waiting' and pkg['start_time'] <= prev_env_state_dict['time_step']\n",
    "            ]\n",
    "            if waiting_pkgs:\n",
    "                target_pos = min(\n",
    "                    (pkg['start_pos'] for pkg in waiting_pkgs),\n",
    "                    key=lambda pos: manhattan_distance((prev_r, prev_c), pos)\n",
    "                )\n",
    "        if target_pos and moved:\n",
    "            dist_before = manhattan_distance((prev_r, prev_c), target_pos)\n",
    "            dist_after = manhattan_distance((curr_r, curr_c), target_pos)\n",
    "            if dist_after < dist_before:\n",
    "                shaped_rewards[agent_idx] += SHAPING_MOVED_CLOSER_TO_TARGET\n",
    "            elif dist_after > dist_before:\n",
    "                shaped_rewards[agent_idx] += SHAPING_MOVED_AWAY_FROM_TARGET\n",
    "\n",
    "        # 4. Phạt đứng yên không cần thiết\n",
    "        if not moved and move_str == 'S' and prev_pkg == 0:\n",
    "            idle_nearby = any(\n",
    "                pkg['status'] == 'waiting' and\n",
    "                pkg['start_time'] <= prev_env_state_dict['time_step'] and\n",
    "                manhattan_distance((prev_r, prev_c), pkg['start_pos']) <= 3\n",
    "                for pkg in persistent_packages_at_prev_state.values()\n",
    "            )\n",
    "            if idle_nearby:\n",
    "                shaped_rewards[agent_idx] += SHAPING_IDLE_WITH_AVAILABLE_TASKS\n",
    "\n",
    "    return global_reward + shaped_rewards.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "71bf0337",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T17:48:22.195539Z",
     "iopub.status.busy": "2025-05-17T17:48:22.195057Z",
     "iopub.status.idle": "2025-05-17T17:48:22.201038Z",
     "shell.execute_reply": "2025-05-17T17:48:22.200327Z"
    },
    "id": "a76kUclNoBwv",
    "outputId": "1e985772-3e91-45d3-c8cd-024fdd57dac9",
    "papermill": {
     "duration": 0.015811,
     "end_time": "2025-05-17T17:48:22.202098",
     "exception": false,
     "start_time": "2025-05-17T17:48:22.186287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tổng shaped reward: 215.04001\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "prev_env_state_dict = {\n",
    "    'robots': [\n",
    "        (2, 2, 0),  # Robot 0: (1,1), không mang hàng\n",
    "        (5, 5, 1),  # Robot 1: (4,4), đang mang package 1\n",
    "    ],\n",
    "    'time_step': 4\n",
    "}\n",
    "current_env_state_dict = {\n",
    "    'robots': [\n",
    "        (3, 2, 2),  # Robot 0: (2,1), vừa nhặt package 2\n",
    "        (5, 4, 0),  # Robot 1: (4,3), vừa thả package 1\n",
    "    ],\n",
    "    'time_step': 5\n",
    "}\n",
    "actions_taken_for_all_agents = [\n",
    "    ('D', '1'),  # Robot 0: Move Down, Pick\n",
    "    ('L', '2'),  # Robot 1: Move Left, Drop\n",
    "]\n",
    "persistent_packages_at_prev_state = {\n",
    "    1: {\n",
    "        'id': 1,\n",
    "        'start_pos': (0, 0),\n",
    "        'target_pos': (4, 3),\n",
    "        'start_time': 0,\n",
    "        'deadline': 10,\n",
    "        'status': 'in_transit'\n",
    "    },\n",
    "    2: {\n",
    "        'id': 2,\n",
    "        'start_pos': (2, 1),\n",
    "        'target_pos': (0, 4),\n",
    "        'start_time': 3,\n",
    "        'deadline': 15,\n",
    "        'status': 'waiting'\n",
    "    }\n",
    "}\n",
    "num_agents = 2\n",
    "global_reward = 10\n",
    "shaped_reward = compute_shaped_rewards(\n",
    "    global_reward,\n",
    "    prev_env_state_dict, current_env_state_dict,\n",
    "    actions_taken_for_all_agents, persistent_packages_at_prev_state, num_agents\n",
    ")\n",
    "print(\"Tổng shaped reward:\", shaped_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f473b4",
   "metadata": {
    "id": "v0J0VO82oBwv",
    "papermill": {
     "duration": 0.008393,
     "end_time": "2025-05-17T17:48:22.218977",
     "exception": false,
     "start_time": "2025-05-17T17:48:22.210584",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ab4e91a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T17:48:22.236427Z",
     "iopub.status.busy": "2025-05-17T17:48:22.236229Z",
     "iopub.status.idle": "2025-05-17T17:48:22.241156Z",
     "shell.execute_reply": "2025-05-17T17:48:22.240667Z"
    },
    "id": "4JZfvKBVoBww",
    "papermill": {
     "duration": 0.014884,
     "end_time": "2025-05-17T17:48:22.242130",
     "exception": false,
     "start_time": "2025-05-17T17:48:22.227246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VectorizedEnv:\n",
    "    def __init__(self, env_cls, num_envs, **env_kwargs):\n",
    "        base_seed = env_kwargs.get('seed', None)\n",
    "        self.envs = []\n",
    "        # If map_file is a list, assign one per env\n",
    "        map_files = env_kwargs.get('map_file', None)\n",
    "        for idx in range(num_envs):\n",
    "            env_args = env_kwargs.copy()\n",
    "            if base_seed is not None:\n",
    "                env_args['seed'] = base_seed + idx\n",
    "            # Assign a different map file if a list is provided\n",
    "            if isinstance(map_files, list):\n",
    "                env_args['map_file'] = map_files[idx % len(map_files)]\n",
    "            self.envs.append(env_cls(**env_args))\n",
    "        self.num_envs = num_envs\n",
    "\n",
    "    def reset(self):\n",
    "        return [env.reset() for env in self.envs]\n",
    "\n",
    "    def step(self, actions):\n",
    "        # actions: list of actions for each env\n",
    "        results = [env.step(action) for env, action in zip(self.envs, actions)]\n",
    "        next_states, rewards, dones, infos = zip(*results)\n",
    "        return list(next_states), list(rewards), list(dones), list(infos)\n",
    "\n",
    "    def render(self):\n",
    "        window_positions = [(100, 100), (600, 100), (100, 600), (600, 600)]\n",
    "        for idx, env in enumerate(self.envs):\n",
    "            pos = window_positions[idx % len(window_positions)]\n",
    "            env.render_pygame(window_pos=pos)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fddbcb71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T17:48:22.260573Z",
     "iopub.status.busy": "2025-05-17T17:48:22.260368Z",
     "iopub.status.idle": "2025-05-17T17:48:22.291250Z",
     "shell.execute_reply": "2025-05-17T17:48:22.290551Z"
    },
    "id": "mZH1uy6UoBww",
    "papermill": {
     "duration": 0.0418,
     "end_time": "2025-05-17T17:48:22.292438",
     "exception": false,
     "start_time": "2025-05-17T17:48:22.250638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MAPPOTrainer:\n",
    "    def __init__(self, vec_env, num_agents, action_dim, obs_shape, global_state_shape,\n",
    "                 vector_obs_dim, global_vector_state_dim, render=False):\n",
    "        self.vec_env = vec_env\n",
    "        self.num_envs = vec_env.num_envs\n",
    "        self.num_agents = num_agents\n",
    "        self.action_dim = action_dim\n",
    "        self.obs_shape = obs_shape # (C, H, W) for local obs\n",
    "        self.global_state_shape = global_state_shape # (C_global, H, W) for global state\n",
    "        self.vector_obs_dim = vector_obs_dim\n",
    "        self.global_vector_state_dim = global_vector_state_dim\n",
    "        self.render = render\n",
    "\n",
    "        self.actor = ActorNetwork(obs_shape, vector_obs_dim, action_dim).to(device)\n",
    "        self.critic = CriticNetwork(global_state_shape, global_vector_state_dim).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=LR_ACTOR, weight_decay=WEIGHT_DECAY)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # For converting integer actions to environment actions\n",
    "        self.le_move = LabelEncoder()\n",
    "        self.le_move.fit(['S', 'L', 'R', 'U', 'D'])\n",
    "        self.le_pkg_op = LabelEncoder()\n",
    "        self.le_pkg_op.fit(['0', '1', '2']) # 0: None, 1: Pickup, 2: Drop\n",
    "        self.NUM_MOVE_ACTIONS = len(self.le_move.classes_)\n",
    "        self.NUM_PKG_OPS = len(self.le_pkg_op.classes_)\n",
    "\n",
    "        # Persistent packages trackers for each environment (for state conversion)\n",
    "        self.persistent_packages_list = [{} for _ in range(self.num_envs)]\n",
    "\n",
    "\n",
    "    def _update_persistent_packages_for_env(self, env_idx, current_env_state_dict):\n",
    "        # This is a simplified version of the DQNTrainer's method, adapted for one env\n",
    "        current_persistent_packages = self.persistent_packages_list[env_idx]\n",
    "\n",
    "        if 'packages' in current_env_state_dict and current_env_state_dict['packages'] is not None:\n",
    "            for pkg_tuple in current_env_state_dict['packages']:\n",
    "                pkg_id = pkg_tuple[0]\n",
    "                if pkg_id not in current_persistent_packages:\n",
    "                    current_persistent_packages[pkg_id] = {\n",
    "                        'id': pkg_id,\n",
    "                        'start_pos': (pkg_tuple[1] - 1, pkg_tuple[2] - 1),\n",
    "                        'target_pos': (pkg_tuple[3] - 1, pkg_tuple[4] - 1),\n",
    "                        'start_time': pkg_tuple[5],\n",
    "                        'deadline': pkg_tuple[6],\n",
    "                        'status': 'waiting'\n",
    "                    }\n",
    "\n",
    "        current_carried_pkg_ids_set = set()\n",
    "        if 'robots' in current_env_state_dict and current_env_state_dict['robots'] is not None:\n",
    "            for r_data in current_env_state_dict['robots']:\n",
    "                carried_id = r_data[2]\n",
    "                if carried_id != 0:\n",
    "                    current_carried_pkg_ids_set.add(carried_id)\n",
    "\n",
    "        packages_to_remove = []\n",
    "        for pkg_id, pkg_data in list(current_persistent_packages.items()):\n",
    "            if pkg_id in current_carried_pkg_ids_set:\n",
    "                current_persistent_packages[pkg_id]['status'] = 'in_transit'\n",
    "            else:\n",
    "                if pkg_data['status'] == 'in_transit':\n",
    "                    packages_to_remove.append(pkg_id)\n",
    "\n",
    "        for pkg_id_to_remove in packages_to_remove:\n",
    "            if pkg_id_to_remove in current_persistent_packages:\n",
    "                del current_persistent_packages[pkg_id_to_remove]\n",
    "        self.persistent_packages_list[env_idx] = current_persistent_packages\n",
    "\n",
    "\n",
    "    def _get_actions_and_values(self, current_local_obs_b_a_c_h_w, current_vector_obs_b_a_d, current_global_states_b_c_h_w, current_global_vector_b_d):\n",
    "        # Ensure input tensors are on the correct device\n",
    "        current_local_obs_b_a_c_h_w = current_local_obs_b_a_c_h_w.to(device)\n",
    "        current_vector_obs_b_a_d = current_vector_obs_b_a_d.to(device)\n",
    "        current_global_states_b_c_h_w = current_global_states_b_c_h_w.to(device)\n",
    "        current_global_vector_b_d = current_global_vector_b_d.to(device)\n",
    "\n",
    "        actor_input_obs = current_local_obs_b_a_c_h_w.reshape(self.num_envs * self.num_agents, self.obs_shape[0], self.obs_shape[1], self.obs_shape[2])\n",
    "        actor_input_vec = current_vector_obs_b_a_d.reshape(self.num_envs * self.num_agents, self.vector_obs_dim)\n",
    "        action_logits = self.actor(actor_input_obs, actor_input_vec) # (NUM_ENVS * NUM_AGENTS, ACTION_DIM)\n",
    "        dist = Categorical(logits=action_logits)\n",
    "        actions_int = dist.sample() # (NUM_ENVS * NUM_AGENTS)\n",
    "        log_probs = dist.log_prob(actions_int) # (NUM_ENVS * NUM_AGENTS)\n",
    "\n",
    "        actions_int_reshaped = actions_int.reshape(self.num_envs, self.num_agents)\n",
    "        log_probs_reshaped = log_probs.reshape(self.num_envs, self.num_agents)\n",
    "\n",
    "        values = self.critic(current_global_states_b_c_h_w, current_global_vector_b_d) # (NUM_ENVS, 1)\n",
    "\n",
    "        return actions_int_reshaped, log_probs_reshaped, values.squeeze(-1) # values squeezed to (NUM_ENVS)\n",
    "\n",
    "    def collect_rollouts(self, current_env_states_list, current_local_obs_list, current_vector_obs_list, current_global_states_list, current_global_vector_list):\n",
    "        # Buffers to store trajectory data\n",
    "        mb_obs = torch.zeros((ROLLOUT_STEPS, self.num_envs, self.num_agents, *self.obs_shape), device=device)\n",
    "        mb_vector_obs = torch.zeros((ROLLOUT_STEPS, self.num_envs, self.num_agents, self.vector_obs_dim), device=device)\n",
    "        mb_global_states = torch.zeros((ROLLOUT_STEPS, self.num_envs, *self.global_state_shape), device=device)\n",
    "        mb_global_vector = torch.zeros((ROLLOUT_STEPS, self.num_envs, self.global_vector_state_dim), device=device)\n",
    "        mb_actions = torch.zeros((ROLLOUT_STEPS, self.num_envs, self.num_agents), dtype=torch.long, device=device)\n",
    "        mb_log_probs = torch.zeros((ROLLOUT_STEPS, self.num_envs, self.num_agents), device=device)\n",
    "        mb_rewards = torch.zeros((ROLLOUT_STEPS, self.num_envs), device=device)\n",
    "        mb_dones = torch.zeros((ROLLOUT_STEPS, self.num_envs), dtype=torch.bool, device=device)\n",
    "        mb_values = torch.zeros((ROLLOUT_STEPS, self.num_envs), device=device)\n",
    "\n",
    "        # Move initial obs/states to device\n",
    "        current_local_obs_list = current_local_obs_list.to(device)\n",
    "        current_vector_obs_list = current_vector_obs_list.to(device)\n",
    "        current_global_states_list = current_global_states_list.to(device)\n",
    "        current_global_vector_list = current_global_vector_list.to(device)\n",
    "\n",
    "        for step in range(ROLLOUT_STEPS):\n",
    "            # Render the environment\n",
    "            if self.render:\n",
    "                self.vec_env.render()\n",
    "\n",
    "            mb_obs[step] = current_local_obs_list\n",
    "            mb_vector_obs[step] = current_vector_obs_list\n",
    "            mb_global_states[step] = current_global_states_list\n",
    "            mb_global_vector[step] = current_global_vector_list\n",
    "\n",
    "            with torch.no_grad():\n",
    "                actions_int_ne_na, log_probs_ne_na, values_ne = self._get_actions_and_values(\n",
    "                    current_local_obs_list,\n",
    "                    current_vector_obs_list,\n",
    "                    current_global_states_list,\n",
    "                    current_global_vector_list\n",
    "                )\n",
    "\n",
    "            mb_actions[step] = actions_int_ne_na\n",
    "            mb_log_probs[step] = log_probs_ne_na\n",
    "            mb_values[step] = values_ne\n",
    "\n",
    "            # Convert integer actions to environment compatible actions\n",
    "            env_actions_batch = []\n",
    "            for env_idx in range(self.num_envs):\n",
    "                env_agent_actions = []\n",
    "                for agent_idx in range(self.num_agents):\n",
    "                    int_act = actions_int_ne_na[env_idx, agent_idx].item()\n",
    "                    move_idx = int_act % self.NUM_MOVE_ACTIONS\n",
    "                    pkg_op_idx = int_act // self.NUM_MOVE_ACTIONS\n",
    "                    if pkg_op_idx >= self.NUM_PKG_OPS: pkg_op_idx = 0 # Safety clamp\n",
    "\n",
    "                    move_str = self.le_move.inverse_transform([move_idx])[0]\n",
    "                    pkg_op_str = self.le_pkg_op.inverse_transform([pkg_op_idx])[0]\n",
    "                    env_agent_actions.append((move_str, pkg_op_str))\n",
    "                env_actions_batch.append(env_agent_actions)\n",
    "\n",
    "            next_env_states_list, global_rewards_ne, dones_ne, _ = self.vec_env.step(env_actions_batch)\n",
    "\n",
    "            # use reward shaping here\n",
    "            reshaped_global_rewards_ne = [compute_shaped_rewards(\n",
    "                                                        global_rewards_ne[env_idx],\n",
    "                                                        current_env_states_list[env_idx],\n",
    "                                                        next_env_states_list[env_idx],\n",
    "                                                        env_actions_batch[env_idx],\n",
    "                                                        self.persistent_packages_list[env_idx],\n",
    "                                                        self.num_agents)\n",
    "                                          for env_idx in range(self.num_envs)]\n",
    "\n",
    "            mb_rewards[step] = torch.tensor(reshaped_global_rewards_ne, dtype=torch.float32, device=device)\n",
    "            mb_dones[step] = torch.tensor(dones_ne, dtype=torch.bool, device=device)\n",
    "\n",
    "            # Prepare next observations and states\n",
    "            next_local_obs_list = torch.zeros_like(current_local_obs_list, device=device)\n",
    "            next_vector_obs_list = torch.zeros_like(current_vector_obs_list, device=device)\n",
    "            next_global_states_list = torch.zeros_like(current_global_states_list, device=device)\n",
    "            next_global_vector_list = torch.zeros_like(current_global_vector_list, device=device)\n",
    "\n",
    "            for env_idx in range(self.num_envs):\n",
    "                if dones_ne[env_idx]:\n",
    "                    # --- Reset environment if done ---\n",
    "                    reset_state = self.vec_env.envs[env_idx].reset()\n",
    "                    self._update_persistent_packages_for_env(env_idx, reset_state)\n",
    "                    current_persistent_packages = self.persistent_packages_list[env_idx]\n",
    "                    next_env_states_list[env_idx] = reset_state\n",
    "                    # Update global state and local obs after reset\n",
    "                    next_global_states_list[env_idx] = torch.from_numpy(convert_global_state(\n",
    "                            reset_state,\n",
    "                            current_persistent_packages,\n",
    "                            MAX_TIME_STEPS_PER_EPISODE,\n",
    "                        )[0]\n",
    "                    ).to(device)\n",
    "                    next_global_vector_list[env_idx] = torch.from_numpy(\n",
    "                        convert_global_state(reset_state,\n",
    "                                             current_persistent_packages,\n",
    "                                             MAX_TIME_STEPS_PER_EPISODE\n",
    "                                             )[1]\n",
    "                    ).float().to(device)\n",
    "\n",
    "                    for agent_idx in range(self.num_agents):\n",
    "                        next_local_obs_list[env_idx, agent_idx] = torch.from_numpy(\n",
    "                            convert_map(reset_state, current_persistent_packages, agent_idx)\n",
    "                        ).float().to(device)\n",
    "                        next_vector_obs_list[env_idx, agent_idx] = torch.from_numpy(\n",
    "                            convert_features(reset_state, current_persistent_packages, agent_idx,\n",
    "                                                    MAX_TIME_STEPS_PER_EPISODE)\n",
    "                        ).float().to(device)\n",
    "                else:\n",
    "                    self._update_persistent_packages_for_env(env_idx, next_env_states_list[env_idx])\n",
    "                    current_persistent_packages = self.persistent_packages_list[env_idx]\n",
    "                    next_global_states_list[env_idx] = torch.from_numpy(convert_global_state(\n",
    "                            next_env_states_list[env_idx],\n",
    "                            current_persistent_packages,\n",
    "                            MAX_TIME_STEPS_PER_EPISODE,\n",
    "                        )[0]\n",
    "                    ).to(device)\n",
    "                    next_global_vector_list[env_idx] = torch.from_numpy(\n",
    "                        convert_global_state(next_env_states_list[env_idx],\n",
    "                                             current_persistent_packages,\n",
    "                                             MAX_TIME_STEPS_PER_EPISODE\n",
    "                                             )[1]\n",
    "                    ).float().to(device)\n",
    "                    for agent_idx in range(self.num_agents):\n",
    "                        next_local_obs_list[env_idx, agent_idx] = torch.from_numpy(\n",
    "                            convert_map(next_env_states_list[env_idx], current_persistent_packages, agent_idx)\n",
    "                        ).float().to(device)\n",
    "                        next_vector_obs_list[env_idx, agent_idx] = torch.from_numpy(\n",
    "                            convert_features(next_env_states_list[env_idx], current_persistent_packages, agent_idx,\n",
    "                                                    MAX_TIME_STEPS_PER_EPISODE)\n",
    "                        ).float().to(device)\n",
    "\n",
    "            current_env_states_list = next_env_states_list\n",
    "            current_local_obs_list = next_local_obs_list\n",
    "            current_vector_obs_list = next_vector_obs_list\n",
    "            current_global_states_list = next_global_states_list\n",
    "            current_global_vector_list = next_global_vector_list\n",
    "\n",
    "        # Calculate advantages using GAE\n",
    "        advantages = torch.zeros_like(mb_rewards, device=device)\n",
    "        last_gae_lambda = 0\n",
    "        with torch.no_grad():\n",
    "            # Get value of the last state in the rollout\n",
    "            next_value_ne = self.critic(current_global_states_list, current_global_vector_list).squeeze(-1) # (NUM_ENVS)\n",
    "\n",
    "        for t in reversed(range(ROLLOUT_STEPS)):\n",
    "            next_non_terminal = 1.0 - mb_dones[t].float()\n",
    "            next_values_step = next_value_ne if t == ROLLOUT_STEPS - 1 else mb_values[t+1]\n",
    "\n",
    "            delta = mb_rewards[t] + GAMMA * next_values_step * next_non_terminal - mb_values[t]\n",
    "            advantages[t] = last_gae_lambda = delta + GAMMA * GAE_LAMBDA * next_non_terminal * last_gae_lambda\n",
    "\n",
    "        returns = advantages + mb_values\n",
    "\n",
    "        # Flatten the batch for training\n",
    "        b_obs = mb_obs.reshape(-1, *self.obs_shape)\n",
    "        b_vector_obs = mb_vector_obs.reshape(-1, self.vector_obs_dim)\n",
    "        b_global_states = mb_global_states.reshape(ROLLOUT_STEPS * self.num_envs, *self.global_state_shape)\n",
    "        b_global_vector = mb_global_vector.reshape(ROLLOUT_STEPS * self.num_envs, self.global_vector_state_dim)\n",
    "        b_actions = mb_actions.reshape(-1)\n",
    "        b_log_probs = mb_log_probs.reshape(-1)\n",
    "\n",
    "        b_advantages = advantages.reshape(ROLLOUT_STEPS * self.num_envs, 1).repeat(1, self.num_agents).reshape(-1)\n",
    "        b_returns_critic = returns.reshape(-1)\n",
    "\n",
    "        return (b_obs, b_vector_obs, b_global_states, b_global_vector, b_actions,\n",
    "           b_log_probs, b_advantages, b_returns_critic,\n",
    "           current_env_states_list, current_local_obs_list, current_vector_obs_list, current_global_states_list, current_global_vector_list,\n",
    "           mb_rewards)\n",
    "\n",
    "    def update_ppo(self, b_obs, b_vector_obs, b_global_states, b_global_vector, b_actions, b_log_probs_old, b_advantages, b_returns_critic):\n",
    "        # Ensure all tensors are on the correct device\n",
    "        b_obs = b_obs.to(device)\n",
    "        b_vector_obs = b_vector_obs.to(device)\n",
    "        b_global_states = b_global_states.to(device)\n",
    "        b_global_vector = b_global_vector.to(device)\n",
    "        b_actions = b_actions.to(device)\n",
    "        b_log_probs_old = b_log_probs_old.to(device)\n",
    "        b_advantages = b_advantages.to(device)\n",
    "        b_returns_critic = b_returns_critic.to(device)\n",
    "\n",
    "        num_samples_actor = ROLLOUT_STEPS * self.num_envs * self.num_agents\n",
    "        num_samples_critic = ROLLOUT_STEPS * self.num_envs\n",
    "\n",
    "        actor_batch_indices = np.arange(num_samples_actor)\n",
    "        critic_batch_indices = np.arange(num_samples_critic)\n",
    "\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            np.random.shuffle(actor_batch_indices)\n",
    "            np.random.shuffle(critic_batch_indices)\n",
    "\n",
    "            # Actor update\n",
    "            for start in range(0, num_samples_actor, MINIBATCH_SIZE):\n",
    "                end = start + MINIBATCH_SIZE\n",
    "                mb_indices = actor_batch_indices[start:end]\n",
    "\n",
    "                mb_obs_slice = b_obs[mb_indices]\n",
    "                mb_vector_obs_slice = b_vector_obs[mb_indices]\n",
    "                mb_actions_slice = b_actions[mb_indices]\n",
    "                mb_log_probs_old_slice = b_log_probs_old[mb_indices]\n",
    "                mb_advantages_slice = b_advantages[mb_indices]\n",
    "\n",
    "                # Normalize advantages (optional but often helpful)\n",
    "                mb_advantages_slice = (mb_advantages_slice - mb_advantages_slice.mean()) / (mb_advantages_slice.std() + 1e-8)\n",
    "\n",
    "                action_logits = self.actor(mb_obs_slice, mb_vector_obs_slice)\n",
    "                dist = Categorical(logits=action_logits)\n",
    "                new_log_probs = dist.log_prob(mb_actions_slice)\n",
    "                entropy = dist.entropy().mean()\n",
    "\n",
    "                log_ratio = new_log_probs - mb_log_probs_old_slice\n",
    "                ratio = torch.exp(log_ratio)\n",
    "\n",
    "                pg_loss1 = -mb_advantages_slice * ratio\n",
    "                pg_loss2 = -mb_advantages_slice * torch.clamp(ratio, 1 - CLIP_EPS, 1 + CLIP_EPS)\n",
    "                actor_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                total_actor_loss = actor_loss - ENTROPY_COEF * entropy\n",
    "\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                total_actor_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.actor.parameters(), MAX_GRAD_NORM)\n",
    "                self.actor_optimizer.step()\n",
    "\n",
    "            # Critic update\n",
    "            for start in range(0, num_samples_critic, MINIBATCH_SIZE // self.num_agents if self.num_agents > 0 else MINIBATCH_SIZE):\n",
    "                end = start + (MINIBATCH_SIZE // self.num_agents if self.num_agents > 0 else MINIBATCH_SIZE)\n",
    "                mb_indices = critic_batch_indices[start:end]\n",
    "\n",
    "                mb_global_states_slice = b_global_states[mb_indices]\n",
    "                mb_global_vector_slice = b_global_vector[mb_indices]\n",
    "                mb_returns_critic_slice = b_returns_critic[mb_indices]\n",
    "\n",
    "                new_values = self.critic(mb_global_states_slice, mb_global_vector_slice).squeeze(-1)\n",
    "                critic_loss = F.mse_loss(new_values, mb_returns_critic_slice)\n",
    "\n",
    "                total_critic_loss = VALUE_LOSS_COEF * critic_loss\n",
    "\n",
    "                self.critic_optimizer.zero_grad()\n",
    "                total_critic_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.critic.parameters(), MAX_GRAD_NORM)\n",
    "                self.critic_optimizer.step()\n",
    "\n",
    "        return actor_loss.item(), critic_loss.item(), entropy.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0ea2dc",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5c441c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T17:48:22.310600Z",
     "iopub.status.busy": "2025-05-17T17:48:22.310384Z",
     "iopub.status.idle": "2025-05-17T19:47:53.814831Z",
     "shell.execute_reply": "2025-05-17T19:47:53.814112Z"
    },
    "id": "gAbzMamgoBww",
    "outputId": "370b1d53-ca3b-47ee-f322-08a4002d8af4",
    "papermill": {
     "duration": 7171.517791,
     "end_time": "2025-05-17T19:47:53.819044",
     "exception": false,
     "start_time": "2025-05-17T17:48:22.301253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Obs shape: (6, 20, 20), Global state shape: (4, 20, 20), Vector obs dim: 1007, Global vector dim: 1301\n",
      "Could not find MAPPO models at prefix models/mappo_map1\n",
      "Starting MAPPO training...\n",
      "Rollout 1: Total Reward = 38.53\n",
      "\n",
      "Training interrupted by user.\n",
      "Saving final model...\n",
      "MAPPO models saved with prefix models/mappo_final\n",
      "\n",
      "Training loop finished or was interrupted.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {device}\")\n",
    "\n",
    "map_files = [f\"map{i}.txt\" for i in range(2, 6)]  # map2.txt to map5.txt\n",
    "vec_env = VectorizedEnv(\n",
    "    Environment,\n",
    "    num_envs=NUM_ENVS,\n",
    "    map_file=map_files,  # Pass the list of map files\n",
    "    n_robots=NUM_AGENTS,\n",
    "    n_packages=N_PACKAGES,\n",
    "    move_cost=MOVE_COST,\n",
    "    delivery_reward=DELIVERY_REWARD,\n",
    "    delay_reward=DELAY_REWARD,\n",
    "    seed=SEED,  # Seed for each sub-environment will be SEED, SEED+1, ...\n",
    "    max_time_steps=MAX_TIME_STEPS_PER_EPISODE,\n",
    ")\n",
    "\n",
    "# Determine observation and global state shapes from one env instance\n",
    "_temp_env = Environment(map_file=map_files[0], n_robots=NUM_AGENTS, n_packages=N_PACKAGES, move_cost=MOVE_COST, delivery_reward=DELIVERY_REWARD, delay_reward=DELAY_REWARD, seed=SEED, max_time_steps=MAX_TIME_STEPS_PER_EPISODE)\n",
    "\n",
    "\n",
    "OBS_SHAPE = (6, _temp_env.n_rows, _temp_env.n_cols)\n",
    "GLOBAL_STATE_SHAPE = (4, _temp_env.n_rows, _temp_env.n_cols)\n",
    "VECTOR_OBS_DIM = convert_features(_temp_env.reset(), {}, 0, MAX_TIME_STEPS_PER_EPISODE).shape[0]\n",
    "GLOBAL_VECTOR_STATE_DIM = convert_global_state(_temp_env.reset(), {}, MAX_TIME_STEPS_PER_EPISODE)[1].shape[0]\n",
    "\n",
    "print(f\"Obs shape: {OBS_SHAPE}, Global state shape: {GLOBAL_STATE_SHAPE}, Vector obs dim: {VECTOR_OBS_DIM}, Global vector dim: {GLOBAL_VECTOR_STATE_DIM}\")\n",
    "\n",
    "trainer = MAPPOTrainer(vec_env, NUM_AGENTS, ACTION_DIM, OBS_SHAPE, GLOBAL_STATE_SHAPE, VECTOR_OBS_DIM, GLOBAL_VECTOR_STATE_DIM)\n",
    "\n",
    "# Load existing model if available\n",
    "# load_mappo_model(trainer.actor, trainer.critic,\"models/mappo_map1\", device=device)\n",
    "\n",
    "episode_rewards_history = []\n",
    "actor_loss_history = []\n",
    "critic_loss_history = []\n",
    "entropy_history = []\n",
    "rollout_reward_history = []\n",
    "\n",
    "print(\"Starting MAPPO training...\")\n",
    "\n",
    "# Initial reset and state preparation\n",
    "current_env_states_list = vec_env.reset() # List of state dicts\n",
    "current_local_obs_list = torch.zeros((NUM_ENVS, NUM_AGENTS, *OBS_SHAPE), device=\"cpu\")\n",
    "current_vector_obs_list = torch.zeros((NUM_ENVS, NUM_AGENTS, VECTOR_OBS_DIM), device=\"cpu\")\n",
    "current_global_states_list = torch.zeros((NUM_ENVS, *GLOBAL_STATE_SHAPE), device=\"cpu\")\n",
    "current_global_vector_list = torch.zeros((NUM_ENVS, GLOBAL_VECTOR_STATE_DIM), device=\"cpu\")\n",
    "\n",
    "for env_idx in range(NUM_ENVS):\n",
    "    trainer._update_persistent_packages_for_env(env_idx, current_env_states_list[env_idx])\n",
    "    current_persistent_packages = trainer.persistent_packages_list[env_idx]\n",
    "    current_global_states_list[env_idx] = torch.from_numpy(convert_global_state(\n",
    "                                                        current_env_states_list[env_idx],\n",
    "                                                        current_persistent_packages,\n",
    "                                                        MAX_TIME_STEPS_PER_EPISODE,\n",
    "                                                        )[0]\n",
    "                                                        )\n",
    "    current_global_vector_list[env_idx] = torch.from_numpy(convert_global_state(\n",
    "                                                        current_env_states_list[env_idx],\n",
    "                                                        current_persistent_packages,\n",
    "                                                        MAX_TIME_STEPS_PER_EPISODE,\n",
    "                                                        )[1]\n",
    "                                                        )\n",
    "    for agent_idx in range(NUM_AGENTS):\n",
    "        current_local_obs_list[env_idx, agent_idx] = torch.from_numpy(\n",
    "            convert_map(current_env_states_list[env_idx], current_persistent_packages, agent_idx)\n",
    "        ).float()\n",
    "        current_vector_obs_list[env_idx, agent_idx] = torch.from_numpy(\n",
    "            convert_features(current_env_states_list[env_idx], current_persistent_packages, agent_idx,\n",
    "                                    MAX_TIME_STEPS_PER_EPISODE)\n",
    "        ).float()\n",
    "\n",
    "num_updates = TOTAL_TIMESTEPS // (ROLLOUT_STEPS * NUM_ENVS)\n",
    "total_steps_done = 0\n",
    "\n",
    "try:\n",
    "    for update_num in range(1, num_updates + 1):\n",
    "        (b_obs, b_vector_obs, b_global_states, b_global_vector, b_actions, b_log_probs_old,\n",
    "            b_advantages, b_returns_critic,\n",
    "            current_env_states_list, current_local_obs_list, current_vector_obs_list, current_global_states_list, current_global_vector_list,\n",
    "            mb_rewards\n",
    "        ) = trainer.collect_rollouts(current_env_states_list, current_local_obs_list, current_vector_obs_list, current_global_states_list, current_global_vector_list)\n",
    "\n",
    "        # Track reward per rollout\n",
    "        rollout_total_reward = mb_rewards.sum().item()\n",
    "        rollout_reward_history.append(rollout_total_reward)\n",
    "        print(f\"Rollout {update_num}: Total Reward = {rollout_total_reward:.2f}\")\n",
    "\n",
    "        actor_loss, critic_loss, entropy = trainer.update_ppo(\n",
    "            b_obs, b_vector_obs, b_global_states, b_global_vector, b_actions, b_log_probs_old, b_advantages, b_returns_critic\n",
    "        )\n",
    "\n",
    "        total_steps_done += ROLLOUT_STEPS * NUM_ENVS\n",
    "\n",
    "        actor_loss_history.append(actor_loss)\n",
    "        critic_loss_history.append(critic_loss)\n",
    "        entropy_history.append(entropy)\n",
    "\n",
    "        if update_num % 10 == 0: # Log every 10 updates\n",
    "            print(f\"Update {update_num}/{num_updates} | Timesteps: {total_steps_done}/{TOTAL_TIMESTEPS}\")\n",
    "            print(f\"  Actor Loss: {actor_loss:.4f} | Critic Loss: {critic_loss:.4f} | Entropy: {entropy:.4f}\")\n",
    "            print(f\"  Rollout Total Reward: {rollout_total_reward:.2f}\")\n",
    "\n",
    "        if update_num % 100 == 0: # Save model periodically\n",
    "            print(f\"Saving checkpoint at update {update_num}...\")\n",
    "            save_mappo_model(trainer.actor, trainer.critic, path_prefix=f\"models/mappo_update{update_num}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted by user.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during training: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    print(\"Saving final model...\")\n",
    "    save_mappo_model(trainer.actor, trainer.critic, path_prefix=\"models/mappo_final\")\n",
    "    print(\"\\nTraining loop finished or was interrupted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1b7e9e",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8397f069",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_MOVE_ACTIONS = 5  # S, L, R, U, D\n",
    "NUM_PKG_OPS    = 3  # None, Pickup, Drop\n",
    "JOINT_ACTION_DIM = NUM_MOVE_ACTIONS * NUM_PKG_OPS\n",
    "        \n",
    "class Agents:\n",
    "    def __init__(self, observation_shape, vector_obs_dim, max_time_steps, weights_path=None, device=None):\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # Joint action encoder\n",
    "        self.le_move = LabelEncoder().fit(['S','L','R','U','D'])\n",
    "        self.le_pkg  = LabelEncoder().fit(['0','1','2']) # Assuming '0' is None, '1' is Pickup, '2' is Drop\n",
    "        self.model = ActorNetwork(observation_shape, vector_obs_dim).to(self.device) # Pass JOINT_ACTION_DIM\n",
    "        if weights_path is not None:\n",
    "            try:\n",
    "                # It's good practice to print a message before loading\n",
    "                print(f\"Loading model weights from: {weights_path}\")\n",
    "                state_dict = torch.load(weights_path, map_location=self.device, weights_only=True)\n",
    "                self.model.load_state_dict(state_dict)\n",
    "                print(\"Model weights loaded successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading model weights: {e}\")\n",
    "\n",
    "        self.model.eval()\n",
    "        self.n_robots = 0\n",
    "        self.is_init = False\n",
    "        self.persistent_packages = {} # Initialize persistent_packages\n",
    "        self.max_time_steps = max_time_steps\n",
    "    def _update_persistent_packages(self, current_env_state_dict):\n",
    "        # This is a simplified version of the DQNTrainer's method, adapted for one env\n",
    "        current_persistent_packages = self.persistent_packages\n",
    "        \n",
    "        if 'packages' in current_env_state_dict and current_env_state_dict['packages'] is not None:\n",
    "            for pkg_tuple in current_env_state_dict['packages']:\n",
    "                pkg_id = pkg_tuple[0]\n",
    "                if pkg_id not in current_persistent_packages:\n",
    "                    current_persistent_packages[pkg_id] = {\n",
    "                        'id': pkg_id,\n",
    "                        'start_pos': (pkg_tuple[1] - 1, pkg_tuple[2] - 1),\n",
    "                        'target_pos': (pkg_tuple[3] - 1, pkg_tuple[4] - 1),\n",
    "                        'start_time': pkg_tuple[5],\n",
    "                        'deadline': pkg_tuple[6],\n",
    "                        'status': 'waiting'\n",
    "                    }\n",
    "\n",
    "        current_carried_pkg_ids_set = set()\n",
    "        if 'robots' in current_env_state_dict and current_env_state_dict['robots'] is not None:\n",
    "            for r_data in current_env_state_dict['robots']:\n",
    "                carried_id = r_data[2]\n",
    "                if carried_id != 0:\n",
    "                    current_carried_pkg_ids_set.add(carried_id)\n",
    "\n",
    "        packages_to_remove = []\n",
    "        for pkg_id, pkg_data in list(current_persistent_packages.items()):\n",
    "            if pkg_id in current_carried_pkg_ids_set:\n",
    "                current_persistent_packages[pkg_id]['status'] = 'in_transit'\n",
    "            else:\n",
    "                if pkg_data['status'] == 'in_transit':\n",
    "                    packages_to_remove.append(pkg_id)\n",
    "        \n",
    "        for pkg_id_to_remove in packages_to_remove:\n",
    "            if pkg_id_to_remove in current_persistent_packages:\n",
    "                del current_persistent_packages[pkg_id_to_remove]\n",
    "        self.persistent_packages = current_persistent_packages\n",
    "\n",
    "    def init_agents(self, state):\n",
    "        self.n_robots = len(state.get('robots', []))\n",
    "        self._update_persistent_packages(state) # Initialize/update based on initial state\n",
    "        self.is_init = True\n",
    "\n",
    "    def get_actions(self, state, deterministic=True):\n",
    "        assert self.is_init, \"Agents not initialized. Call init_agents(state) first.\"\n",
    "        \n",
    "        # Update persistent packages based on the current state\n",
    "        self._update_persistent_packages(state)\n",
    "        \n",
    "        actions = []\n",
    "        for i in range(self.n_robots):\n",
    "            # Prepare observation and vector features\n",
    "            obs = convert_map(state, self.persistent_packages, current_robot_idx=i)\n",
    "            vector_obs = convert_features(\n",
    "                state, \n",
    "                self.persistent_packages, \n",
    "                i, \n",
    "                self.max_time_steps,\n",
    "            )\n",
    "            obs_t = torch.tensor(obs, dtype=torch.float32).to(self.device)\n",
    "            vector_obs_t = torch.tensor(vector_obs, dtype=torch.float32).to(self.device)\n",
    "            if obs_t.dim() == 3:\n",
    "                obs_t = obs_t.unsqueeze(0)\n",
    "                vector_obs_t = vector_obs_t.unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                logits = self.model(obs_t, vector_obs_t)  # shape [1, JOINT_ACTION_DIM]\n",
    "                if deterministic:\n",
    "                    joint_idx = logits.argmax(dim=1).item()\n",
    "                else:\n",
    "                    probs = torch.softmax(logits, dim=1)\n",
    "                    joint_idx = torch.multinomial(probs, num_samples=1).item()\n",
    "            \n",
    "            # decode joint index\n",
    "            move_idx   = joint_idx % NUM_MOVE_ACTIONS\n",
    "            pkg_idx    = joint_idx // NUM_MOVE_ACTIONS\n",
    "            \n",
    "            move_str   = self.le_move.inverse_transform([move_idx])[0]\n",
    "            pkg_str    = self.le_pkg.inverse_transform([pkg_idx])[0]\n",
    "            actions.append((move_str, pkg_str))\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a40c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "Loading model weights from: /home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\n",
      "Model weights loaded successfully.\n",
      "\n",
      "=== MAPPO Evaluation Results ===\n",
      "mean_reward=-5.95±4.26\n",
      "mean_delivered=1.37±1.27\n",
      "mean_delivery_rate=0.27%±0.25%\n"
     ]
    }
   ],
   "source": [
    "# eval_env = Environment(\"map2.txt\", 1000, 5, 100, -0.01, 10., 1., 10)\n",
    "\n",
    "# Cấu hình test\n",
    "num_agents = 5\n",
    "n_packages = 100\n",
    "seed = 2025\n",
    "max_time_steps = 1000\n",
    "map_path = \"map1.txt\"\n",
    "num_test_episodes = 100\n",
    "mappo_model_path = \"/home/hungmanh/home_work/RL/marl-delivery/models/mappo_final_actor_1.pt\"\n",
    "device = \"cuda\"\n",
    "\n",
    "# Lấy thông tin đầu vào\n",
    "temp_env = Environment(map_file=map_path, max_time_steps=max_time_steps,\n",
    "                       n_robots=num_agents, n_packages=n_packages, seed=seed)\n",
    "temp_state = temp_env.reset()\n",
    "obs_shape = (6, temp_env.n_rows, temp_env.n_cols)\n",
    "try:\n",
    "    vec_obs_dim = convert_features(temp_state, {}, 0, max_time_steps).shape[0]\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi khi tạo vector_obs_dim: {e}. Đặt tạm vec_obs_dim = 0.\")\n",
    "    vec_obs_dim = 0\n",
    "del temp_env\n",
    "\n",
    "# Khởi tạo thông số môi trường và agent\n",
    "env_config = {\n",
    "    'max_time_steps': max_time_steps,\n",
    "    'n_packages': n_packages,\n",
    "    'n_agents': num_agents,\n",
    "    'seed': seed,\n",
    "    'map': map_path,\n",
    "}\n",
    "mappo_params = {\n",
    "    'observation_shape': obs_shape,\n",
    "    'vector_obs_dim': vec_obs_dim,\n",
    "    'model_path': mappo_model_path,\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "# Hàm test MAPPO agent\n",
    "def run_eval_mappo(num_episodes, mappo_params, env_config):\n",
    "    rewards, delivered, delivery_rate = [], [], []\n",
    "    for ep in range(num_episodes):\n",
    "        env = Environment(\n",
    "            map_file=env_config['map'],\n",
    "            max_time_steps=env_config['max_time_steps'],\n",
    "            n_robots=env_config['n_agents'],\n",
    "            n_packages=env_config['n_packages'],\n",
    "            seed=env_config['seed'] + ep\n",
    "        )\n",
    "        state = env.reset()\n",
    "        agent = Agents(\n",
    "            mappo_params['observation_shape'],\n",
    "            mappo_params['vector_obs_dim'],\n",
    "            env_config['max_time_steps'],\n",
    "            mappo_params['model_path'],\n",
    "            mappo_params['device']\n",
    "        )\n",
    "        agent.init_agents(state)\n",
    "        done = False\n",
    "        infos = {}\n",
    "        while not done:\n",
    "            actions = agent.get_actions(state, deterministic=False)\n",
    "            state, _, current_done, infos = env.step(actions)\n",
    "            done = current_done\n",
    "        rewards.append(infos.get('total_reward', env.total_reward))\n",
    "        delivered.append(sum(1 for p in env.packages if p.status == 'delivered'))\n",
    "        if env_config['n_packages'] > 0:\n",
    "            rate = (delivered[-1] / env_config['n_packages']) * 100\n",
    "            delivery_rate.append(rate)\n",
    "    return {\n",
    "        \"mean_reward\": np.mean(rewards),\n",
    "        \"std_reward\": np.std(rewards),\n",
    "        \"mean_delivered\": np.mean(delivered),\n",
    "        \"std_delivered\": np.std(delivered),\n",
    "        \"mean_delivery_rate\": np.mean(delivery_rate),\n",
    "        \"std_delivery_rate\": np.std(delivery_rate),\n",
    "    }\n",
    "\n",
    "\n",
    "metrics = run_eval_mappo(num_test_episodes, mappo_params, env_config)\n",
    "print(\"\\n=== MAPPO Evaluation Results ===\")\n",
    "print(f\"mean_reward={metrics['mean_reward']:.2f}±{metrics['std_reward']:.2f}\")\n",
    "print(f\"mean_delivered={metrics['mean_delivered']:.2f}±{metrics['std_delivered']:.2f}\")\n",
    "print(f\"mean_delivery_rate={metrics['mean_delivery_rate']:.2f}%±{metrics['std_delivery_rate']:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7189.060221,
   "end_time": "2025-05-17T19:47:56.605375",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-17T17:48:07.545154",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
