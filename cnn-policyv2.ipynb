{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AM3r0wPe5N0K"
   },
   "source": [
    "Solving Package delivery using single-agent PPO with a naive feature representation learning: concatenante all the feature in to a single state vector, and multiple robot actions as a multi discrete distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T14:24:00.824019Z",
     "iopub.status.busy": "2025-05-15T14:24:00.823838Z",
     "iopub.status.idle": "2025-05-15T14:24:01.473592Z",
     "shell.execute_reply": "2025-05-15T14:24:01.472651Z",
     "shell.execute_reply.started": "2025-05-15T14:24:00.824001Z"
    },
    "id": "9Ro5mHQ3GnN8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# !git clone https://github.com/cuongtv312/marl-delivery.git\n",
    "%cd marl-delivery\n",
    "# !uv add -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-14T14:12:10.927665Z",
     "iopub.status.idle": "2025-05-14T14:12:10.927909Z",
     "shell.execute_reply": "2025-05-14T14:12:10.927812Z",
     "shell.execute_reply.started": "2025-05-14T14:12:10.927797Z"
    },
    "id": "uWjMBXQoG4JL",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install stable-baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T14:24:27.027680Z",
     "iopub.status.busy": "2025-05-15T14:24:27.027006Z",
     "iopub.status.idle": "2025-05-15T14:24:30.909065Z",
     "shell.execute_reply": "2025-05-15T14:24:30.908076Z",
     "shell.execute_reply.started": "2025-05-15T14:24:27.027651Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T14:24:47.290464Z",
     "iopub.status.busy": "2025-05-15T14:24:47.289639Z",
     "iopub.status.idle": "2025-05-15T14:24:47.293858Z",
     "shell.execute_reply": "2025-05-15T14:24:47.293254Z",
     "shell.execute_reply.started": "2025-05-15T14:24:47.290437Z"
    },
    "id": "309nvG-V8Otr",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from env import Environment\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T14:24:49.799301Z",
     "iopub.status.busy": "2025-05-15T14:24:49.798637Z",
     "iopub.status.idle": "2025-05-15T14:24:49.806314Z",
     "shell.execute_reply": "2025-05-15T14:24:49.805561Z",
     "shell.execute_reply.started": "2025-05-15T14:24:49.799270Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 4, 0), (5, 3, 0)]\n",
      "[(1, 6, 5, 4, 3, 0, 26), (2, 2, 2, 2, 3, 0, 22), (3, 4, 5, 5, 6, 0, 22)]\n",
      "[[1, 1, 1, 1, 1, 1, 1], [1, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0, 1], [1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "env = Environment('map.txt', 10, 2, 5)\n",
    "state = env.reset()\n",
    "print(state[\"robots\"])\n",
    "print(state[\"packages\"])\n",
    "print(state[\"map\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T14:24:53.111866Z",
     "iopub.status.busy": "2025-05-15T14:24:53.111094Z",
     "iopub.status.idle": "2025-05-15T14:24:53.120962Z",
     "shell.execute_reply": "2025-05-15T14:24:53.120090Z",
     "shell.execute_reply.started": "2025-05-15T14:24:53.111834Z"
    },
    "id": "rq1hlk4b8Q37",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def convert_state(state, max_robots=20, max_packages=1000, device='cuda'):\n",
    "    \"\"\"\n",
    "    Convert raw state dict to input tensor dict for CNNPolicy.\n",
    "    The tensors are concatenated together as a single input for the model.\n",
    "    \"\"\"\n",
    "    # Convert map to (1, 1, H, W)\n",
    "    map_tensor = torch.tensor(state[\"map\"], dtype=torch.float32, device=device).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Process robots (each tuple has 3 values)\n",
    "    robots = state[\"robots\"]\n",
    "    robot_array = np.zeros((max_robots, 3), dtype=np.float32)\n",
    "    for i, robot in enumerate(robots[:max_robots]):\n",
    "        robot_array[i] = np.array(robot, dtype=np.float32)\n",
    "    robot_tensor = torch.tensor(robot_array.flatten(), device=device)  # shape (max_robots*3,)\n",
    "\n",
    "    # Process packages (each tuple has 7 values)\n",
    "    packages = state[\"packages\"]\n",
    "    package_array = np.zeros((max_packages, 7), dtype=np.float32)\n",
    "    for i, pkg in enumerate(packages[:max_packages]):\n",
    "        package_array[i] = np.array(pkg, dtype=np.float32)\n",
    "    package_tensor = torch.tensor(package_array.flatten(), device=device)  # shape (max_packages*7,)\n",
    "\n",
    "    # Time step as tensor (1,)\n",
    "    time_tensor = torch.tensor([state[\"time_step\"]], dtype=torch.float32, device=device)\n",
    "\n",
    "    # Concatenate all tensors\n",
    "    combined_input = torch.cat([\n",
    "        map_tensor.flatten(),  # Flatten map (1, 1, H, W) -> (H*W,)\n",
    "        robot_tensor,          # shape (max_robots*3,)\n",
    "        package_tensor,        # shape (max_packages*7,)\n",
    "        time_tensor            # shape (1,)\n",
    "    ], dim=-1).unsqueeze(0)  # Final shape: (1, H*W + max_robots*3 + max_packages*7 + 1)\n",
    "\n",
    "    return combined_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T14:25:14.499419Z",
     "iopub.status.busy": "2025-05-15T14:25:14.499105Z",
     "iopub.status.idle": "2025-05-15T14:25:14.508636Z",
     "shell.execute_reply": "2025-05-15T14:25:14.507946Z",
     "shell.execute_reply.started": "2025-05-15T14:25:14.499372Z"
    },
    "id": "7SHRHHeF8SjO",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def reward_shaping(original_reward, env: Environment, prev_state, actions):\n",
    "    shaped_reward = original_reward\n",
    "    shaping_factor = 0.5\n",
    "\n",
    "    for i, robot in enumerate(env.robots):\n",
    "        prev_pos = tuple(prev_state['robots'][i][:2])\n",
    "        curr_pos = robot.position\n",
    "        grid = env.load_map()  # Giả sử đây là 2D list (0: trống, 1: tường)\n",
    "\n",
    "        if robot.carrying:\n",
    "            pkg = env.packages[robot.carrying - 1]\n",
    "            d_prev = bfs_distance(prev_pos, pkg.target, grid)\n",
    "            d_curr = bfs_distance(curr_pos, pkg.target, grid)\n",
    "            if d_curr < d_prev:\n",
    "                shaped_reward += shaping_factor\n",
    "            else:\n",
    "                shaped_reward -= 0.2\n",
    "        else:\n",
    "            for pkg in env.packages:\n",
    "                if pkg.status == 'waiting':\n",
    "                    d_prev = bfs_distance(prev_pos, pkg.start, grid)\n",
    "                    d_curr = bfs_distance(curr_pos, pkg.start, grid)\n",
    "                    if d_curr < d_prev:\n",
    "                        shaped_reward += shaping_factor * 0.5\n",
    "                    break\n",
    "\n",
    "        if curr_pos == prev_pos:\n",
    "            shaped_reward -= 0.1\n",
    "\n",
    "        for pkg in env.packages:\n",
    "            if pkg.status == 'delivering' and pkg.picked_by == i:\n",
    "                waiting_time = pkg.pick_time - pkg.start_time if pkg.pick_time else 0\n",
    "                shaped_reward += max(0, 1.0 - 0.01 * waiting_time)\n",
    "\n",
    "    return shaped_reward\n",
    "def bfs_distance(start, goal, grid):\n",
    "    rows, cols = len(grid), len(grid[0])\n",
    "    visited = set()\n",
    "    queue = deque([(start, 0)])\n",
    "\n",
    "    while queue:\n",
    "        (x, y), dist = queue.popleft()\n",
    "        if (x, y) == goal:\n",
    "            return dist\n",
    "\n",
    "        for dx, dy in [(-1,0), (1,0), (0,-1), (0,1)]:\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if 0 <= nx < rows and 0 <= ny < cols:\n",
    "                if grid[nx][ny] == 0 and (nx, ny) not in visited:  # 0 là ô trống\n",
    "                    visited.add((nx, ny))\n",
    "                    queue.append(((nx, ny), dist + 1))\n",
    "    \n",
    "    return float('inf')  # Không tìm được đường đi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T14:25:17.286288Z",
     "iopub.status.busy": "2025-05-15T14:25:17.285997Z",
     "iopub.status.idle": "2025-05-15T14:25:17.292187Z",
     "shell.execute_reply": "2025-05-15T14:25:17.291624Z",
     "shell.execute_reply.started": "2025-05-15T14:25:17.286263Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 4, 0), (5, 3, 0)]\n",
      "[(1, 6, 5, 4, 3, 0, 26), (2, 2, 2, 2, 3, 0, 22), (3, 4, 5, 5, 6, 0, 22)]\n",
      "({'time_step': 1, 'map': [[1, 1, 1, 1, 1, 1, 1], [1, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0, 1], [1, 1, 1, 1, 1, 1, 1]], 'robots': [(5, 3, 0), (4, 3, 0)], 'packages': []}, -0.02, False, {})\n"
     ]
    }
   ],
   "source": [
    "env = Environment('map.txt', 10, 2, 5)\n",
    "state = env.reset()\n",
    "print(state[\"robots\"])\n",
    "print(state[\"packages\"])\n",
    "# print(state[\"map\"])\n",
    "\n",
    "# print(env.load_map())\n",
    "print(env.step([('L', 1),('U',0)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T14:25:50.821304Z",
     "iopub.status.busy": "2025-05-15T14:25:50.820992Z",
     "iopub.status.idle": "2025-05-15T14:25:50.828833Z",
     "shell.execute_reply": "2025-05-15T14:25:50.828108Z",
     "shell.execute_reply.started": "2025-05-15T14:25:50.821281Z"
    },
    "id": "kfrZJa4jG6yE",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Avoid to modify the Env class,\n",
    "# If it is neccessary, you should describe those changes clearly in report and code\n",
    "class Env(gym.Env):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Env, self).__init__()\n",
    "        self.env = Environment(*args, **kwargs)\n",
    "\n",
    "        self.action_space = spaces.multi_discrete.MultiDiscrete([5, 3]*self.env.n_robots)\n",
    "\n",
    "\n",
    "        self.prev_state = self.env.reset()\n",
    "        first_state=convert_state(self.prev_state)\n",
    "        # Define observation space as a dictionary\n",
    "\n",
    "        self.observation_space = spaces.Box(low=0, high=100, shape=first_state.shape, dtype=np.float32)\n",
    "\n",
    "\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        self.le1, self.le2= LabelEncoder(), LabelEncoder()\n",
    "        self.le1.fit(['S', 'L', 'R', 'U', 'D'])\n",
    "        self.le2.fit(['0','1', '2'])\n",
    "\n",
    "    def reset(self, *args, **kwargs):\n",
    "        self.prev_state = self.env.reset()\n",
    "        return convert_state(self.prev_state), {}\n",
    "\n",
    "    def render(self, *args, **kwargs):\n",
    "        return self.env.render()\n",
    "\n",
    "    def step(self, action):\n",
    "        ret = []\n",
    "        ret.append(self.le1.inverse_transform(action.reshape(-1, 2).T[0]))\n",
    "        ret.append(self.le2.inverse_transform(action.reshape(-1, 2).T[1]))\n",
    "        action = list(zip(*ret))\n",
    "\n",
    "        # You should not modify the infos object\n",
    "        s, r, done, infos = self.env.step(action)\n",
    "        new_r = reward_shaping(r, self.env, self.prev_state, action)\n",
    "        self.prev_state = s\n",
    "        return convert_state(s), new_r, \\\n",
    "            done, False, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "\n",
    "# ==== Hyperparameters ====\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "GAMMA = 0.99\n",
    "LR = 2.5e-4\n",
    "CLIP_EPS = 0.2\n",
    "UPDATE_EPOCHS = 4\n",
    "MAX_TIMESTEPS = 1000\n",
    "ENTROPY_COEF = 0.01\n",
    "VALUE_COEF = 0.5\n",
    "\n",
    "# ==== CNN Policy ====\n",
    "class CNNPolicy(nn.Module):\n",
    "    def __init__(self, act_dim):\n",
    "        super(CNNPolicy, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))  # Tạo output luôn có size (B, C, 1, 1)\n",
    "\n",
    "        self.fc1 = nn.Linear(64, 256)\n",
    "        self.fc2 = nn.Linear(256, act_dim)\n",
    "        self.value_head = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.conv1(state))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.global_pool(x)  # output: (B, 64, 1, 1)\n",
    "        x = x.squeeze(-1).squeeze(-1)  # output: (B, 64)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "\n",
    "        action_probs = torch.softmax(self.fc2(x), dim=-1)\n",
    "        state_value = self.value_head(x)\n",
    "        return action_probs, state_value\n",
    "\n",
    "\n",
    "# ==== Buffer ====\n",
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.states, self.actions, self.logprobs = [], [], []\n",
    "        self.rewards, self.dones, self.values = [], [], []\n",
    "\n",
    "    def clear(self):\n",
    "        self.__init__()\n",
    "\n",
    "# ==== PPO Agent ====\n",
    "class PPOAgent:\n",
    "    def __init__(self,act_dim):\n",
    "        self.policy = CNNPolicy(act_dim).to(DEVICE)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=LR)\n",
    "        self.buffer = RolloutBuffer()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state_3d = state.unsqueeze(0).unsqueeze(0)\n",
    "        # state_tensor = torch.FloatTensor(state).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "        probs, value = self.policy(state_3d)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "\n",
    "        self.buffer.states.append(state_3d)\n",
    "        self.buffer.actions.append(action)\n",
    "        self.buffer.logprobs.append(dist.log_prob(action))\n",
    "        self.buffer.values.append(value.squeeze())\n",
    "\n",
    "        return action.item()\n",
    "\n",
    "    def compute_returns_and_advantages(self, next_value):\n",
    "        returns, advs = [], []\n",
    "        gae = 0\n",
    "        values = self.buffer.values + [next_value]\n",
    "        for i in reversed(range(len(self.buffer.rewards))):\n",
    "            delta = self.buffer.rewards[i] + GAMMA * values[i + 1] * (1 - self.buffer.dones[i]) - values[i]\n",
    "            gae = delta + GAMMA * gae * (1 - self.buffer.dones[i])\n",
    "            advs.insert(0, gae)\n",
    "            returns.insert(0, gae + values[i])\n",
    "        return returns, advs\n",
    "\n",
    "    def update(self, next_value):\n",
    "        returns, advs = self.compute_returns_and_advantages(next_value)\n",
    "\n",
    "        states = torch.cat(self.buffer.states).to(DEVICE)\n",
    "        actions = torch.tensor(self.buffer.actions).to(DEVICE)\n",
    "        old_logprobs = torch.stack(self.buffer.logprobs).detach().to(DEVICE)\n",
    "        returns = torch.tensor(returns).detach().unsqueeze(1).to(DEVICE)\n",
    "        advs = torch.tensor(advs).detach().unsqueeze(1).to(DEVICE)\n",
    "\n",
    "        for _ in range(UPDATE_EPOCHS):\n",
    "            probs, values = self.policy(states)\n",
    "            dist = Categorical(probs)\n",
    "            logprobs = dist.log_prob(actions)\n",
    "            entropy = dist.entropy().mean()\n",
    "\n",
    "            ratio = torch.exp(logprobs - old_logprobs)\n",
    "            surr1 = ratio * advs\n",
    "            surr2 = torch.clamp(ratio, 1 - CLIP_EPS, 1 + CLIP_EPS) * advs\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            critic_loss = nn.MSELoss()(values, returns)\n",
    "\n",
    "            loss = actor_loss + VALUE_COEF * critic_loss - ENTROPY_COEF * entropy\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        self.buffer.clear()\n",
    "\n",
    "# ==== Training Loop ====\n",
    "def train(env):\n",
    "    obs_shape = env.observation_space.shape  # (H, W)\n",
    "    act_dim = int(np.prod(env.action_space.nvec))\n",
    "\n",
    "    agent = PPOAgent(act_dim)\n",
    "\n",
    "    for episode in range(10):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for t in range(MAX_TIMESTEPS):\n",
    "            action_flat = agent.select_action(state)\n",
    "            print(action_flat)\n",
    "            action = np.unravel_index(action_flat, env.action_space.nvec)\n",
    "            action = np.array(action)\n",
    "\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            agent.buffer.rewards.append(reward)\n",
    "            agent.buffer.dones.append(done)\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_state_tensor = next_state.unsqueeze(0).unsqueeze(0)\n",
    "            _, next_value = agent.policy(next_state_tensor)\n",
    "        agent.update(next_value.squeeze())\n",
    "        print(f\"Episode {episode}, Reward: {episode_reward}\")\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T14:26:03.000905Z",
     "iopub.status.busy": "2025-05-15T14:26:03.000419Z",
     "iopub.status.idle": "2025-05-15T14:26:27.471931Z",
     "shell.execute_reply": "2025-05-15T14:26:27.471254Z",
     "shell.execute_reply.started": "2025-05-15T14:26:03.000883Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "126381\n",
      "532055\n",
      "457905\n",
      "315133\n",
      "159436\n",
      "707697\n",
      "203187\n",
      "421280\n",
      "122151\n",
      "17843\n",
      "100828\n",
      "418593\n",
      "268933\n",
      "175739\n",
      "573221\n",
      "680143\n",
      "489842\n",
      "544234\n",
      "495906\n",
      "526937\n",
      "651024\n",
      "267410\n",
      "396176\n",
      "6758\n",
      "677667\n",
      "510727\n",
      "572351\n",
      "59510\n",
      "133945\n",
      "171678\n",
      "48370\n",
      "421637\n",
      "151592\n",
      "735567\n",
      "348736\n",
      "41701\n",
      "384294\n",
      "608294\n",
      "194444\n",
      "211570\n",
      "45564\n",
      "370620\n",
      "641121\n",
      "687926\n",
      "173749\n",
      "174803\n",
      "289855\n",
      "194869\n",
      "567231\n",
      "67765\n",
      "200326\n",
      "162303\n",
      "320127\n",
      "49607\n",
      "743674\n",
      "346534\n",
      "284657\n",
      "715131\n",
      "14409\n",
      "69605\n",
      "657914\n",
      "260405\n",
      "285437\n",
      "468340\n",
      "116039\n",
      "514507\n",
      "246254\n",
      "464569\n",
      "226258\n",
      "118011\n",
      "326781\n",
      "218568\n",
      "719004\n",
      "330586\n",
      "403383\n",
      "640816\n",
      "538744\n",
      "749320\n",
      "16274\n",
      "351407\n",
      "704479\n",
      "18892\n",
      "363201\n",
      "171536\n",
      "573223\n",
      "707640\n",
      "552374\n",
      "592835\n",
      "25966\n",
      "390250\n",
      "55\n",
      "466239\n",
      "401310\n",
      "288182\n",
      "480327\n",
      "533935\n",
      "330310\n",
      "390735\n",
      "281130\n",
      "267849\n",
      "Episode 0, Reward: -1.3300000000000027\n",
      "267310\n",
      "115487\n",
      "558763\n",
      "385633\n",
      "108915\n",
      "640946\n",
      "68783\n",
      "575436\n",
      "594262\n",
      "412217\n",
      "309029\n",
      "141186\n",
      "123588\n",
      "346769\n",
      "446309\n",
      "74471\n",
      "572318\n",
      "168131\n",
      "240803\n",
      "261835\n",
      "296914\n",
      "180429\n",
      "56082\n",
      "566706\n",
      "335097\n",
      "420219\n",
      "620846\n",
      "65292\n",
      "531438\n",
      "721244\n",
      "304328\n",
      "174770\n",
      "610628\n",
      "70392\n",
      "305310\n",
      "43865\n",
      "470698\n",
      "523775\n",
      "487933\n",
      "617745\n",
      "688450\n",
      "745826\n",
      "258963\n",
      "15825\n",
      "683503\n",
      "492678\n",
      "88145\n",
      "43115\n",
      "694839\n",
      "97902\n",
      "189684\n",
      "592350\n",
      "612817\n",
      "212865\n",
      "111096\n",
      "150739\n",
      "626629\n",
      "39023\n",
      "264565\n",
      "447208\n",
      "132851\n",
      "450731\n",
      "299077\n",
      "44961\n",
      "416509\n",
      "84041\n",
      "273719\n",
      "324782\n",
      "331996\n",
      "586717\n",
      "430973\n",
      "434411\n",
      "419377\n",
      "475066\n",
      "632426\n",
      "367241\n",
      "550030\n",
      "336284\n",
      "286097\n",
      "503958\n",
      "583328\n",
      "39025\n",
      "732900\n",
      "383737\n",
      "127811\n",
      "522380\n",
      "119766\n",
      "234338\n",
      "550780\n",
      "128491\n",
      "479465\n",
      "22745\n",
      "662348\n",
      "630007\n",
      "758721\n",
      "640048\n",
      "447135\n",
      "177744\n",
      "193541\n",
      "377785\n",
      "Episode 1, Reward: 66.82999999999998\n",
      "93561\n",
      "437363\n",
      "574637\n",
      "170159\n",
      "663895\n",
      "441013\n",
      "604200\n",
      "496037\n",
      "551726\n",
      "227248\n",
      "718403\n",
      "107563\n",
      "356766\n",
      "585763\n",
      "536782\n",
      "373340\n",
      "235415\n",
      "231369\n",
      "458811\n",
      "230835\n",
      "241234\n",
      "321690\n",
      "652659\n",
      "646534\n",
      "130372\n",
      "408199\n",
      "272241\n",
      "104623\n",
      "573248\n",
      "463066\n",
      "431951\n",
      "404613\n",
      "488695\n",
      "222333\n",
      "231814\n",
      "19756\n",
      "641348\n",
      "668665\n",
      "486987\n",
      "644726\n",
      "648764\n",
      "549441\n",
      "601943\n",
      "260854\n",
      "443433\n",
      "137892\n",
      "281242\n",
      "488277\n",
      "215254\n",
      "338024\n",
      "412341\n",
      "209033\n",
      "370084\n",
      "126653\n",
      "367758\n",
      "135499\n",
      "255938\n",
      "349501\n",
      "80330\n",
      "477209\n",
      "687851\n",
      "91672\n",
      "584703\n",
      "361248\n",
      "18878\n",
      "217472\n",
      "107859\n",
      "757399\n",
      "474013\n",
      "637374\n",
      "616676\n",
      "309475\n",
      "403660\n",
      "491809\n",
      "91867\n",
      "568933\n",
      "12778\n",
      "94392\n",
      "562025\n",
      "383052\n",
      "380688\n",
      "399893\n",
      "642566\n",
      "501916\n",
      "184606\n",
      "558292\n",
      "52345\n",
      "197412\n",
      "351340\n",
      "625905\n",
      "143737\n",
      "386093\n",
      "418119\n",
      "162268\n",
      "640391\n",
      "441891\n",
      "111182\n",
      "25184\n",
      "701636\n",
      "179983\n",
      "Episode 2, Reward: 27.229999999999997\n",
      "119547\n",
      "93740\n",
      "63962\n",
      "44869\n",
      "661315\n",
      "612840\n",
      "280374\n",
      "562922\n",
      "653280\n",
      "71681\n",
      "489644\n",
      "70911\n",
      "266646\n",
      "620842\n",
      "675112\n",
      "534264\n",
      "9812\n",
      "430223\n",
      "413338\n",
      "2173\n",
      "551618\n",
      "695132\n",
      "198566\n",
      "213128\n",
      "755219\n",
      "322764\n",
      "613210\n",
      "171688\n",
      "112739\n",
      "481916\n",
      "522905\n",
      "131689\n",
      "384053\n",
      "531388\n",
      "325634\n",
      "643662\n",
      "122991\n",
      "89834\n",
      "485108\n",
      "553944\n",
      "65595\n",
      "106354\n",
      "705990\n",
      "331956\n",
      "272234\n",
      "409550\n",
      "641617\n",
      "173066\n",
      "550003\n",
      "740161\n",
      "492930\n",
      "193248\n",
      "351509\n",
      "500783\n",
      "685543\n",
      "29786\n",
      "344083\n",
      "428849\n",
      "6138\n",
      "679512\n",
      "600424\n",
      "597687\n",
      "591804\n",
      "435954\n",
      "497163\n",
      "81465\n",
      "380094\n",
      "436463\n",
      "86982\n",
      "452467\n",
      "367607\n",
      "37345\n",
      "436357\n",
      "677225\n",
      "671833\n",
      "354196\n",
      "309009\n",
      "247617\n",
      "205738\n",
      "384369\n",
      "376525\n",
      "271486\n",
      "202038\n",
      "519937\n",
      "293087\n",
      "142006\n",
      "528123\n",
      "295754\n",
      "706250\n",
      "387303\n",
      "118173\n",
      "34971\n",
      "413610\n",
      "593429\n",
      "47986\n",
      "2595\n",
      "495087\n",
      "540601\n",
      "75945\n",
      "388369\n",
      "Episode 3, Reward: 41.09\n",
      "9514\n",
      "722066\n",
      "207372\n",
      "107800\n",
      "134009\n",
      "548518\n",
      "559805\n",
      "225070\n",
      "575577\n",
      "567740\n",
      "116400\n",
      "603149\n",
      "386793\n",
      "434231\n",
      "400001\n",
      "247118\n",
      "608230\n",
      "70171\n",
      "300995\n",
      "670996\n",
      "146289\n",
      "735797\n",
      "591019\n",
      "290088\n",
      "578215\n",
      "116595\n",
      "41861\n",
      "211847\n",
      "537471\n",
      "321473\n",
      "637304\n",
      "504246\n",
      "221927\n",
      "283930\n",
      "501701\n",
      "567776\n",
      "599101\n",
      "99340\n",
      "548956\n",
      "116854\n",
      "592350\n",
      "330080\n",
      "200314\n",
      "6362\n",
      "583751\n",
      "689786\n",
      "87969\n",
      "655041\n",
      "515120\n",
      "456959\n",
      "66714\n",
      "223529\n",
      "273943\n",
      "617518\n",
      "176438\n",
      "617132\n",
      "532539\n",
      "5898\n",
      "554003\n",
      "728833\n",
      "276945\n",
      "421072\n",
      "38242\n",
      "388445\n",
      "643284\n",
      "112014\n",
      "700714\n",
      "328241\n",
      "257361\n",
      "697140\n",
      "44419\n",
      "315039\n",
      "581035\n",
      "494230\n",
      "144602\n",
      "466697\n",
      "268257\n",
      "281179\n",
      "595616\n",
      "232979\n",
      "407510\n",
      "2386\n",
      "9046\n",
      "494000\n",
      "277918\n",
      "2559\n",
      "28131\n",
      "246068\n",
      "300788\n",
      "293412\n",
      "528458\n",
      "281805\n",
      "274028\n",
      "494450\n",
      "324756\n",
      "606366\n",
      "350025\n",
      "65667\n",
      "469479\n",
      "598396\n",
      "Episode 4, Reward: 50.32000000000003\n",
      "84210\n",
      "298854\n",
      "260888\n",
      "124148\n",
      "445153\n",
      "103238\n",
      "67218\n",
      "177055\n",
      "477898\n",
      "383144\n",
      "282313\n",
      "477916\n",
      "520054\n",
      "157498\n",
      "579881\n",
      "694600\n",
      "685486\n",
      "381330\n",
      "656682\n",
      "387056\n",
      "626919\n",
      "420359\n",
      "512266\n",
      "587207\n",
      "398360\n",
      "401389\n",
      "538867\n",
      "645659\n",
      "423915\n",
      "467598\n",
      "63088\n",
      "367246\n",
      "123362\n",
      "574511\n",
      "571891\n",
      "265540\n",
      "82541\n",
      "60290\n",
      "402992\n",
      "131522\n",
      "543488\n",
      "129428\n",
      "80128\n",
      "117396\n",
      "59402\n",
      "374720\n",
      "748354\n",
      "409133\n",
      "77895\n",
      "93818\n",
      "243602\n",
      "417203\n",
      "422151\n",
      "295785\n",
      "531952\n",
      "247369\n",
      "105125\n",
      "428865\n",
      "357358\n",
      "518702\n",
      "31123\n",
      "168227\n",
      "373073\n",
      "545297\n",
      "591329\n",
      "103550\n",
      "409893\n",
      "253725\n",
      "88258\n",
      "546379\n",
      "182671\n",
      "435566\n",
      "324115\n",
      "18660\n",
      "313745\n",
      "558335\n",
      "683597\n",
      "322468\n",
      "725573\n",
      "340876\n",
      "344828\n",
      "755533\n",
      "511551\n",
      "717259\n",
      "83516\n",
      "137624\n",
      "327892\n",
      "487060\n",
      "30327\n",
      "706313\n",
      "298289\n",
      "418627\n",
      "429156\n",
      "248463\n",
      "51037\n",
      "20660\n",
      "325501\n",
      "457883\n",
      "653804\n",
      "229872\n",
      "Episode 5, Reward: 110.02000000000001\n",
      "206819\n",
      "137885\n",
      "462131\n",
      "404451\n",
      "37416\n",
      "711105\n",
      "313402\n",
      "733928\n",
      "120668\n",
      "466440\n",
      "245929\n",
      "493563\n",
      "443649\n",
      "463484\n",
      "602296\n",
      "566189\n",
      "527603\n",
      "415264\n",
      "85229\n",
      "486384\n",
      "397459\n",
      "727346\n",
      "127626\n",
      "237706\n",
      "732571\n",
      "740771\n",
      "119223\n",
      "452211\n",
      "540178\n",
      "221773\n",
      "240621\n",
      "281291\n",
      "505154\n",
      "755433\n",
      "274728\n",
      "40620\n",
      "509529\n",
      "20149\n",
      "206530\n",
      "256866\n",
      "125887\n",
      "240696\n",
      "257317\n",
      "276998\n",
      "525688\n",
      "412001\n",
      "527685\n",
      "173817\n",
      "126205\n",
      "577206\n",
      "106580\n",
      "422664\n",
      "22740\n",
      "449971\n",
      "666195\n",
      "172161\n",
      "620553\n",
      "380038\n",
      "412941\n",
      "55180\n",
      "540080\n",
      "141908\n",
      "152116\n",
      "658516\n",
      "375905\n",
      "348285\n",
      "647814\n",
      "86334\n",
      "48358\n",
      "405584\n",
      "180809\n",
      "568059\n",
      "603488\n",
      "331204\n",
      "643580\n",
      "29034\n",
      "584529\n",
      "619645\n",
      "549264\n",
      "535715\n",
      "705176\n",
      "698899\n",
      "215965\n",
      "59712\n",
      "64668\n",
      "5119\n",
      "578616\n",
      "288123\n",
      "228023\n",
      "521597\n",
      "67584\n",
      "611569\n",
      "732088\n",
      "602512\n",
      "250339\n",
      "533054\n",
      "371133\n",
      "713342\n",
      "434780\n",
      "178521\n",
      "Episode 6, Reward: 46.58000000000002\n",
      "568428\n",
      "367281\n",
      "454739\n",
      "717224\n",
      "639751\n",
      "435397\n",
      "141656\n",
      "52850\n",
      "224690\n",
      "414529\n",
      "661157\n",
      "25334\n",
      "257984\n",
      "428678\n",
      "589703\n",
      "746542\n",
      "405381\n",
      "377084\n",
      "672141\n",
      "175863\n",
      "391517\n",
      "406287\n",
      "421728\n",
      "736440\n",
      "496426\n",
      "128441\n",
      "673283\n",
      "168790\n",
      "505134\n",
      "623505\n",
      "380777\n",
      "29969\n",
      "276756\n",
      "620220\n",
      "87812\n",
      "206107\n",
      "5249\n",
      "599126\n",
      "209910\n",
      "25216\n",
      "240285\n",
      "148728\n",
      "106214\n",
      "98491\n",
      "265684\n",
      "476288\n",
      "159482\n",
      "70381\n",
      "249964\n",
      "610541\n",
      "285864\n",
      "519115\n",
      "474126\n",
      "229059\n",
      "482440\n",
      "652204\n",
      "657766\n",
      "61619\n",
      "700923\n",
      "511908\n",
      "376122\n",
      "337302\n",
      "138288\n",
      "598892\n",
      "75813\n",
      "461551\n",
      "573337\n",
      "695828\n",
      "351111\n",
      "13151\n",
      "158947\n",
      "152683\n",
      "311531\n",
      "275566\n",
      "475954\n",
      "500943\n",
      "170355\n",
      "18090\n",
      "19181\n",
      "283730\n",
      "243477\n",
      "243455\n",
      "418776\n",
      "700888\n",
      "653513\n",
      "117073\n",
      "122483\n",
      "179018\n",
      "615314\n",
      "523030\n",
      "265883\n",
      "40452\n",
      "564417\n",
      "403634\n",
      "435434\n",
      "314357\n",
      "478711\n",
      "437285\n",
      "270182\n",
      "180015\n",
      "Episode 7, Reward: 44.80000000000002\n",
      "690192\n",
      "358774\n",
      "474847\n",
      "526776\n",
      "617387\n",
      "399924\n",
      "197737\n",
      "154637\n",
      "506389\n",
      "584982\n",
      "186756\n",
      "300660\n",
      "148961\n",
      "577086\n",
      "550489\n",
      "719048\n",
      "183876\n",
      "654865\n",
      "237181\n",
      "433398\n",
      "677097\n",
      "575319\n",
      "143800\n",
      "148952\n",
      "411201\n",
      "40992\n",
      "76718\n",
      "742665\n",
      "624547\n",
      "321761\n",
      "3557\n",
      "536161\n",
      "371080\n",
      "35797\n",
      "175499\n",
      "576360\n",
      "293296\n",
      "497523\n",
      "621902\n",
      "166249\n",
      "584533\n",
      "179431\n",
      "451182\n",
      "758900\n",
      "395667\n",
      "193215\n",
      "81701\n",
      "123938\n",
      "612605\n",
      "639310\n",
      "651474\n",
      "685987\n",
      "532134\n",
      "476034\n",
      "585419\n",
      "633508\n",
      "260458\n",
      "145169\n",
      "3468\n",
      "86048\n",
      "577364\n",
      "446640\n",
      "528062\n",
      "638545\n",
      "332868\n",
      "611418\n",
      "271174\n",
      "668733\n",
      "146564\n",
      "646251\n",
      "562281\n",
      "710470\n",
      "536428\n",
      "165552\n",
      "233529\n",
      "490882\n",
      "282870\n",
      "39885\n",
      "45228\n",
      "97479\n",
      "613765\n",
      "453584\n",
      "334355\n",
      "528599\n",
      "744774\n",
      "51445\n",
      "561237\n",
      "695893\n",
      "316838\n",
      "414177\n",
      "335962\n",
      "201199\n",
      "431792\n",
      "63180\n",
      "541852\n",
      "419655\n",
      "733895\n",
      "593292\n",
      "557165\n",
      "56219\n",
      "Episode 8, Reward: 75.00000000000001\n",
      "227115\n",
      "125852\n",
      "297062\n",
      "449937\n",
      "224612\n",
      "641740\n",
      "33796\n",
      "248780\n",
      "686687\n",
      "493865\n",
      "672879\n",
      "507554\n",
      "694625\n",
      "477282\n",
      "335595\n",
      "596921\n",
      "514944\n",
      "255397\n",
      "270345\n",
      "569911\n",
      "303984\n",
      "360172\n",
      "510239\n",
      "259952\n",
      "120887\n",
      "585092\n",
      "410671\n",
      "414494\n",
      "692388\n",
      "264191\n",
      "153850\n",
      "316466\n",
      "37602\n",
      "569928\n",
      "668797\n",
      "318305\n",
      "529833\n",
      "180247\n",
      "388804\n",
      "595496\n",
      "392264\n",
      "696573\n",
      "26799\n",
      "464710\n",
      "242876\n",
      "603715\n",
      "615273\n",
      "522398\n",
      "463777\n",
      "241331\n",
      "270643\n",
      "657813\n",
      "34398\n",
      "486049\n",
      "709317\n",
      "323633\n",
      "474314\n",
      "329527\n",
      "677475\n",
      "186539\n",
      "264103\n",
      "711875\n",
      "154368\n",
      "283780\n",
      "167569\n",
      "517343\n",
      "75936\n",
      "49461\n",
      "458805\n",
      "376393\n",
      "413910\n",
      "73534\n",
      "510880\n",
      "223933\n",
      "535623\n",
      "38260\n",
      "539664\n",
      "142005\n",
      "689932\n",
      "443512\n",
      "616940\n",
      "565360\n",
      "492355\n",
      "76464\n",
      "129870\n",
      "32584\n",
      "332916\n",
      "628378\n",
      "501117\n",
      "185523\n",
      "688861\n",
      "209083\n",
      "480271\n",
      "331770\n",
      "177901\n",
      "213375\n",
      "379819\n",
      "159251\n",
      "719567\n",
      "313087\n",
      "Episode 9, Reward: 52.980000000000004\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "env = Env('map2.txt', 100, 5, 20, -0.01, 10., 1., 10)\n",
    "\n",
    "state = env.reset()\n",
    "print(len(state))\n",
    "model = train(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T14:31:11.129029Z",
     "iopub.status.busy": "2025-05-14T14:31:11.128386Z",
     "iopub.status.idle": "2025-05-14T14:31:11.134733Z",
     "shell.execute_reply": "2025-05-14T14:31:11.133912Z",
     "shell.execute_reply.started": "2025-05-14T14:31:11.129006Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate(agent, env, episodes=10, render=False):\n",
    "    total_rewards = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for _ in range(MAX_TIMESTEPS):\n",
    "            # Không cần lưu logprob hay value\n",
    "            state_tensor = state.unsqueeze(0).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                action_probs, _ = agent.policy(state_tensor)\n",
    "            action = torch.argmax(action_probs, dim=-1).item()\n",
    "            action = np.unravel_index(action, env.action_space.nvec)\n",
    "            action = np.array(action)\n",
    "            print(action)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        total_rewards.append(episode_reward)\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    return avg_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T14:31:15.888079Z",
     "iopub.status.busy": "2025-05-14T14:31:15.887331Z",
     "iopub.status.idle": "2025-05-14T14:31:21.454162Z",
     "shell.execute_reply": "2025-05-14T14:31:21.453343Z",
     "shell.execute_reply.started": "2025-05-14T14:31:15.888052Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "2.3760000000000088\n"
     ]
    }
   ],
   "source": [
    "eval_env = Env('map1.txt', 100, 5, 100, -0.01, 10., 1., 10)\n",
    "model.policy.eval()\n",
    "ev = evaluate(model, eval_env, render=False)\n",
    "print(ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.status.busy": "2025-05-14T14:12:10.947589Z",
     "iopub.status.idle": "2025-05-14T14:12:10.947977Z",
     "shell.execute_reply": "2025-05-14T14:12:10.947825Z",
     "shell.execute_reply.started": "2025-05-14T14:12:10.947809Z"
    },
    "id": "WVpXVAz8Kn9C",
    "outputId": "da85df6f-1219-444b-eeac-0bcf9bd6bf83",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip freeze | grep stable_baselines3"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
