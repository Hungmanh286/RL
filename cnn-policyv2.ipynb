{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AM3r0wPe5N0K"
   },
   "source": [
    "Solving Package delivery using single-agent PPO with a naive feature representation learning: concatenante all the feature in to a single state vector, and multiple robot actions as a multi discrete distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T14:24:00.824019Z",
     "iopub.status.busy": "2025-05-15T14:24:00.823838Z",
     "iopub.status.idle": "2025-05-15T14:24:01.473592Z",
     "shell.execute_reply": "2025-05-15T14:24:01.472651Z",
     "shell.execute_reply.started": "2025-05-15T14:24:00.824001Z"
    },
    "id": "9Ro5mHQ3GnN8",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'marl-delivery'\n",
      "/home/hungmanh/home_work/RL/marl-delivery\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "# !git clone https://github.com/cuongtv312/marl-delivery.git\n",
    "%cd marl-delivery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T14:24:47.290464Z",
     "iopub.status.busy": "2025-05-15T14:24:47.289639Z",
     "iopub.status.idle": "2025-05-15T14:24:47.293858Z",
     "shell.execute_reply": "2025-05-15T14:24:47.293254Z",
     "shell.execute_reply.started": "2025-05-15T14:24:47.290437Z"
    },
    "id": "309nvG-V8Otr",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from env import Environment\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T14:24:53.111866Z",
     "iopub.status.busy": "2025-05-15T14:24:53.111094Z",
     "iopub.status.idle": "2025-05-15T14:24:53.120962Z",
     "shell.execute_reply": "2025-05-15T14:24:53.120090Z",
     "shell.execute_reply.started": "2025-05-15T14:24:53.111834Z"
    },
    "id": "rq1hlk4b8Q37",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MAX_PACKAGES = 100  # Số lượng gói hàng tối đa bạn muốn hỗ trợ\n",
    "\n",
    "def convert_state(state):\n",
    "    current_time = state.get(\"time_step\", 0)\n",
    "    map_tensor = np.array(state[\"map\"], dtype=np.float32)\n",
    "\n",
    "    # Robot features: (x, y, status)\n",
    "    robot_features = []\n",
    "    for robot_x, robot_y, status in state[\"robots\"]:\n",
    "        robot_features.extend([robot_x, robot_y, float(status)])\n",
    "\n",
    "    # Package features: (pickup_x, pickup_y, dropoff_x, dropoff_y, appear_time, deadline, is_active)\n",
    "    package_features = []\n",
    "    for pkg in sorted(state[\"packages\"], key=lambda p: (p[3], p[0])):\n",
    "        _, px, py, dx, dy, t_appear, t_deadline = pkg\n",
    "        is_active = 1.0 if (current_time >= t_appear and current_time < t_deadline) else 0.0\n",
    "        package_features.extend([px, py, dx, dy, t_appear, t_deadline, is_active])\n",
    "\n",
    "    # Padding cho đủ MAX_PACKAGES\n",
    "    n_pkgs = len(state[\"packages\"])\n",
    "    pkg_feature_len = 7\n",
    "    if n_pkgs < MAX_PACKAGES:\n",
    "        package_features += [0.0] * ((MAX_PACKAGES - n_pkgs) * pkg_feature_len)\n",
    "\n",
    "    feature_vector = np.array(\n",
    "        robot_features + package_features + [float(current_time)],\n",
    "        dtype=np.float32\n",
    "    )\n",
    "\n",
    "    return map_tensor, feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T14:25:14.499419Z",
     "iopub.status.busy": "2025-05-15T14:25:14.499105Z",
     "iopub.status.idle": "2025-05-15T14:25:14.508636Z",
     "shell.execute_reply": "2025-05-15T14:25:14.507946Z",
     "shell.execute_reply.started": "2025-05-15T14:25:14.499372Z"
    },
    "id": "7SHRHHeF8SjO",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def reward_shaping(r, env, state, action):\n",
    "    additional = 0\n",
    "    for robot in env.robots:\n",
    "        if robot.carrying:\n",
    "            pkg = env.packages[robot.carrying-1]\n",
    "            # Thưởng theo khoảng cách tới đích\n",
    "            dist = np.linalg.norm(np.array(robot.position) - np.array(pkg.target))\n",
    "            additional += 0.1 * (1 - dist/env.n_rows)\n",
    "        else:\n",
    "            # Khuyến khích đi gần điểm xuất phát của package\n",
    "            for p in env.packages:\n",
    "                if p.status == 'waiting' and p.start_time <= env.t:\n",
    "                    dist = np.linalg.norm(np.array(robot.position) - np.array(p.start))\n",
    "                    additional += 0.05 * (1 - dist/10) if dist < 5 else 0\n",
    "    \n",
    "    # Phạt hành động di chuyển không cần thiết\n",
    "    for act in action:\n",
    "        if act[0] in ['L','R','U','D'] and not robot.carrying:\n",
    "            additional -= 0.01\n",
    "    \n",
    "    return r + additional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T14:25:50.821304Z",
     "iopub.status.busy": "2025-05-15T14:25:50.820992Z",
     "iopub.status.idle": "2025-05-15T14:25:50.828833Z",
     "shell.execute_reply": "2025-05-15T14:25:50.828108Z",
     "shell.execute_reply.started": "2025-05-15T14:25:50.821281Z"
    },
    "id": "kfrZJa4jG6yE",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Avoid to modify the Env class,\n",
    "# If it is neccessary, you should describe those changes clearly in report and code\n",
    "class Env(gym.Env):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Env, self).__init__()\n",
    "        self.env = Environment(*args, **kwargs)\n",
    "\n",
    "        self.action_space = spaces.multi_discrete.MultiDiscrete([5, 3]*self.env.n_robots)\n",
    "        self.n_agents = self.env.n_robots \n",
    "\n",
    "        self.prev_state = self.env.reset()\n",
    "        map, feature=convert_state(self.prev_state)\n",
    "        # Define observation space as a dictionary\n",
    "\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"map\": spaces.Box(low=0, high=100, shape=map.shape, dtype=np.float32),\n",
    "            \"feature\": spaces.Box(low=0, high=100, shape=feature.shape, dtype=np.float32)\n",
    "        })\n",
    "\n",
    "\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        self.le1, self.le2= LabelEncoder(), LabelEncoder()\n",
    "        self.le1.fit(['S', 'L', 'R', 'U', 'D'])\n",
    "        self.le2.fit(['0','1', '2'])\n",
    "\n",
    "    def reset(self, *args, **kwargs):\n",
    "        self.prev_state = self.env.reset()\n",
    "        return convert_state(self.prev_state), {}\n",
    "\n",
    "    def render(self, *args, **kwargs):\n",
    "        return self.env.render()\n",
    "\n",
    "    def step(self, action):\n",
    "        ret = []\n",
    "        ret.append(self.le1.inverse_transform(action.reshape(-1, 2).T[0]))\n",
    "        ret.append(self.le2.inverse_transform(action.reshape(-1, 2).T[1]))\n",
    "        action = list(zip(*ret))\n",
    "\n",
    "        # You should not modify the infos object\n",
    "        s, r, done, infos = self.env.step(action)\n",
    "        new_r = reward_shaping(r, self.env, self.prev_state, action)\n",
    "        self.prev_state = s\n",
    "        return convert_state(s), new_r, \\\n",
    "            done, False, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict('feature': Box(0.0, 100.0, (58,), float32), 'map': Box(0.0, 100.0, (20, 20), float32))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Env('map2.txt', 100, 5, 20, -0.01, 10., 1., 10)\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32), array([ 8.,  8.,  0.,  6.,  8.,  0.,  2.,  7.,  0.,  9.,  4.,  0.,  4.,\n",
      "        4.,  0.,  2.,  3.,  3.,  3.,  0., 34.,  1.,  9.,  2.,  3.,  3.,\n",
      "        0., 17.,  1.,  9.,  7.,  5.,  4.,  0., 25.,  1.,  3.,  7.,  6.,\n",
      "        3.,  0., 22.,  1.,  8.,  6.,  7.,  8.,  0., 21.,  1.,  6.,  8.,\n",
      "        8.,  8.,  0., 16.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "        0.], dtype=float32)), {})\n"
     ]
    }
   ],
   "source": [
    "# env = Env('map1.txt', 1000, 5, 100,-0.01, 10., 1., 10)\n",
    "eval_env = Env('map2.txt',1000, 5, 100,-0.01, 10., 1., 10)\n",
    "\n",
    "\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, _, info = eval_env.step(action)\n",
    "    #print('='*10)\n",
    "    #eval_env.unwrapped.env.render()\n",
    "    if dones:\n",
    "        break\n",
    "\n",
    "print(info)\n",
    "\n",
    "state_convert = env.reset()\n",
    "\n",
    "print(state_convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import random\n",
    "from collections import deque\n",
    " \n",
    "# CNN cho việc xử lý map tensor\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, input_channels=1, map_size=20):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Tính toán kích thước đầu ra sau các lớp conv\n",
    "        output_size = map_size // 2  # Sau stride=2 ở conv3\n",
    "        self.fc = nn.Linear(64 * output_size * output_size, 256)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, 1, map_size, map_size]\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = F.relu(self.fc(x))\n",
    "        return x\n",
    "\n",
    "# MLP cho việc xử lý feature vector\n",
    "class MLPEncoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLPEncoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 256)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Actor network\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, map_size, feature_dim, n_move_actions, n_status_actions, n_agents):\n",
    "        super(Actor, self).__init__()\n",
    "        self.cnn_encoder = CNNEncoder(input_channels=1, map_size=map_size)\n",
    "        self.mlp_encoder = MLPEncoder(feature_dim)\n",
    "        self.fc_combine = nn.Linear(256 + 256, 256)\n",
    "        self.move_heads = nn.ModuleList([nn.Linear(256, n_move_actions) for _ in range(n_agents)])\n",
    "        self.status_heads = nn.ModuleList([nn.Linear(256, n_status_actions) for _ in range(n_agents)])\n",
    "\n",
    "    def forward(self, map_tensor, feature_vector):\n",
    "        batch_size = map_tensor.size(0)\n",
    "        map_tensor = map_tensor.unsqueeze(1)\n",
    "        map_features = self.cnn_encoder(map_tensor)\n",
    "        feature_features = self.mlp_encoder(feature_vector)\n",
    "        combined = torch.cat([map_features, feature_features], dim=1)\n",
    "        combined = F.relu(self.fc_combine(combined))\n",
    "        move_probs = [F.softmax(head(combined), dim=-1) for head in self.move_heads]\n",
    "        status_probs = [F.softmax(head(combined), dim=-1) for head in self.status_heads]\n",
    "        return move_probs, status_probs\n",
    "\n",
    "# Critic network\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, map_size, feature_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.cnn_encoder = CNNEncoder(input_channels=1, map_size=map_size)\n",
    "        self.mlp_encoder = MLPEncoder(feature_dim)\n",
    "        \n",
    "        # Kết hợp đầu ra của CNN và MLP\n",
    "        self.fc_combine = nn.Linear(256 + 256, 256)\n",
    "        self.fc_value = nn.Linear(256, 1)\n",
    "    \n",
    "    def forward(self, map_tensor, feature_vector):\n",
    "        # Xử lý map qua CNN\n",
    "        map_tensor = map_tensor.unsqueeze(1)  # Thêm kênh input\n",
    "        map_features = self.cnn_encoder(map_tensor)\n",
    "        \n",
    "        # Xử lý feature vector qua MLP\n",
    "        feature_features = self.mlp_encoder(feature_vector)\n",
    "        \n",
    "        # Kết hợp đặc trưng\n",
    "        combined = torch.cat([map_features, feature_features], dim=1)\n",
    "        combined = F.relu(self.fc_combine(combined))\n",
    "        \n",
    "        # Tính toán giá trị\n",
    "        value = self.fc_value(combined)\n",
    "        \n",
    "        return value\n",
    "\n",
    "# MAPPO Agent\n",
    "class MAPPOAgent:\n",
    "    def __init__(self, map_size, feature_dim, n_move_actions, n_status_actions,n_agents,\n",
    "                 actor_lr=3e-4, critic_lr=1e-3, gamma=0.99, \n",
    "                 gae_lambda=0.95, clip_param=0.2, value_coef=0.5, \n",
    "                 entropy_coef=0.01, max_grad_norm=0.5):\n",
    "        \n",
    "        self.actor = Actor(map_size, feature_dim, n_move_actions, n_status_actions,n_agents)\n",
    "        self.critic = Critic(map_size, feature_dim)\n",
    "        \n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.clip_param = clip_param\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        \n",
    "        self.n_agents = n_agents\n",
    "        self.n_move_actions = n_move_actions\n",
    "        self.n_status_actions = n_status_actions\n",
    "    \n",
    "    def get_action(self, map_tensor, feature_vector):\n",
    "        map_tensor = torch.FloatTensor(map_tensor).unsqueeze(0)\n",
    "        feature_vector = torch.FloatTensor(feature_vector).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            move_probs, status_probs = self.actor(map_tensor, feature_vector)\n",
    "            value = self.critic(map_tensor, feature_vector)\n",
    "        actions = []\n",
    "        log_probs = []\n",
    "        for move_p, status_p in zip(move_probs, status_probs):\n",
    "            move_dist = Categorical(move_p)\n",
    "            status_dist = Categorical(status_p)\n",
    "            move_a = move_dist.sample()\n",
    "            status_a = status_dist.sample()\n",
    "            actions.append([move_a.item(), status_a.item()])\n",
    "            log_probs.append(move_dist.log_prob(move_a).item() + status_dist.log_prob(status_a).item())\n",
    "        return np.array(actions), log_probs, value.item()\n",
    "    \n",
    "    def evaluate_actions(self, map_tensors, feature_vectors, actions):\n",
    "        move_probs, status_probs = self.actor(map_tensors, feature_vectors)\n",
    "        values = self.critic(map_tensors, feature_vectors)\n",
    "\n",
    "        action_log_probs = []\n",
    "        entropy = 0\n",
    "\n",
    "        # actions shape: [batch, n_agents, 2]\n",
    "        # Tách move và status action cho từng agent\n",
    "        move_actions = actions[:, :, 0]\n",
    "        status_actions = actions[:, :, 1]\n",
    "\n",
    "        for i in range(self.n_agents):\n",
    "            move_dist = Categorical(move_probs[i])\n",
    "            status_dist = Categorical(status_probs[i])\n",
    "            move_log_prob = move_dist.log_prob(move_actions[:, i])\n",
    "            status_log_prob = status_dist.log_prob(status_actions[:, i])\n",
    "            action_log_probs.append(move_log_prob + status_log_prob)\n",
    "            entropy += (move_dist.entropy().mean() + status_dist.entropy().mean()) / 2\n",
    "\n",
    "        action_log_probs = torch.stack(action_log_probs, dim=1)  # [batch, n_agents]\n",
    "\n",
    "        return values, action_log_probs, entropy / self.n_agents\n",
    "    \n",
    "    def update(self, memories):\n",
    "        # Trích xuất dữ liệu từ bộ nhớ\n",
    "        maps = []\n",
    "        features = []\n",
    "        actions = []\n",
    "        old_log_probs = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        values = []\n",
    "        \n",
    "        for memory in memories:\n",
    "            maps.append(torch.FloatTensor(np.array(memory.maps)))\n",
    "            features.append(torch.FloatTensor(np.array(memory.features)))\n",
    "            actions.append(torch.LongTensor(np.array(memory.actions)))\n",
    "            old_log_probs.append(torch.FloatTensor(np.array(memory.log_probs)))\n",
    "            rewards.append(torch.FloatTensor(np.array(memory.rewards)))\n",
    "            masks.append(torch.FloatTensor(np.array(memory.masks)))\n",
    "            values.append(torch.FloatTensor(np.array(memory.values)))\n",
    "        \n",
    "        maps = torch.cat(maps)\n",
    "        features = torch.cat(features)\n",
    "        actions = torch.cat(actions)\n",
    "        old_log_probs = torch.cat(old_log_probs)\n",
    "        rewards = torch.cat(rewards)\n",
    "        masks = torch.cat(masks)\n",
    "        values = torch.cat(values)\n",
    "        \n",
    "        # Tính toán returns và advantages sử dụng GAE\n",
    "        returns = torch.zeros_like(rewards)\n",
    "        advantages = torch.zeros_like(rewards)\n",
    "        \n",
    "        last_value = self.critic(maps[-1].unsqueeze(0), features[-1].unsqueeze(0)).detach()\n",
    "        last_gae_lam = 0\n",
    "        \n",
    "        for t in reversed(range(len(rewards))):\n",
    "            if t == len(rewards) - 1:\n",
    "                next_value = last_value\n",
    "            else:\n",
    "                next_value = values[t + 1]\n",
    "            \n",
    "            next_non_terminal = masks[t]\n",
    "            delta = rewards[t] + self.gamma * next_value * next_non_terminal - values[t]\n",
    "            last_gae_lam = delta + self.gamma * self.gae_lambda * next_non_terminal * last_gae_lam\n",
    "            advantages[t] = last_gae_lam\n",
    "        \n",
    "        returns = advantages + values\n",
    "        \n",
    "        # Chuẩn hóa lợi thế\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        # Đảm bảo shape [batch, 1]\n",
    "        if advantages.dim() == 1:\n",
    "            advantages = advantages.unsqueeze(1)\n",
    "        # Lặp lại cho từng agent để có shape [batch, n_agents]\n",
    "        advantages = advantages.expand(-1, self.n_agents)\n",
    "\n",
    "        # Tối ưu hóa policy and value networks\n",
    "        for _ in range(10):  # K epochs\n",
    "            values, action_log_probs, entropy = self.evaluate_actions(maps, features, actions)\n",
    "            # action_log_probs: [batch, n_agents], old_log_probs: [batch, n_agents]\n",
    "            ratios = torch.exp(action_log_probs - old_log_probs)\n",
    "            # Surrogate loss\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1.0 - self.clip_param, 1.0 + self.clip_param) * advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            value_loss = F.mse_loss(values, returns.unsqueeze(-1))\n",
    "            entropy_loss = -entropy.mean()\n",
    "            \n",
    "            # Tổng loss\n",
    "            loss = policy_loss + self.value_coef * value_loss + self.entropy_coef * entropy_loss\n",
    "            \n",
    "            # Gradient descent\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)\n",
    "            torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)\n",
    "            \n",
    "            self.actor_optimizer.step()\n",
    "            self.critic_optimizer.step()\n",
    "        \n",
    "        return policy_loss.item(), value_loss.item(), entropy_loss.item()\n",
    "\n",
    "# Lớp Memory để lưu trữ kinh nghiệm\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.maps = []\n",
    "        self.features = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.masks = []\n",
    "        self.values = []\n",
    "    \n",
    "    def push(self, map_tensor, feature_vector, action, log_prob, reward, mask, value):\n",
    "        self.maps.append(map_tensor)\n",
    "        self.features.append(feature_vector)\n",
    "        self.actions.append(action)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.rewards.append(reward)\n",
    "        self.masks.append(mask)\n",
    "        self.values.append(value)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.maps = []\n",
    "        self.features = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.masks = []\n",
    "        self.values = []\n",
    "\n",
    "# Hàm huấn luyện\n",
    "def train(env, agent, num_episodes=1000, max_steps=100, update_interval=2048):\n",
    "    memories = [Memory() for _ in range(env.env.n_robots)]\n",
    "    global_step = 0\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        map_tensor, feature_vector = obs\n",
    "        \n",
    "        episode_reward = 0\n",
    "        step = 0\n",
    "        \n",
    "        while step < max_steps:\n",
    "            # Chọn hành động\n",
    "            actions, log_probs, value = agent.get_action(map_tensor, feature_vector)\n",
    "            \n",
    "            # Thực hiện hành động\n",
    "            next_obs, reward, terminated, truncated, info = env.step(actions)\n",
    "            next_map_tensor, next_feature_vector = next_obs\n",
    "            \n",
    "            # Lưu trữ kinh nghiệm\n",
    "            mask = 1.0 - float(terminated or truncated)\n",
    "            for i in range(env.n_agents):\n",
    "                memories[i].push(\n",
    "                    map_tensor,\n",
    "                    feature_vector,\n",
    "                    actions,\n",
    "                    log_probs,\n",
    "                    reward,  # Reward trung bình\n",
    "                    mask,\n",
    "                    value\n",
    "                )\n",
    "            \n",
    "            # Cập nhật\n",
    "            if global_step % update_interval == 0 and global_step > 0:\n",
    "                policy_loss, value_loss, entropy_loss = agent.update(memories)\n",
    "                print(f\"Episode {episode}, Step {step}, Policy Loss: {policy_loss:.4f}, Value Loss: {value_loss:.4f}, Entropy Loss: {entropy_loss:.4f}\")\n",
    "                \n",
    "                # Xóa bộ nhớ sau khi cập nhật\n",
    "                for memory in memories:\n",
    "                    memory.clear()\n",
    "            \n",
    "            # Cập nhật trạng thái và reward\n",
    "            map_tensor, feature_vector = next_map_tensor, next_feature_vector\n",
    "            episode_reward += reward\n",
    "            global_step += 1\n",
    "            step += 1\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        \n",
    "        # In thông tin\n",
    "        if episode % 10 == 0:\n",
    "            avg_reward = sum(episode_rewards[-10:]) / 10\n",
    "            print(f\"Episode {episode}, Avg Reward: {avg_reward:.4f}\")\n",
    "    \n",
    "    return episode_rewards\n",
    "\n",
    "def run_trained_model(env, agent, num_episodes=10):\n",
    "    for episode in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        map_tensor, feature_vector = obs\n",
    "        \n",
    "        episode_reward = 0\n",
    "        step = 0\n",
    "        \n",
    "        while True:\n",
    "            # Chọn hành động\n",
    "            actions, _, _ = agent.get_action(map_tensor, feature_vector)\n",
    "            \n",
    "            # Thực hiện hành động\n",
    "            next_obs, reward, terminated, truncated, info = env.step(actions)\n",
    "            next_map_tensor, next_feature_vector = next_obs\n",
    "            \n",
    "            # Cập nhật trạng thái và reward\n",
    "            map_tensor, feature_vector = next_map_tensor, next_feature_vector\n",
    "            episode_reward += reward\n",
    "            step += 1\n",
    "            \n",
    "            print(f\"Step {step}, Action: {actions}, Reward: {reward}\")\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        print(f\"Episode {episode}, Total Reward: {episode_reward}, Steps: {step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map tensor shape: (20, 20)\n",
      "Feature vector shape: (716,)\n",
      "Sampled action: [4 2 0 0 1 2 3 0 0 2]\n",
      "Reward: 0.1464200112061815\n",
      "Next map tensor shape: (20, 20)\n",
      "Next feature vector shape: (716,)\n",
      "\n",
      "Bắt đầu huấn luyện...\n",
      "Episode 0, Step 200, Policy Loss: -0.0219, Value Loss: 0.2977, Entropy Loss: -1.2336\n",
      "Episode 0, Step 400, Policy Loss: -0.0358, Value Loss: 0.3716, Entropy Loss: -1.1858\n",
      "Episode 0, Step 600, Policy Loss: -0.0328, Value Loss: 2.7015, Entropy Loss: -1.0796\n",
      "Episode 0, Step 800, Policy Loss: -0.0425, Value Loss: 2.8436, Entropy Loss: -0.8483\n",
      "Episode 0, Avg Reward: 20.2325\n",
      "Episode 1, Step 0, Policy Loss: -0.0327, Value Loss: 27.5846, Entropy Loss: -0.8673\n",
      "Episode 1, Step 200, Policy Loss: -0.0212, Value Loss: 0.9849, Entropy Loss: -1.2281\n",
      "Episode 1, Step 400, Policy Loss: -0.0355, Value Loss: 0.4199, Entropy Loss: -1.1335\n",
      "Episode 1, Step 600, Policy Loss: -0.0387, Value Loss: 0.3118, Entropy Loss: -1.0190\n",
      "Episode 1, Step 800, Policy Loss: -0.0385, Value Loss: 2.1167, Entropy Loss: -0.9442\n",
      "Episode 2, Step 0, Policy Loss: -0.0239, Value Loss: 5.9109, Entropy Loss: -0.7619\n",
      "Episode 2, Step 200, Policy Loss: -0.0260, Value Loss: 0.4529, Entropy Loss: -1.2093\n",
      "Episode 2, Step 400, Policy Loss: -0.0296, Value Loss: 0.5017, Entropy Loss: -1.1413\n",
      "Episode 2, Step 600, Policy Loss: -0.0384, Value Loss: 1.6658, Entropy Loss: -1.0197\n",
      "Episode 2, Step 800, Policy Loss: -0.0215, Value Loss: 0.6801, Entropy Loss: -0.9241\n",
      "\n",
      "Chạy mô hình đã huấn luyện...\n",
      "Step 1, Action: [[3 0]\n",
      " [3 0]\n",
      " [0 2]\n",
      " [1 1]\n",
      " [1 2]], Reward: 0.10211754388729263\n",
      "Step 2, Action: [[0 0]\n",
      " [0 2]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [3 0]], Reward: 0.14503206390857296\n",
      "Step 3, Action: [[0 1]\n",
      " [2 0]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [2 1]], Reward: 0.07951028757086347\n",
      "Step 4, Action: [[1 2]\n",
      " [0 2]\n",
      " [3 2]\n",
      " [0 1]\n",
      " [1 0]], Reward: 0.11337722339831618\n",
      "Step 5, Action: [[0 2]\n",
      " [2 2]\n",
      " [1 0]\n",
      " [3 1]\n",
      " [2 1]], Reward: 0.10674859230063558\n",
      "Step 6, Action: [[3 2]\n",
      " [3 1]\n",
      " [2 1]\n",
      " [0 1]\n",
      " [1 1]], Reward: 0.1241886116991581\n",
      "Step 7, Action: [[3 2]\n",
      " [3 0]\n",
      " [2 1]\n",
      " [0 1]\n",
      " [1 1]], Reward: 0.1341886116991581\n",
      "Step 8, Action: [[0 1]\n",
      " [1 0]\n",
      " [0 2]\n",
      " [3 1]\n",
      " [2 1]], Reward: 0.08093720399979368\n",
      "Step 9, Action: [[2 0]\n",
      " [3 1]\n",
      " [1 2]\n",
      " [0 1]\n",
      " [2 1]], Reward: 0.07406199636068178\n",
      "Step 10, Action: [[0 1]\n",
      " [0 0]\n",
      " [2 0]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.08349718460127117\n",
      "Step 11, Action: [[1 2]\n",
      " [2 1]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [3 0]], Reward: 0.0828301079989491\n",
      "Step 12, Action: [[0 1]\n",
      " [0 0]\n",
      " [3 2]\n",
      " [3 1]\n",
      " [2 0]], Reward: 0.10575904018708361\n",
      "Step 13, Action: [[1 2]\n",
      " [3 2]\n",
      " [1 0]\n",
      " [2 2]\n",
      " [1 0]], Reward: 0.07575904018708363\n",
      "Step 14, Action: [[3 1]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [2 2]], Reward: 0.08687341943349462\n",
      "Step 15, Action: [[1 1]\n",
      " [2 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [3 2]], Reward: 0.0749805154343392\n",
      "Step 16, Action: [[2 0]\n",
      " [2 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]], Reward: 0.07868797237521812\n",
      "Step 17, Action: [[3 0]\n",
      " [3 1]\n",
      " [1 1]\n",
      " [3 0]\n",
      " [2 1]], Reward: 0.10721457987086079\n",
      "Step 18, Action: [[1 1]\n",
      " [2 1]\n",
      " [1 0]\n",
      " [1 2]\n",
      " [3 0]], Reward: 0.09532167587170538\n",
      "Step 19, Action: [[2 1]\n",
      " [1 0]\n",
      " [3 1]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.08310530779522735\n",
      "Step 20, Action: [[1 0]\n",
      " [1 0]\n",
      " [3 0]\n",
      " [2 2]\n",
      " [0 1]], Reward: 0.05658135538272898\n",
      "Step 21, Action: [[3 2]\n",
      " [2 0]\n",
      " [2 2]\n",
      " [0 2]\n",
      " [2 0]], Reward: 0.07719688351081727\n",
      "Step 22, Action: [[2 1]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [3 0]], Reward: 0.06847425938188441\n",
      "Step 23, Action: [[0 1]\n",
      " [1 2]\n",
      " [2 0]\n",
      " [3 1]\n",
      " [1 1]], Reward: 0.051458980337503187\n",
      "Step 24, Action: [[1 0]\n",
      " [0 2]\n",
      " [3 1]\n",
      " [0 0]\n",
      " [2 2]], Reward: 0.06943094794733885\n",
      "Step 25, Action: [[1 1]\n",
      " [3 2]\n",
      " [3 0]\n",
      " [2 1]\n",
      " [0 0]], Reward: 0.08899603571959384\n",
      "Step 26, Action: [[1 2]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [2 0]], Reward: 0.09601982781650761\n",
      "Step 27, Action: [[1 0]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [3 0]], Reward: 0.09899603571959384\n",
      "Step 28, Action: [[2 2]\n",
      " [1 2]\n",
      " [1 0]\n",
      " [2 1]\n",
      " [2 0]], Reward: 0.06165060021700627\n",
      "Step 29, Action: [[1 1]\n",
      " [3 1]\n",
      " [2 1]\n",
      " [3 1]\n",
      " [1 1]], Reward: 0.06197224362268007\n",
      "Step 30, Action: [[1 2]\n",
      " [3 2]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [1 2]], Reward: 0.07838050759150555\n",
      "Step 31, Action: [[1 1]\n",
      " [1 0]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [3 2]], Reward: 0.061356715494591776\n",
      "Step 32, Action: [[1 2]\n",
      " [0 1]\n",
      " [3 2]\n",
      " [3 1]\n",
      " [1 0]], Reward: 0.07961156384768217\n",
      "Step 33, Action: [[1 1]\n",
      " [2 1]\n",
      " [2 0]\n",
      " [2 1]\n",
      " [1 1]], Reward: 0.02899603571959388\n",
      "Step 34, Action: [[3 1]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [0 1]\n",
      " [0 0]], Reward: 0.04899603571959388\n",
      "Step 35, Action: [[3 1]\n",
      " [3 1]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [3 0]], Reward: 0.05197224362268006\n",
      "Step 36, Action: [[1 1]\n",
      " [2 1]\n",
      " [3 2]\n",
      " [1 0]\n",
      " [0 2]], Reward: 0.03197224362268006\n",
      "Step 37, Action: [[3 1]\n",
      " [0 2]\n",
      " [1 2]\n",
      " [0 0]\n",
      " [2 1]], Reward: 0.05899603571959387\n",
      "Step 38, Action: [[1 0]\n",
      " [2 0]\n",
      " [0 2]\n",
      " [0 2]\n",
      " [2 0]], Reward: 0.04813309894451824\n",
      "Step 39, Action: [[3 2]\n",
      " [1 2]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [2 1]], Reward: 0.055759040187083636\n",
      "Step 40, Action: [[0 1]\n",
      " [1 2]\n",
      " [2 0]\n",
      " [0 1]\n",
      " [0 1]], Reward: -0.027024591736438328\n",
      "Step 41, Action: [[0 1]\n",
      " [2 1]\n",
      " [1 1]\n",
      " [2 1]\n",
      " [0 0]], Reward: -0.03621320343559644\n",
      "Step 42, Action: [[2 0]\n",
      " [3 2]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [3 0]], Reward: 0.05142611678940569\n",
      "Step 43, Action: [[2 1]\n",
      " [3 2]\n",
      " [2 1]\n",
      " [0 1]\n",
      " [1 1]], Reward: -0.006426916428930202\n",
      "Step 44, Action: [[0 1]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [2 1]\n",
      " [1 0]], Reward: -0.02621320343559644\n",
      "Step 45, Action: [[2 2]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [2 1]], Reward: -0.028027756377319947\n",
      "Step 46, Action: [[3 0]\n",
      " [2 1]\n",
      " [2 2]\n",
      " [3 2]\n",
      " [1 0]], Reward: 0.00645898033750316\n",
      "Step 47, Action: [[1 0]\n",
      " [2 1]\n",
      " [3 1]\n",
      " [1 2]\n",
      " [0 2]], Reward: -0.0035410196624968415\n",
      "Step 48, Action: [[0 2]\n",
      " [3 0]\n",
      " [3 2]\n",
      " [1 1]\n",
      " [1 1]], Reward: 0.01645898033750316\n",
      "Step 49, Action: [[0 2]\n",
      " [2 2]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [3 1]], Reward: 0.012313404060046221\n",
      "Step 50, Action: [[1 2]\n",
      " [0 1]\n",
      " [2 2]\n",
      " [2 1]\n",
      " [2 0]], Reward: -0.007686595939953779\n",
      "Step 51, Action: [[2 2]\n",
      " [2 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [2 1]], Reward: -0.0049999999999999906\n",
      "Step 52, Action: [[3 0]\n",
      " [2 1]\n",
      " [3 1]\n",
      " [3 1]\n",
      " [2 2]], Reward: 0.04500000000000001\n",
      "Step 53, Action: [[2 2]\n",
      " [0 2]\n",
      " [2 1]\n",
      " [0 2]\n",
      " [3 1]], Reward: 0.02500000000000001\n",
      "Step 54, Action: [[1 1]\n",
      " [3 1]\n",
      " [3 2]\n",
      " [0 1]\n",
      " [2 1]], Reward: 0.025000000000000012\n",
      "Step 55, Action: [[3 1]\n",
      " [1 0]\n",
      " [0 2]\n",
      " [2 2]\n",
      " [3 1]], Reward: 0.012313404060046225\n",
      "Step 56, Action: [[0 1]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [3 1]\n",
      " [2 2]], Reward: -0.0035410196624968415\n",
      "Step 57, Action: [[0 2]\n",
      " [3 2]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [2 2]], Reward: -0.0358113883008419\n",
      "Step 58, Action: [[3 2]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [1 0]], Reward: -0.0358113883008419\n",
      "Step 59, Action: [[2 2]\n",
      " [3 2]\n",
      " [2 0]\n",
      " [0 0]\n",
      " [0 2]], Reward: -0.025811388300841905\n",
      "Step 60, Action: [[1 1]\n",
      " [0 2]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [0 2]], Reward: -0.006213203435596436\n",
      "Step 61, Action: [[1 0]\n",
      " [3 2]\n",
      " [2 1]\n",
      " [3 2]\n",
      " [0 0]], Reward: -0.015811388300841903\n",
      "Step 62, Action: [[1 0]\n",
      " [0 2]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [1 0]], Reward: -0.006213203435596434\n",
      "Step 63, Action: [[3 1]\n",
      " [2 1]\n",
      " [1 2]\n",
      " [1 2]\n",
      " [0 1]], Reward: 0.00616085532183815\n",
      "Step 64, Action: [[2 0]\n",
      " [3 0]\n",
      " [3 2]\n",
      " [3 1]\n",
      " [1 2]], Reward: 0.026160855321838154\n",
      "Step 65, Action: [[1 0]\n",
      " [2 2]\n",
      " [3 2]\n",
      " [0 2]\n",
      " [2 1]], Reward: -0.003839144678161849\n",
      "Step 66, Action: [[2 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [3 1]\n",
      " [2 1]], Reward: -0.01383914467816185\n",
      "Step 67, Action: [[0 1]\n",
      " [2 2]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [2 0]], Reward: -0.023839144678161853\n",
      "Step 68, Action: [[0 2]\n",
      " [3 2]\n",
      " [1 0]\n",
      " [3 0]\n",
      " [2 2]], Reward: 0.02616085532183815\n",
      "Step 69, Action: [[3 1]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [0 1]\n",
      " [3 0]], Reward: 0.013786796564403566\n",
      "Step 70, Action: [[0 0]\n",
      " [1 2]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [2 1]], Reward: -0.04000000000000001\n",
      "Step 71, Action: [[1 2]\n",
      " [3 2]\n",
      " [1 0]\n",
      " [1 2]\n",
      " [3 1]], Reward: 0.018768943743823395\n",
      "Step 72, Action: [[3 0]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [0 0]\n",
      " [3 1]], Reward: 0.018768943743823395\n",
      "Step 73, Action: [[1 0]\n",
      " [3 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [3 1]], Reward: 0.028768943743823397\n",
      "Step 74, Action: [[1 1]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [3 1]], Reward: 0.018768943743823395\n",
      "Step 75, Action: [[1 2]\n",
      " [3 1]\n",
      " [0 1]\n",
      " [0 2]\n",
      " [3 1]], Reward: -0.010000000000000004\n",
      "Step 76, Action: [[0 1]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 0]], Reward: -0.0406155281280883\n",
      "Step 77, Action: [[1 0]\n",
      " [1 2]\n",
      " [0 0]\n",
      " [2 1]\n",
      " [1 0]], Reward: -0.0523606797749979\n",
      "Step 78, Action: [[1 2]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 1]\n",
      " [2 1]], Reward: -0.08\n",
      "Step 79, Action: [[2 2]\n",
      " [1 2]\n",
      " [2 0]\n",
      " [3 0]\n",
      " [0 1]], Reward: -0.07\n",
      "Step 80, Action: [[0 1]\n",
      " [3 2]\n",
      " [3 1]\n",
      " [1 0]\n",
      " [0 0]], Reward: -0.03\n",
      "Step 81, Action: [[3 2]\n",
      " [3 1]\n",
      " [2 2]\n",
      " [2 2]\n",
      " [3 0]], Reward: -0.04\n",
      "Step 82, Action: [[3 1]\n",
      " [3 1]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [2 0]], Reward: -0.03\n",
      "Step 83, Action: [[3 1]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [3 2]\n",
      " [0 2]], Reward: -0.03\n",
      "Step 84, Action: [[1 1]\n",
      " [0 0]\n",
      " [0 2]\n",
      " [3 2]\n",
      " [0 0]], Reward: -0.05\n",
      "Step 85, Action: [[1 2]\n",
      " [0 2]\n",
      " [3 2]\n",
      " [0 2]\n",
      " [3 0]], Reward: -0.03\n",
      "Step 86, Action: [[1 2]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [2 1]\n",
      " [0 2]], Reward: -0.05\n",
      "Step 87, Action: [[0 1]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [1 0]\n",
      " [2 0]], Reward: -0.060000000000000005\n",
      "Step 88, Action: [[0 0]\n",
      " [0 2]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [2 0]], Reward: -0.05\n",
      "Step 89, Action: [[3 0]\n",
      " [2 2]\n",
      " [1 0]\n",
      " [3 1]\n",
      " [2 0]], Reward: -0.03\n",
      "Step 90, Action: [[1 2]\n",
      " [2 1]\n",
      " [2 2]\n",
      " [2 1]\n",
      " [2 0]], Reward: -0.08\n",
      "Step 91, Action: [[0 2]\n",
      " [1 0]\n",
      " [2 2]\n",
      " [0 2]\n",
      " [0 1]], Reward: -0.07\n",
      "Step 92, Action: [[0 0]\n",
      " [0 1]\n",
      " [2 0]\n",
      " [0 0]\n",
      " [2 0]], Reward: -0.060000000000000005\n",
      "Step 93, Action: [[3 1]\n",
      " [0 1]\n",
      " [0 2]\n",
      " [0 0]\n",
      " [0 0]], Reward: -0.04\n",
      "Step 94, Action: [[1 2]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [3 0]\n",
      " [1 0]], Reward: -0.07\n",
      "Step 95, Action: [[2 2]\n",
      " [0 2]\n",
      " [0 1]\n",
      " [3 2]\n",
      " [0 2]], Reward: -0.05\n",
      "Step 96, Action: [[0 2]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [2 2]\n",
      " [1 0]], Reward: -0.07\n",
      "Step 97, Action: [[1 2]\n",
      " [3 2]\n",
      " [3 1]\n",
      " [3 0]\n",
      " [2 0]], Reward: -0.03\n",
      "Step 98, Action: [[0 0]\n",
      " [3 0]\n",
      " [0 0]\n",
      " [3 0]\n",
      " [3 0]], Reward: -0.02\n",
      "Step 99, Action: [[3 2]\n",
      " [3 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [1 1]], Reward: -0.05\n",
      "Step 100, Action: [[3 1]\n",
      " [2 2]\n",
      " [3 1]\n",
      " [3 1]\n",
      " [2 2]], Reward: -0.04\n",
      "Step 101, Action: [[1 2]\n",
      " [3 0]\n",
      " [0 0]\n",
      " [3 2]\n",
      " [0 0]], Reward: -0.03\n",
      "Step 102, Action: [[1 1]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [3 0]\n",
      " [2 2]], Reward: -0.06\n",
      "Step 103, Action: [[3 0]\n",
      " [2 2]\n",
      " [2 1]\n",
      " [1 2]\n",
      " [1 1]], Reward: -0.08\n",
      "Step 104, Action: [[1 0]\n",
      " [3 2]\n",
      " [3 2]\n",
      " [1 1]\n",
      " [0 0]], Reward: -0.04\n",
      "Step 105, Action: [[0 2]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [3 1]], Reward: -0.05\n",
      "Step 106, Action: [[3 0]\n",
      " [2 1]\n",
      " [2 0]\n",
      " [0 2]\n",
      " [2 1]], Reward: -0.07\n",
      "Step 107, Action: [[0 0]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [2 0]\n",
      " [2 2]], Reward: -0.04\n",
      "Step 108, Action: [[0 1]\n",
      " [1 2]\n",
      " [2 1]\n",
      " [1 1]\n",
      " [0 1]], Reward: -0.07\n",
      "Step 109, Action: [[1 0]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]], Reward: -0.06\n",
      "Step 110, Action: [[0 0]\n",
      " [3 0]\n",
      " [2 2]\n",
      " [3 2]\n",
      " [2 1]], Reward: -0.05\n",
      "Step 111, Action: [[1 2]\n",
      " [2 2]\n",
      " [2 1]\n",
      " [0 1]\n",
      " [2 0]], Reward: -0.060000000000000005\n",
      "Step 112, Action: [[1 1]\n",
      " [0 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [3 2]], Reward: -0.04\n",
      "Step 113, Action: [[2 2]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [3 2]], Reward: -0.06\n",
      "Step 114, Action: [[0 2]\n",
      " [1 2]\n",
      " [1 0]\n",
      " [3 2]\n",
      " [1 1]], Reward: -0.07\n",
      "Step 115, Action: [[1 1]\n",
      " [3 2]\n",
      " [0 1]\n",
      " [0 0]\n",
      " [3 2]], Reward: -0.04\n",
      "Step 116, Action: [[2 0]\n",
      " [0 0]\n",
      " [2 0]\n",
      " [0 0]\n",
      " [1 1]], Reward: -0.07\n",
      "Step 117, Action: [[1 2]\n",
      " [3 0]\n",
      " [3 2]\n",
      " [1 0]\n",
      " [2 1]], Reward: -0.05\n",
      "Step 118, Action: [[0 2]\n",
      " [0 1]\n",
      " [3 1]\n",
      " [1 0]\n",
      " [0 0]], Reward: -0.04\n",
      "Step 119, Action: [[1 1]\n",
      " [0 2]\n",
      " [1 0]\n",
      " [3 0]\n",
      " [2 1]], Reward: -0.05\n",
      "Step 120, Action: [[1 2]\n",
      " [2 2]\n",
      " [2 1]\n",
      " [1 0]\n",
      " [3 2]], Reward: -0.06\n",
      "Step 121, Action: [[1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [3 0]\n",
      " [0 0]], Reward: -0.04\n",
      "Step 122, Action: [[3 2]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [1 2]\n",
      " [3 0]], Reward: -0.03\n",
      "Step 123, Action: [[3 2]\n",
      " [3 1]\n",
      " [1 2]\n",
      " [1 2]\n",
      " [3 0]], Reward: -0.02\n",
      "Step 124, Action: [[1 2]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [3 0]\n",
      " [1 0]], Reward: -0.07\n",
      "Step 125, Action: [[2 2]\n",
      " [3 2]\n",
      " [3 2]\n",
      " [1 1]\n",
      " [2 0]], Reward: -0.05\n",
      "Step 126, Action: [[0 1]\n",
      " [2 1]\n",
      " [3 1]\n",
      " [3 0]\n",
      " [1 2]], Reward: -0.04\n",
      "Step 127, Action: [[3 2]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [1 2]\n",
      " [2 1]], Reward: -0.05\n",
      "Step 128, Action: [[3 1]\n",
      " [2 0]\n",
      " [3 2]\n",
      " [2 2]\n",
      " [0 0]], Reward: -0.04\n",
      "Step 129, Action: [[1 0]\n",
      " [3 0]\n",
      " [0 1]\n",
      " [3 2]\n",
      " [3 1]], Reward: -0.02\n",
      "Step 130, Action: [[1 2]\n",
      " [3 2]\n",
      " [0 1]\n",
      " [1 2]\n",
      " [3 0]], Reward: -0.05\n",
      "Step 131, Action: [[2 0]\n",
      " [2 1]\n",
      " [2 0]\n",
      " [3 0]\n",
      " [3 0]], Reward: -0.06\n",
      "Step 132, Action: [[2 2]\n",
      " [3 2]\n",
      " [2 0]\n",
      " [3 1]\n",
      " [2 1]], Reward: -0.05\n",
      "Step 133, Action: [[0 1]\n",
      " [3 1]\n",
      " [1 0]\n",
      " [3 0]\n",
      " [3 0]], Reward: -0.03\n",
      "Step 134, Action: [[1 2]\n",
      " [0 2]\n",
      " [0 0]\n",
      " [2 1]\n",
      " [2 0]], Reward: -0.07\n",
      "Step 135, Action: [[3 2]\n",
      " [2 2]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [0 0]], Reward: -0.04\n",
      "Step 136, Action: [[2 1]\n",
      " [2 2]\n",
      " [3 1]\n",
      " [1 1]\n",
      " [0 0]], Reward: -0.05\n",
      "Step 137, Action: [[3 0]\n",
      " [0 2]\n",
      " [3 0]\n",
      " [3 0]\n",
      " [3 2]], Reward: -0.01\n",
      "Step 138, Action: [[0 2]\n",
      " [3 0]\n",
      " [3 0]\n",
      " [3 1]\n",
      " [0 2]], Reward: -0.02\n",
      "Step 139, Action: [[0 1]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [0 2]\n",
      " [3 0]], Reward: -0.04\n",
      "Step 140, Action: [[3 0]\n",
      " [3 2]\n",
      " [3 2]\n",
      " [3 0]\n",
      " [0 1]], Reward: -0.01\n",
      "Step 141, Action: [[0 0]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [1 0]], Reward: -0.04\n",
      "Step 142, Action: [[1 2]\n",
      " [3 2]\n",
      " [0 1]\n",
      " [0 0]\n",
      " [2 1]], Reward: -0.05\n",
      "Step 143, Action: [[1 1]\n",
      " [3 2]\n",
      " [3 1]\n",
      " [1 0]\n",
      " [3 1]], Reward: -0.03\n",
      "Step 144, Action: [[0 2]\n",
      " [2 0]\n",
      " [3 1]\n",
      " [3 2]\n",
      " [2 0]], Reward: -0.03\n",
      "Step 145, Action: [[0 0]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [2 2]], Reward: 0.030000000000000013\n",
      "Step 146, Action: [[0 0]\n",
      " [3 2]\n",
      " [2 2]\n",
      " [3 2]\n",
      " [2 1]], Reward: 0.045\n",
      "Step 147, Action: [[0 2]\n",
      " [3 1]\n",
      " [1 1]\n",
      " [3 0]\n",
      " [2 1]], Reward: 0.0029912287450431056\n",
      "Step 148, Action: [[2 0]\n",
      " [0 2]\n",
      " [1 2]\n",
      " [0 2]\n",
      " [0 1]], Reward: -0.027008771254956897\n",
      "Step 149, Action: [[1 2]\n",
      " [3 2]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [2 1]], Reward: -0.017008771254956895\n",
      "Step 150, Action: [[1 0]\n",
      " [0 0]\n",
      " [3 2]\n",
      " [0 0]\n",
      " [2 0]], Reward: 0.0029912287450431056\n",
      "Step 151, Action: [[0 1]\n",
      " [0 2]\n",
      " [2 1]\n",
      " [0 0]\n",
      " [1 1]], Reward: -0.027008771254956897\n",
      "Step 152, Action: [[3 1]\n",
      " [3 0]\n",
      " [3 1]\n",
      " [3 0]\n",
      " [1 1]], Reward: 0.022991228745043106\n",
      "Step 153, Action: [[1 0]\n",
      " [1 2]\n",
      " [0 2]\n",
      " [1 0]\n",
      " [3 1]], Reward: -0.008523499553598124\n",
      "Step 154, Action: [[3 2]\n",
      " [2 0]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [2 1]], Reward: -0.007008771254956895\n",
      "Step 155, Action: [[0 2]\n",
      " [2 2]\n",
      " [3 1]\n",
      " [1 2]\n",
      " [3 0]], Reward: 0.004098300562505253\n",
      "Step 156, Action: [[0 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [3 2]\n",
      " [3 1]], Reward: 0.004098300562505253\n",
      "Step 157, Action: [[3 0]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [1 2]\n",
      " [2 0]], Reward: 0.004098300562505253\n",
      "Step 158, Action: [[1 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [2 0]], Reward: -0.005901699437494747\n",
      "Step 159, Action: [[1 0]\n",
      " [2 2]\n",
      " [2 1]\n",
      " [0 0]\n",
      " [3 0]], Reward: -0.015226805085936303\n",
      "Step 160, Action: [[1 2]\n",
      " [3 2]\n",
      " [3 2]\n",
      " [3 0]\n",
      " [3 0]], Reward: 0.0347731949140637\n",
      "Step 161, Action: [[3 0]\n",
      " [0 2]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [2 2]], Reward: 0.014773194914063698\n",
      "Step 162, Action: [[0 1]\n",
      " [1 0]\n",
      " [3 1]\n",
      " [0 1]\n",
      " [2 0]], Reward: -0.005901699437494747\n",
      "Step 163, Action: [[1 2]\n",
      " [3 1]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [3 2]], Reward: 0.014098300562505254\n",
      "Step 164, Action: [[1 1]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [0 2]\n",
      " [2 0]], Reward: -0.007008771254956895\n",
      "Step 165, Action: [[0 0]\n",
      " [2 2]\n",
      " [2 2]\n",
      " [0 0]\n",
      " [2 1]], Reward: -0.02590169943749475\n",
      "Step 166, Action: [[0 1]\n",
      " [0 2]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [3 1]], Reward: -0.005901699437494747\n",
      "Step 167, Action: [[0 0]\n",
      " [0 2]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [2 0]], Reward: -0.005901699437494747\n",
      "Step 168, Action: [[2 1]\n",
      " [1 2]\n",
      " [2 2]\n",
      " [3 2]\n",
      " [3 1]], Reward: -0.017008771254956895\n",
      "Step 169, Action: [[0 0]\n",
      " [3 2]\n",
      " [3 2]\n",
      " [0 0]\n",
      " [0 1]], Reward: 0.012991228745043106\n",
      "Step 170, Action: [[1 0]\n",
      " [3 1]\n",
      " [2 0]\n",
      " [3 2]\n",
      " [0 0]], Reward: 0.0029912287450431056\n",
      "Step 171, Action: [[1 1]\n",
      " [2 2]\n",
      " [2 2]\n",
      " [0 1]\n",
      " [2 2]], Reward: -0.015901699437494747\n",
      "Step 172, Action: [[3 2]\n",
      " [3 1]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.014098300562505254\n",
      "Step 173, Action: [[3 1]\n",
      " [2 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [2 1]], Reward: 0.004098300562505253\n",
      "Step 174, Action: [[1 1]\n",
      " [0 0]\n",
      " [2 2]\n",
      " [3 2]\n",
      " [0 1]], Reward: -0.005901699437494747\n",
      "Step 175, Action: [[0 0]\n",
      " [3 0]\n",
      " [1 0]\n",
      " [0 2]\n",
      " [2 0]], Reward: -0.005901699437494747\n",
      "Step 176, Action: [[0 0]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [0 0]], Reward: -0.027008771254956897\n",
      "Step 177, Action: [[1 1]\n",
      " [2 1]\n",
      " [0 2]\n",
      " [1 1]\n",
      " [2 1]], Reward: -0.007008771254956895\n",
      "Step 178, Action: [[2 2]\n",
      " [3 2]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [1 0]], Reward: -0.027008771254956893\n",
      "Step 179, Action: [[0 1]\n",
      " [0 2]\n",
      " [3 1]\n",
      " [0 1]\n",
      " [3 0]], Reward: 0.012991228745043106\n",
      "Step 180, Action: [[2 1]\n",
      " [0 1]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [1 0]], Reward: -0.017008771254956895\n",
      "Step 181, Action: [[0 0]\n",
      " [0 2]\n",
      " [0 1]\n",
      " [2 2]\n",
      " [0 0]], Reward: -0.017008771254956895\n",
      "Step 182, Action: [[1 1]\n",
      " [1 0]\n",
      " [2 2]\n",
      " [3 0]\n",
      " [2 1]], Reward: -0.018523499553598125\n",
      "Step 183, Action: [[0 1]\n",
      " [3 1]\n",
      " [2 1]\n",
      " [1 1]\n",
      " [2 0]], Reward: -0.008523499553598124\n",
      "Step 184, Action: [[1 0]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [2 2]], Reward: -0.018523499553598125\n",
      "Step 185, Action: [[1 1]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [2 1]], Reward: -0.020415229867972867\n",
      "Step 186, Action: [[1 2]\n",
      " [2 0]\n",
      " [2 1]\n",
      " [3 1]\n",
      " [2 0]], Reward: -0.018523499553598125\n",
      "Step 187, Action: [[0 2]\n",
      " [1 2]\n",
      " [2 0]\n",
      " [3 1]\n",
      " [2 1]], Reward: -0.020415229867972867\n",
      "Step 188, Action: [[0 1]\n",
      " [3 2]\n",
      " [0 0]\n",
      " [3 1]\n",
      " [0 0]], Reward: 0.009584770132027133\n",
      "Step 189, Action: [[0 2]\n",
      " [3 2]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [0 1]], Reward: -0.010415229867972867\n",
      "Step 190, Action: [[1 2]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 1]\n",
      " [3 1]], Reward: -0.0004152298679728668\n",
      "Step 191, Action: [[0 2]\n",
      " [0 1]\n",
      " [2 2]\n",
      " [3 1]\n",
      " [0 1]], Reward: -0.010415229867972867\n",
      "Step 192, Action: [[1 2]\n",
      " [3 2]\n",
      " [1 1]\n",
      " [3 0]\n",
      " [3 1]], Reward: 0.009584770132027133\n",
      "Step 193, Action: [[0 0]\n",
      " [2 1]\n",
      " [2 1]\n",
      " [0 1]\n",
      " [0 2]], Reward: -0.028523499553598126\n",
      "Step 194, Action: [[0 0]\n",
      " [2 0]\n",
      " [0 2]\n",
      " [3 1]\n",
      " [3 2]], Reward: 0.0029912287450431056\n",
      "Step 195, Action: [[1 1]\n",
      " [3 2]\n",
      " [1 0]\n",
      " [3 1]\n",
      " [3 0]], Reward: 0.012991228745043106\n",
      "Step 196, Action: [[0 0]\n",
      " [3 0]\n",
      " [0 1]\n",
      " [2 1]\n",
      " [2 2]], Reward: 0.0029912287450431056\n",
      "Step 197, Action: [[1 0]\n",
      " [3 2]\n",
      " [3 1]\n",
      " [3 0]\n",
      " [2 1]], Reward: 0.022991228745043106\n",
      "Step 198, Action: [[1 2]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [2 1]], Reward: -0.005901699437494747\n",
      "Step 199, Action: [[0 1]\n",
      " [2 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [2 2]], Reward: -0.005901699437494747\n",
      "Step 200, Action: [[1 1]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [0 0]\n",
      " [0 1]], Reward: -0.005901699437494747\n",
      "Step 201, Action: [[3 1]\n",
      " [3 0]\n",
      " [0 0]\n",
      " [3 2]\n",
      " [1 0]], Reward: 0.014098300562505254\n",
      "Step 202, Action: [[0 2]\n",
      " [3 1]\n",
      " [2 2]\n",
      " [0 0]\n",
      " [2 1]], Reward: -0.015901699437494747\n",
      "Step 203, Action: [[0 2]\n",
      " [3 2]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [3 0]], Reward: 0.004098300562505253\n",
      "Step 204, Action: [[1 1]\n",
      " [3 2]\n",
      " [1 1]\n",
      " [0 2]\n",
      " [1 0]], Reward: -0.005901699437494747\n",
      "Step 205, Action: [[0 2]\n",
      " [3 2]\n",
      " [3 0]\n",
      " [3 0]\n",
      " [3 2]], Reward: 0.034098300562505256\n",
      "Step 206, Action: [[1 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]], Reward: -0.005901699437494747\n",
      "Step 207, Action: [[2 1]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [2 0]], Reward: -0.005901699437494747\n",
      "Step 208, Action: [[0 1]\n",
      " [2 2]\n",
      " [1 0]\n",
      " [3 2]\n",
      " [1 0]], Reward: -0.005901699437494747\n",
      "Step 209, Action: [[1 2]\n",
      " [2 2]\n",
      " [2 1]\n",
      " [3 1]\n",
      " [1 1]], Reward: -0.0352268050859363\n",
      "Step 210, Action: [[3 0]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [2 0]], Reward: -0.0052268050859363025\n",
      "Step 211, Action: [[1 0]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [0 1]\n",
      " [0 2]], Reward: -0.0052268050859363025\n",
      "Step 212, Action: [[3 1]\n",
      " [2 1]\n",
      " [3 1]\n",
      " [0 2]\n",
      " [2 0]], Reward: 0.004773194914063698\n",
      "Step 213, Action: [[0 1]\n",
      " [3 1]\n",
      " [0 2]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.014773194914063698\n",
      "Step 214, Action: [[1 0]\n",
      " [0 0]\n",
      " [3 1]\n",
      " [1 0]\n",
      " [0 0]], Reward: 0.004773194914063698\n",
      "Step 215, Action: [[3 0]\n",
      " [2 1]\n",
      " [3 1]\n",
      " [1 0]\n",
      " [0 2]], Reward: 0.014773194914063698\n",
      "Step 216, Action: [[0 1]\n",
      " [2 2]\n",
      " [3 1]\n",
      " [3 2]\n",
      " [1 0]], Reward: 0.004773194914063698\n",
      "Step 217, Action: [[1 1]\n",
      " [3 2]\n",
      " [1 1]\n",
      " [3 0]\n",
      " [2 1]], Reward: 0.004773194914063698\n",
      "Step 218, Action: [[1 1]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [2 1]], Reward: -0.005901699437494747\n",
      "Step 219, Action: [[3 0]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [1 0]\n",
      " [1 1]], Reward: -0.005901699437494747\n",
      "Step 220, Action: [[1 2]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [1 2]\n",
      " [1 0]], Reward: -0.015901699437494747\n",
      "Step 221, Action: [[1 2]\n",
      " [3 0]\n",
      " [1 0]\n",
      " [3 2]\n",
      " [2 1]], Reward: 0.004098300562505253\n",
      "Step 222, Action: [[0 1]\n",
      " [3 2]\n",
      " [0 2]\n",
      " [0 1]\n",
      " [0 1]], Reward: 0.004098300562505253\n",
      "Step 223, Action: [[1 2]\n",
      " [1 0]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [3 1]], Reward: 0.0029912287450431056\n",
      "Step 224, Action: [[3 2]\n",
      " [3 0]\n",
      " [0 1]\n",
      " [0 2]\n",
      " [1 1]], Reward: 0.0029912287450431056\n",
      "Step 225, Action: [[1 2]\n",
      " [2 0]\n",
      " [3 1]\n",
      " [3 0]\n",
      " [3 1]], Reward: 0.014098300562505254\n",
      "Step 226, Action: [[0 1]\n",
      " [2 2]\n",
      " [3 2]\n",
      " [3 2]\n",
      " [0 0]], Reward: 0.014098300562505254\n",
      "Step 227, Action: [[1 2]\n",
      " [0 2]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [3 0]], Reward: 0.014098300562505254\n",
      "Step 228, Action: [[0 1]\n",
      " [3 2]\n",
      " [0 2]\n",
      " [3 2]\n",
      " [0 0]], Reward: 0.014098300562505254\n",
      "Step 229, Action: [[0 2]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [1 0]], Reward: 0.014098300562505254\n",
      "Step 230, Action: [[1 2]\n",
      " [3 1]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [2 0]], Reward: 0.004098300562505253\n",
      "Step 231, Action: [[0 2]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 1]], Reward: -0.015901699437494747\n",
      "Step 232, Action: [[1 0]\n",
      " [3 2]\n",
      " [2 1]\n",
      " [1 0]\n",
      " [2 1]], Reward: -0.015901699437494747\n",
      "Step 233, Action: [[0 1]\n",
      " [3 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [3 2]], Reward: 0.004098300562505253\n",
      "Step 234, Action: [[1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 0]\n",
      " [0 0]], Reward: -0.017008771254956895\n",
      "Step 235, Action: [[0 1]\n",
      " [3 1]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [3 1]], Reward: 0.022991228745043106\n",
      "Step 236, Action: [[0 2]\n",
      " [3 0]\n",
      " [1 0]\n",
      " [3 2]\n",
      " [3 0]], Reward: 0.012991228745043106\n",
      "Step 237, Action: [[1 0]\n",
      " [1 2]\n",
      " [2 0]\n",
      " [0 2]\n",
      " [0 0]], Reward: -0.028523499553598126\n",
      "Step 238, Action: [[1 1]\n",
      " [3 2]\n",
      " [2 1]\n",
      " [0 0]\n",
      " [0 1]], Reward: -0.008523499553598124\n",
      "Step 239, Action: [[1 2]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [0 1]\n",
      " [0 0]], Reward: -0.027008771254956897\n",
      "Step 240, Action: [[0 2]\n",
      " [2 2]\n",
      " [2 1]\n",
      " [3 0]\n",
      " [1 1]], Reward: -0.025901699437494746\n",
      "Step 241, Action: [[1 1]\n",
      " [3 1]\n",
      " [1 0]\n",
      " [3 2]\n",
      " [3 0]], Reward: 0.014098300562505254\n",
      "Step 242, Action: [[3 1]\n",
      " [0 0]\n",
      " [2 1]\n",
      " [0 2]\n",
      " [3 0]], Reward: 0.004098300562505253\n",
      "Step 243, Action: [[1 2]\n",
      " [1 0]\n",
      " [2 2]\n",
      " [1 0]\n",
      " [1 0]], Reward: -0.017008771254956895\n",
      "Step 244, Action: [[0 0]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [0 0]], Reward: -0.015901699437494747\n",
      "Step 245, Action: [[0 1]\n",
      " [3 0]\n",
      " [2 0]\n",
      " [0 2]\n",
      " [0 0]], Reward: -0.005901699437494747\n",
      "Step 246, Action: [[0 0]\n",
      " [3 2]\n",
      " [3 0]\n",
      " [3 0]\n",
      " [1 1]], Reward: 0.024098300562505254\n",
      "Step 247, Action: [[1 1]\n",
      " [1 0]\n",
      " [1 2]\n",
      " [1 2]\n",
      " [0 1]], Reward: -0.027008771254956897\n",
      "Step 248, Action: [[0 1]\n",
      " [3 0]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [3 0]], Reward: 0.0029912287450431056\n",
      "Step 249, Action: [[1 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]], Reward: -0.017008771254956895\n",
      "Step 250, Action: [[1 2]\n",
      " [3 2]\n",
      " [0 2]\n",
      " [1 2]\n",
      " [1 0]], Reward: 0.0029912287450431056\n",
      "Step 251, Action: [[1 2]\n",
      " [3 2]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [1 1]], Reward: 0.012991228745043106\n",
      "Step 252, Action: [[3 1]\n",
      " [0 2]\n",
      " [0 2]\n",
      " [1 0]\n",
      " [0 1]], Reward: 0.0029912287450431056\n",
      "Step 253, Action: [[3 1]\n",
      " [3 2]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [0 0]], Reward: 0.012991228745043106\n",
      "Step 254, Action: [[0 1]\n",
      " [3 2]\n",
      " [2 1]\n",
      " [0 2]\n",
      " [0 1]], Reward: -0.007008771254956895\n",
      "Step 255, Action: [[0 0]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 0]], Reward: -0.015901699437494747\n",
      "Step 256, Action: [[1 1]\n",
      " [2 2]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [3 1]], Reward: -0.015226805085936303\n",
      "Step 257, Action: [[3 0]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [1 0]], Reward: 0.014773194914063698\n",
      "Step 258, Action: [[1 0]\n",
      " [3 2]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [3 1]], Reward: 0.014773194914063698\n",
      "Step 259, Action: [[3 0]\n",
      " [3 2]\n",
      " [3 1]\n",
      " [1 2]\n",
      " [2 1]], Reward: 0.014773194914063698\n",
      "Step 260, Action: [[1 0]\n",
      " [2 2]\n",
      " [3 0]\n",
      " [3 2]\n",
      " [3 0]], Reward: 0.024773194914063698\n",
      "Step 261, Action: [[3 0]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [0 2]], Reward: 0.014773194914063698\n",
      "Step 262, Action: [[1 1]\n",
      " [3 1]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [1 0]], Reward: -0.0052268050859363025\n",
      "Step 263, Action: [[1 2]\n",
      " [2 1]\n",
      " [0 1]\n",
      " [3 2]\n",
      " [2 1]], Reward: -0.0052268050859363025\n",
      "Step 264, Action: [[0 2]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [3 0]\n",
      " [1 1]], Reward: -0.0052268050859363025\n",
      "Step 265, Action: [[1 2]\n",
      " [3 2]\n",
      " [0 2]\n",
      " [1 1]\n",
      " [3 1]], Reward: 0.014773194914063698\n",
      "Step 266, Action: [[0 1]\n",
      " [3 1]\n",
      " [2 2]\n",
      " [2 2]\n",
      " [3 0]], Reward: 0.014773194914063698\n",
      "Step 267, Action: [[0 2]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [1 2]], Reward: -0.015901699437494747\n",
      "Step 268, Action: [[3 0]\n",
      " [3 2]\n",
      " [1 1]\n",
      " [0 2]\n",
      " [1 1]], Reward: 0.014098300562505254\n",
      "Step 269, Action: [[1 1]\n",
      " [2 2]\n",
      " [2 1]\n",
      " [1 2]\n",
      " [0 2]], Reward: -0.025226805085936305\n",
      "Step 270, Action: [[3 0]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [2 1]\n",
      " [3 0]], Reward: 0.014773194914063698\n",
      "Step 271, Action: [[0 2]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [0 2]\n",
      " [0 2]], Reward: -0.0052268050859363025\n",
      "Step 272, Action: [[1 2]\n",
      " [3 2]\n",
      " [3 0]\n",
      " [3 2]\n",
      " [3 1]], Reward: 0.0347731949140637\n",
      "Step 273, Action: [[1 2]\n",
      " [2 2]\n",
      " [0 1]\n",
      " [3 0]\n",
      " [3 0]], Reward: 0.014773194914063698\n",
      "Step 274, Action: [[3 1]\n",
      " [2 0]\n",
      " [2 2]\n",
      " [3 1]\n",
      " [3 1]], Reward: 0.024773194914063698\n",
      "Step 275, Action: [[0 2]\n",
      " [3 1]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [0 1]], Reward: 0.014773194914063698\n",
      "Step 276, Action: [[0 2]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 2]\n",
      " [1 0]], Reward: -0.0052268050859363025\n",
      "Step 277, Action: [[1 1]\n",
      " [2 2]\n",
      " [1 0]\n",
      " [0 2]\n",
      " [0 1]], Reward: -0.0052268050859363025\n",
      "Step 278, Action: [[0 2]\n",
      " [1 1]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [3 1]], Reward: -0.005901699437494747\n",
      "Step 279, Action: [[3 2]\n",
      " [3 2]\n",
      " [2 1]\n",
      " [3 2]\n",
      " [3 2]], Reward: 0.034098300562505256\n",
      "Step 280, Action: [[1 2]\n",
      " [1 2]\n",
      " [2 1]\n",
      " [0 2]\n",
      " [1 0]], Reward: -0.017008771254956895\n",
      "Step 281, Action: [[1 1]\n",
      " [0 1]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [1 1]], Reward: -0.007008771254956895\n",
      "Step 282, Action: [[1 0]\n",
      " [3 2]\n",
      " [2 1]\n",
      " [3 2]\n",
      " [3 1]], Reward: 0.012991228745043106\n",
      "Step 283, Action: [[3 1]\n",
      " [2 0]\n",
      " [3 0]\n",
      " [0 1]\n",
      " [3 1]], Reward: 0.014098300562505254\n",
      "Step 284, Action: [[0 0]\n",
      " [0 2]\n",
      " [0 2]\n",
      " [0 2]\n",
      " [0 1]], Reward: -0.005901699437494747\n",
      "Step 285, Action: [[0 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [2 1]\n",
      " [0 1]], Reward: -0.015901699437494747\n",
      "Step 286, Action: [[1 1]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [1 1]], Reward: 0.004098300562505253\n",
      "Step 287, Action: [[1 2]\n",
      " [0 2]\n",
      " [1 2]\n",
      " [0 2]\n",
      " [2 0]], Reward: -0.015901699437494747\n",
      "Step 288, Action: [[1 1]\n",
      " [2 1]\n",
      " [2 0]\n",
      " [3 1]\n",
      " [3 1]], Reward: -0.0052268050859363025\n",
      "Step 289, Action: [[1 0]\n",
      " [3 2]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 1]], Reward: -0.0052268050859363025\n",
      "Step 290, Action: [[1 1]\n",
      " [0 2]\n",
      " [2 1]\n",
      " [3 1]\n",
      " [2 0]], Reward: -0.0052268050859363025\n",
      "Step 291, Action: [[0 2]\n",
      " [2 0]\n",
      " [0 2]\n",
      " [1 0]\n",
      " [2 1]], Reward: -0.015226805085936303\n",
      "Step 292, Action: [[2 1]\n",
      " [3 2]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [2 0]], Reward: -0.0052268050859363025\n",
      "Step 293, Action: [[3 0]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [2 0]], Reward: 0.014773194914063698\n",
      "Step 294, Action: [[0 2]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.014773194914063698\n",
      "Step 295, Action: [[2 2]\n",
      " [3 2]\n",
      " [1 0]\n",
      " [1 2]\n",
      " [2 0]], Reward: -0.0052268050859363025\n",
      "Step 296, Action: [[1 0]\n",
      " [0 2]\n",
      " [3 1]\n",
      " [3 0]\n",
      " [2 1]], Reward: 0.004773194914063698\n",
      "Step 297, Action: [[3 2]\n",
      " [0 0]\n",
      " [2 1]\n",
      " [0 0]\n",
      " [2 0]], Reward: 0.004773194914063698\n",
      "Step 298, Action: [[0 2]\n",
      " [2 1]\n",
      " [3 1]\n",
      " [1 2]\n",
      " [2 2]], Reward: 0.004773194914063698\n",
      "Step 299, Action: [[0 1]\n",
      " [3 2]\n",
      " [2 1]\n",
      " [2 0]\n",
      " [0 1]], Reward: -0.0052268050859363025\n",
      "Step 300, Action: [[1 2]\n",
      " [3 2]\n",
      " [1 0]\n",
      " [3 0]\n",
      " [1 0]], Reward: 0.004773194914063698\n",
      "Step 301, Action: [[3 1]\n",
      " [2 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 1]], Reward: 0.004773194914063698\n",
      "Step 302, Action: [[0 0]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [0 1]], Reward: -0.0052268050859363025\n",
      "Step 303, Action: [[3 2]\n",
      " [3 0]\n",
      " [1 0]\n",
      " [3 0]\n",
      " [2 0]], Reward: 0.014773194914063698\n",
      "Step 304, Action: [[1 2]\n",
      " [2 2]\n",
      " [0 0]\n",
      " [3 0]\n",
      " [0 1]], Reward: 0.004773194914063698\n",
      "Step 305, Action: [[2 2]\n",
      " [3 2]\n",
      " [1 0]\n",
      " [3 2]\n",
      " [0 0]], Reward: 0.004773194914063698\n",
      "Step 306, Action: [[0 2]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [0 0]\n",
      " [3 1]], Reward: 0.014773194914063698\n",
      "Step 307, Action: [[0 1]\n",
      " [2 2]\n",
      " [1 0]\n",
      " [1 2]\n",
      " [1 1]], Reward: -0.015226805085936303\n",
      "Step 308, Action: [[1 0]\n",
      " [2 2]\n",
      " [3 0]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.004773194914063698\n",
      "Step 309, Action: [[1 1]\n",
      " [3 2]\n",
      " [2 0]\n",
      " [0 0]\n",
      " [0 0]], Reward: -0.0052268050859363025\n",
      "Step 310, Action: [[3 0]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [3 1]], Reward: 0.024773194914063698\n",
      "Step 311, Action: [[3 0]\n",
      " [2 0]\n",
      " [3 1]\n",
      " [3 0]\n",
      " [0 0]], Reward: 0.024773194914063698\n",
      "Step 312, Action: [[1 0]\n",
      " [3 2]\n",
      " [2 1]\n",
      " [1 1]\n",
      " [0 1]], Reward: 0.004773194914063698\n",
      "Step 313, Action: [[3 2]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [2 1]], Reward: -0.005901699437494747\n",
      "Step 314, Action: [[1 2]\n",
      " [3 0]\n",
      " [3 2]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.024098300562505254\n",
      "Step 315, Action: [[2 2]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [0 1]], Reward: -0.005901699437494747\n",
      "Step 316, Action: [[1 1]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 1]], Reward: -0.015901699437494747\n",
      "Step 317, Action: [[0 1]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [1 0]\n",
      " [2 0]], Reward: -0.005901699437494747\n",
      "Step 318, Action: [[1 1]\n",
      " [2 1]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [0 0]], Reward: 0.004098300562505253\n",
      "Step 319, Action: [[3 1]\n",
      " [3 2]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [0 1]], Reward: 0.004098300562505253\n",
      "Step 320, Action: [[1 2]\n",
      " [2 0]\n",
      " [3 2]\n",
      " [3 0]\n",
      " [0 0]], Reward: 0.004773194914063698\n",
      "Step 321, Action: [[1 1]\n",
      " [2 2]\n",
      " [2 1]\n",
      " [1 2]\n",
      " [0 2]], Reward: -0.0052268050859363025\n",
      "Step 322, Action: [[0 1]\n",
      " [3 2]\n",
      " [1 1]\n",
      " [2 2]\n",
      " [0 0]], Reward: 0.004773194914063698\n",
      "Step 323, Action: [[0 1]\n",
      " [3 1]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [2 1]], Reward: 0.014773194914063698\n",
      "Step 324, Action: [[1 2]\n",
      " [3 2]\n",
      " [2 1]\n",
      " [3 0]\n",
      " [1 1]], Reward: 0.004773194914063698\n",
      "Step 325, Action: [[1 0]\n",
      " [2 0]\n",
      " [0 1]\n",
      " [3 2]\n",
      " [2 1]], Reward: -0.0052268050859363025\n",
      "Step 326, Action: [[1 2]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [0 2]], Reward: 0.004773194914063698\n",
      "Step 327, Action: [[1 2]\n",
      " [2 0]\n",
      " [3 1]\n",
      " [0 1]\n",
      " [2 0]], Reward: 0.004773194914063698\n",
      "Step 328, Action: [[1 2]\n",
      " [0 2]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 0]], Reward: -0.015226805085936303\n",
      "Step 329, Action: [[1 2]\n",
      " [2 0]\n",
      " [3 1]\n",
      " [3 0]\n",
      " [0 2]], Reward: 0.014773194914063698\n",
      "Step 330, Action: [[3 1]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [3 0]], Reward: 0.024773194914063698\n",
      "Step 331, Action: [[3 0]\n",
      " [3 1]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.024773194914063698\n",
      "Step 332, Action: [[1 2]\n",
      " [1 1]\n",
      " [1 2]\n",
      " [0 2]\n",
      " [0 1]], Reward: -0.02590169943749475\n",
      "Step 333, Action: [[3 0]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 1]], Reward: 0.004098300562505253\n",
      "Step 334, Action: [[3 0]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [2 1]], Reward: 0.004098300562505253\n",
      "Step 335, Action: [[3 2]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [0 2]\n",
      " [3 1]], Reward: 0.012991228745043106\n",
      "Step 336, Action: [[1 0]\n",
      " [2 2]\n",
      " [3 1]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.004098300562505253\n",
      "Step 337, Action: [[1 2]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [3 0]\n",
      " [0 1]], Reward: 0.014098300562505254\n",
      "Step 338, Action: [[1 1]\n",
      " [1 1]\n",
      " [0 2]\n",
      " [3 1]\n",
      " [0 1]], Reward: -0.007008771254956895\n",
      "Step 339, Action: [[3 1]\n",
      " [3 0]\n",
      " [0 2]\n",
      " [0 0]\n",
      " [1 1]], Reward: 0.0029912287450431056\n",
      "Step 340, Action: [[3 2]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [0 0]], Reward: 0.012991228745043106\n",
      "Step 341, Action: [[1 0]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [3 1]], Reward: 0.022991228745043106\n",
      "Step 342, Action: [[3 0]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [2 0]], Reward: -0.007008771254956895\n",
      "Step 343, Action: [[3 0]\n",
      " [2 2]\n",
      " [2 1]\n",
      " [0 1]\n",
      " [3 1]], Reward: -0.005901699437494747\n",
      "Step 344, Action: [[1 2]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [0 1]], Reward: 0.004098300562505253\n",
      "Step 345, Action: [[3 1]\n",
      " [3 2]\n",
      " [3 0]\n",
      " [3 2]\n",
      " [2 1]], Reward: 0.034098300562505256\n",
      "Step 346, Action: [[0 0]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [3 0]\n",
      " [1 1]], Reward: -0.005901699437494747\n",
      "Step 347, Action: [[0 2]\n",
      " [3 2]\n",
      " [1 0]\n",
      " [3 1]\n",
      " [2 1]], Reward: 0.004098300562505253\n",
      "Step 348, Action: [[1 0]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [3 1]], Reward: 0.014098300562505254\n",
      "Step 349, Action: [[3 2]\n",
      " [3 1]\n",
      " [2 2]\n",
      " [3 1]\n",
      " [0 2]], Reward: 0.014098300562505254\n",
      "Step 350, Action: [[3 2]\n",
      " [2 2]\n",
      " [2 0]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.004773194914063698\n",
      "Step 351, Action: [[1 2]\n",
      " [3 0]\n",
      " [3 1]\n",
      " [3 1]\n",
      " [0 0]], Reward: 0.024773194914063698\n",
      "Step 352, Action: [[1 1]\n",
      " [3 0]\n",
      " [3 2]\n",
      " [1 0]\n",
      " [3 1]], Reward: 0.024773194914063698\n",
      "Step 353, Action: [[1 2]\n",
      " [3 2]\n",
      " [2 2]\n",
      " [3 0]\n",
      " [0 0]], Reward: 0.014773194914063698\n",
      "Step 354, Action: [[3 1]\n",
      " [2 0]\n",
      " [2 2]\n",
      " [0 0]\n",
      " [2 1]], Reward: 0.004773194914063698\n",
      "Step 355, Action: [[0 2]\n",
      " [3 2]\n",
      " [3 2]\n",
      " [0 0]\n",
      " [0 1]], Reward: 0.014773194914063698\n",
      "Step 356, Action: [[1 0]\n",
      " [2 1]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.004773194914063698\n",
      "Step 357, Action: [[0 1]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [3 1]], Reward: 0.024773194914063698\n",
      "Step 358, Action: [[3 2]\n",
      " [3 2]\n",
      " [2 0]\n",
      " [3 0]\n",
      " [2 1]], Reward: 0.024773194914063698\n",
      "Step 359, Action: [[0 1]\n",
      " [3 2]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [1 1]], Reward: -0.0052268050859363025\n",
      "Step 360, Action: [[1 0]\n",
      " [3 2]\n",
      " [2 2]\n",
      " [3 1]\n",
      " [0 0]], Reward: 0.014773194914063698\n",
      "Step 361, Action: [[1 2]\n",
      " [2 0]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [1 1]], Reward: -0.015226805085936303\n",
      "Step 362, Action: [[1 1]\n",
      " [0 2]\n",
      " [0 2]\n",
      " [1 2]\n",
      " [2 1]], Reward: -0.015226805085936303\n",
      "Step 363, Action: [[1 0]\n",
      " [2 2]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [3 1]], Reward: 0.004773194914063698\n",
      "Step 364, Action: [[0 2]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [3 2]], Reward: 0.024773194914063698\n",
      "Step 365, Action: [[1 0]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [0 2]\n",
      " [0 0]], Reward: -0.0052268050859363025\n",
      "Step 366, Action: [[0 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [2 1]], Reward: -0.0052268050859363025\n",
      "Step 367, Action: [[0 1]\n",
      " [2 0]\n",
      " [2 2]\n",
      " [3 1]\n",
      " [1 1]], Reward: -0.0052268050859363025\n",
      "Step 368, Action: [[1 2]\n",
      " [3 2]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [0 0]], Reward: 0.004773194914063698\n",
      "Step 369, Action: [[1 2]\n",
      " [3 2]\n",
      " [3 1]\n",
      " [3 0]\n",
      " [1 1]], Reward: 0.014773194914063698\n",
      "Step 370, Action: [[0 1]\n",
      " [3 2]\n",
      " [2 0]\n",
      " [3 2]\n",
      " [0 0]], Reward: 0.014773194914063698\n",
      "Step 371, Action: [[1 2]\n",
      " [1 2]\n",
      " [2 2]\n",
      " [3 1]\n",
      " [2 1]], Reward: -0.015901699437494747\n",
      "Step 372, Action: [[1 2]\n",
      " [2 2]\n",
      " [3 2]\n",
      " [1 0]\n",
      " [0 0]], Reward: -0.0052268050859363025\n",
      "Step 373, Action: [[1 0]\n",
      " [3 1]\n",
      " [3 0]\n",
      " [0 1]\n",
      " [1 1]], Reward: 0.004773194914063698\n",
      "Step 374, Action: [[2 2]\n",
      " [3 2]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [3 1]], Reward: 0.014773194914063698\n",
      "Step 375, Action: [[1 0]\n",
      " [0 1]\n",
      " [2 0]\n",
      " [3 0]\n",
      " [0 0]], Reward: 0.004773194914063698\n",
      "Step 376, Action: [[1 2]\n",
      " [0 1]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [1 1]], Reward: 0.004773194914063698\n",
      "Step 377, Action: [[1 0]\n",
      " [1 2]\n",
      " [1 2]\n",
      " [2 0]\n",
      " [0 1]], Reward: -0.02590169943749475\n",
      "Step 378, Action: [[1 0]\n",
      " [3 0]\n",
      " [2 0]\n",
      " [3 0]\n",
      " [0 0]], Reward: 0.004098300562505253\n",
      "Step 379, Action: [[1 0]\n",
      " [2 2]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [0 1]], Reward: -0.015226805085936303\n",
      "Step 380, Action: [[1 2]\n",
      " [0 2]\n",
      " [0 2]\n",
      " [2 0]\n",
      " [0 1]], Reward: -0.0052268050859363025\n",
      "Step 381, Action: [[3 1]\n",
      " [1 2]\n",
      " [0 2]\n",
      " [3 0]\n",
      " [0 2]], Reward: 0.004098300562505253\n",
      "Step 382, Action: [[1 0]\n",
      " [2 0]\n",
      " [2 1]\n",
      " [1 2]\n",
      " [3 1]], Reward: -0.0052268050859363025\n",
      "Step 383, Action: [[1 0]\n",
      " [2 0]\n",
      " [0 1]\n",
      " [1 2]\n",
      " [0 1]], Reward: -0.0052268050859363025\n",
      "Step 384, Action: [[0 0]\n",
      " [2 2]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 0]], Reward: -0.0052268050859363025\n",
      "Step 385, Action: [[1 0]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [0 2]\n",
      " [3 1]], Reward: 0.014773194914063698\n",
      "Step 386, Action: [[1 2]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [3 0]\n",
      " [0 2]], Reward: 0.014773194914063698\n",
      "Step 387, Action: [[0 1]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [3 1]], Reward: 0.014773194914063698\n",
      "Step 388, Action: [[1 1]\n",
      " [0 0]\n",
      " [3 1]\n",
      " [3 1]\n",
      " [2 1]], Reward: 0.004773194914063698\n",
      "Step 389, Action: [[1 1]\n",
      " [1 0]\n",
      " [0 2]\n",
      " [3 1]\n",
      " [0 0]], Reward: -0.005901699437494747\n",
      "Step 390, Action: [[1 1]\n",
      " [0 1]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [0 1]], Reward: -0.005901699437494747\n",
      "Step 391, Action: [[1 2]\n",
      " [3 2]\n",
      " [0 1]\n",
      " [3 1]\n",
      " [1 0]], Reward: 0.004098300562505253\n",
      "Step 392, Action: [[1 0]\n",
      " [3 0]\n",
      " [0 2]\n",
      " [3 2]\n",
      " [2 1]], Reward: 0.004098300562505253\n",
      "Step 393, Action: [[0 0]\n",
      " [2 0]\n",
      " [0 0]\n",
      " [3 2]\n",
      " [1 1]], Reward: -0.005901699437494747\n",
      "Step 394, Action: [[1 2]\n",
      " [3 0]\n",
      " [2 2]\n",
      " [1 0]\n",
      " [0 0]], Reward: -0.005901699437494747\n",
      "Step 395, Action: [[1 1]\n",
      " [2 1]\n",
      " [1 2]\n",
      " [0 0]\n",
      " [2 1]], Reward: -0.025226805085936305\n",
      "Step 396, Action: [[1 1]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [0 1]\n",
      " [0 0]], Reward: 0.004773194914063698\n",
      "Step 397, Action: [[1 2]\n",
      " [2 0]\n",
      " [3 0]\n",
      " [3 1]\n",
      " [0 2]], Reward: 0.014773194914063698\n",
      "Step 398, Action: [[0 2]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [1 0]\n",
      " [2 1]], Reward: -0.015226805085936303\n",
      "Step 399, Action: [[0 0]\n",
      " [3 1]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [2 0]], Reward: 0.014773194914063698\n",
      "Step 400, Action: [[1 1]\n",
      " [3 0]\n",
      " [2 2]\n",
      " [3 0]\n",
      " [0 1]], Reward: 0.014773194914063698\n",
      "Step 401, Action: [[1 2]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [3 2]], Reward: 0.024773194914063698\n",
      "Step 402, Action: [[0 2]\n",
      " [0 2]\n",
      " [1 2]\n",
      " [0 1]\n",
      " [1 1]], Reward: -0.015226805085936303\n",
      "Step 403, Action: [[1 2]\n",
      " [3 0]\n",
      " [2 1]\n",
      " [0 1]\n",
      " [0 1]], Reward: 0.004773194914063698\n",
      "Step 404, Action: [[1 1]\n",
      " [3 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [2 1]], Reward: -0.0052268050859363025\n",
      "Step 405, Action: [[0 2]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]], Reward: 0.004773194914063698\n",
      "Step 406, Action: [[0 0]\n",
      " [3 2]\n",
      " [1 0]\n",
      " [3 1]\n",
      " [1 1]], Reward: 0.004773194914063698\n",
      "Step 407, Action: [[3 1]\n",
      " [3 2]\n",
      " [1 0]\n",
      " [1 2]\n",
      " [0 2]], Reward: 0.014773194914063698\n",
      "Step 408, Action: [[1 1]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [2 2]], Reward: -0.0052268050859363025\n",
      "Step 409, Action: [[1 2]\n",
      " [2 0]\n",
      " [3 2]\n",
      " [0 2]\n",
      " [0 0]], Reward: 0.004773194914063698\n",
      "Step 410, Action: [[3 2]\n",
      " [3 0]\n",
      " [3 2]\n",
      " [3 2]\n",
      " [3 1]], Reward: 0.0447731949140637\n",
      "Step 411, Action: [[0 2]\n",
      " [3 2]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [3 0]], Reward: 0.014773194914063698\n",
      "Step 412, Action: [[0 1]\n",
      " [2 2]\n",
      " [3 1]\n",
      " [1 0]\n",
      " [0 1]], Reward: 0.004773194914063698\n",
      "Step 413, Action: [[1 0]\n",
      " [3 0]\n",
      " [0 2]\n",
      " [0 1]\n",
      " [0 1]], Reward: 0.004773194914063698\n",
      "Step 414, Action: [[3 1]\n",
      " [3 0]\n",
      " [1 0]\n",
      " [3 1]\n",
      " [2 2]], Reward: 0.024773194914063698\n",
      "Step 415, Action: [[1 0]\n",
      " [2 2]\n",
      " [3 1]\n",
      " [0 1]\n",
      " [2 1]], Reward: 0.004773194914063698\n",
      "Step 416, Action: [[1 0]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [0 1]\n",
      " [0 1]], Reward: 0.004773194914063698\n",
      "Step 417, Action: [[3 2]\n",
      " [0 2]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [0 0]], Reward: 0.014773194914063698\n",
      "Step 418, Action: [[0 0]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [3 1]\n",
      " [1 0]], Reward: -0.005901699437494747\n",
      "Step 419, Action: [[3 2]\n",
      " [1 1]\n",
      " [1 2]\n",
      " [0 2]\n",
      " [1 2]], Reward: -0.027008771254956893\n",
      "Step 420, Action: [[1 0]\n",
      " [3 2]\n",
      " [0 2]\n",
      " [0 1]\n",
      " [0 1]], Reward: 0.0029912287450431056\n",
      "Step 421, Action: [[3 0]\n",
      " [3 2]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [2 1]], Reward: 0.0029912287450431056\n",
      "Step 422, Action: [[2 2]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [0 2]\n",
      " [0 1]], Reward: -0.018523499553598125\n",
      "Step 423, Action: [[0 1]\n",
      " [2 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [2 1]], Reward: -0.027008771254956897\n",
      "Step 424, Action: [[3 0]\n",
      " [3 2]\n",
      " [0 1]\n",
      " [3 2]\n",
      " [0 0]], Reward: 0.022991228745043106\n",
      "Step 425, Action: [[0 1]\n",
      " [2 1]\n",
      " [1 2]\n",
      " [0 1]\n",
      " [0 0]], Reward: -0.015901699437494747\n",
      "Step 426, Action: [[1 2]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [1 2]\n",
      " [0 1]], Reward: -0.015901699437494747\n",
      "Step 427, Action: [[0 2]\n",
      " [3 0]\n",
      " [3 2]\n",
      " [0 0]\n",
      " [0 1]], Reward: 0.014098300562505254\n",
      "Step 428, Action: [[3 1]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [1 0]], Reward: 0.004098300562505253\n",
      "Step 429, Action: [[3 2]\n",
      " [3 2]\n",
      " [1 1]\n",
      " [1 2]\n",
      " [3 0]], Reward: 0.024098300562505254\n",
      "Step 430, Action: [[3 1]\n",
      " [3 2]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [3 1]], Reward: 0.014098300562505254\n",
      "Step 431, Action: [[0 2]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [0 0]], Reward: -0.0052268050859363025\n",
      "Step 432, Action: [[1 1]\n",
      " [0 2]\n",
      " [0 2]\n",
      " [0 2]\n",
      " [1 1]], Reward: -0.015226805085936303\n",
      "Step 433, Action: [[0 2]\n",
      " [1 2]\n",
      " [2 1]\n",
      " [0 2]\n",
      " [3 1]], Reward: -0.005901699437494747\n",
      "Step 434, Action: [[3 0]\n",
      " [2 2]\n",
      " [1 0]\n",
      " [1 2]\n",
      " [0 2]], Reward: -0.0052268050859363025\n",
      "Step 435, Action: [[1 1]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [0 2]\n",
      " [0 0]], Reward: -0.02590169943749475\n",
      "Step 436, Action: [[3 0]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.024098300562505254\n",
      "Step 437, Action: [[1 0]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [3 2]\n",
      " [2 1]], Reward: -0.007008771254956895\n",
      "Step 438, Action: [[3 0]\n",
      " [0 2]\n",
      " [1 2]\n",
      " [0 2]\n",
      " [3 1]], Reward: 0.0029912287450431056\n",
      "Step 439, Action: [[0 0]\n",
      " [3 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [2 1]], Reward: -0.007008771254956895\n",
      "Step 440, Action: [[0 2]\n",
      " [2 0]\n",
      " [3 0]\n",
      " [3 2]\n",
      " [1 1]], Reward: 0.0029912287450431056\n",
      "Step 441, Action: [[0 2]\n",
      " [3 1]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [2 1]], Reward: 0.0029912287450431056\n",
      "Step 442, Action: [[1 0]\n",
      " [3 0]\n",
      " [2 1]\n",
      " [0 1]\n",
      " [0 0]], Reward: -0.007008771254956895\n",
      "Step 443, Action: [[1 1]\n",
      " [1 2]\n",
      " [1 2]\n",
      " [0 1]\n",
      " [0 0]], Reward: -0.028523499553598126\n",
      "Step 444, Action: [[1 2]\n",
      " [2 2]\n",
      " [0 1]\n",
      " [0 2]\n",
      " [3 2]], Reward: -0.007008771254956895\n",
      "Step 445, Action: [[3 1]\n",
      " [1 2]\n",
      " [1 2]\n",
      " [0 0]\n",
      " [2 1]], Reward: -0.018523499553598125\n",
      "Step 446, Action: [[0 0]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [0 1]\n",
      " [1 2]], Reward: -0.018523499553598125\n",
      "Step 447, Action: [[0 1]\n",
      " [3 0]\n",
      " [2 0]\n",
      " [0 1]\n",
      " [1 1]], Reward: -0.018523499553598125\n",
      "Step 448, Action: [[0 1]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [0 1]\n",
      " [0 0]], Reward: -0.008523499553598124\n",
      "Step 449, Action: [[1 1]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [2 1]], Reward: 0.001476500446401876\n",
      "Step 450, Action: [[1 0]\n",
      " [3 2]\n",
      " [1 1]\n",
      " [0 2]\n",
      " [2 1]], Reward: -0.008523499553598124\n",
      "Step 451, Action: [[2 0]\n",
      " [3 2]\n",
      " [3 1]\n",
      " [0 0]\n",
      " [3 0]], Reward: 0.011476500446401876\n",
      "Step 452, Action: [[1 0]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [0 1]\n",
      " [2 1]], Reward: -0.018523499553598125\n",
      "Step 453, Action: [[0 0]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [0 0]], Reward: 0.011476500446401876\n",
      "Step 454, Action: [[3 2]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [3 1]], Reward: 0.021476500446401876\n",
      "Step 455, Action: [[1 1]\n",
      " [2 0]\n",
      " [2 1]\n",
      " [3 0]\n",
      " [1 0]], Reward: -0.027008771254956893\n",
      "Step 456, Action: [[0 1]\n",
      " [2 2]\n",
      " [2 1]\n",
      " [3 0]\n",
      " [2 2]], Reward: -0.025901699437494746\n",
      "Step 457, Action: [[0 1]\n",
      " [2 2]\n",
      " [3 2]\n",
      " [0 1]\n",
      " [3 0]], Reward: 0.014098300562505254\n",
      "Step 458, Action: [[1 2]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [0 2]\n",
      " [2 1]], Reward: -0.005901699437494747\n",
      "Step 459, Action: [[1 1]\n",
      " [3 2]\n",
      " [2 0]\n",
      " [0 0]\n",
      " [1 0]], Reward: -0.015901699437494747\n",
      "Step 460, Action: [[1 0]\n",
      " [3 2]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [1 1]], Reward: -0.005901699437494747\n",
      "Step 461, Action: [[0 2]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [3 1]], Reward: 0.004098300562505253\n",
      "Step 462, Action: [[2 0]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [1 2]\n",
      " [1 1]], Reward: -0.005901699437494747\n",
      "Step 463, Action: [[3 2]\n",
      " [2 0]\n",
      " [3 2]\n",
      " [0 2]\n",
      " [2 1]], Reward: 0.004098300562505253\n",
      "Step 464, Action: [[0 0]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 0]], Reward: -0.005901699437494747\n",
      "Step 465, Action: [[1 1]\n",
      " [3 2]\n",
      " [0 2]\n",
      " [1 2]\n",
      " [0 0]], Reward: 0.004098300562505253\n",
      "Step 466, Action: [[0 0]\n",
      " [3 0]\n",
      " [2 2]\n",
      " [0 2]\n",
      " [0 1]], Reward: -0.005901699437494747\n",
      "Step 467, Action: [[0 0]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [0 2]\n",
      " [1 0]], Reward: -0.03700877125495689\n",
      "Step 468, Action: [[3 0]\n",
      " [1 2]\n",
      " [0 1]\n",
      " [3 2]\n",
      " [2 1]], Reward: -0.008523499553598124\n",
      "Step 469, Action: [[0 0]\n",
      " [3 2]\n",
      " [3 1]\n",
      " [1 2]\n",
      " [1 1]], Reward: 0.001476500446401876\n",
      "Step 470, Action: [[1 0]\n",
      " [2 2]\n",
      " [3 1]\n",
      " [1 1]\n",
      " [0 1]], Reward: -0.007008771254956895\n",
      "Step 471, Action: [[3 2]\n",
      " [2 2]\n",
      " [1 0]\n",
      " [0 2]\n",
      " [1 1]], Reward: -0.005901699437494747\n",
      "Step 472, Action: [[0 0]\n",
      " [3 2]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [2 1]], Reward: 0.004098300562505253\n",
      "Step 473, Action: [[1 2]\n",
      " [3 2]\n",
      " [2 1]\n",
      " [1 1]\n",
      " [0 0]], Reward: -0.005901699437494747\n",
      "Step 474, Action: [[1 0]\n",
      " [2 2]\n",
      " [0 2]\n",
      " [0 1]\n",
      " [0 0]], Reward: -0.015226805085936303\n",
      "Step 475, Action: [[0 1]\n",
      " [3 2]\n",
      " [2 2]\n",
      " [0 2]\n",
      " [0 1]], Reward: 0.004773194914063698\n",
      "Step 476, Action: [[3 2]\n",
      " [3 0]\n",
      " [0 2]\n",
      " [1 1]\n",
      " [0 1]], Reward: 0.014773194914063698\n",
      "Step 477, Action: [[1 2]\n",
      " [3 2]\n",
      " [2 1]\n",
      " [3 2]\n",
      " [2 0]], Reward: 0.004773194914063698\n",
      "Step 478, Action: [[0 1]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [0 2]\n",
      " [2 1]], Reward: 0.004773194914063698\n",
      "Step 479, Action: [[3 1]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [0 2]\n",
      " [3 0]], Reward: 0.014773194914063698\n",
      "Step 480, Action: [[1 1]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.004773194914063698\n",
      "Step 481, Action: [[0 0]\n",
      " [3 2]\n",
      " [1 0]\n",
      " [3 2]\n",
      " [1 0]], Reward: 0.043592855026564764\n",
      "Step 482, Action: [[1 1]\n",
      " [2 0]\n",
      " [2 2]\n",
      " [0 1]\n",
      " [0 0]], Reward: 0.03359285502656476\n",
      "Step 483, Action: [[0 2]\n",
      " [2 0]\n",
      " [0 2]\n",
      " [0 2]\n",
      " [1 1]], Reward: 0.02359285502656476\n",
      "Step 484, Action: [[1 1]\n",
      " [1 2]\n",
      " [1 2]\n",
      " [1 2]\n",
      " [0 2]], Reward: 0.01409830056250527\n",
      "Step 485, Action: [[0 1]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.04409830056250527\n",
      "Step 486, Action: [[0 0]\n",
      " [3 1]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [2 1]], Reward: 0.03409830056250527\n",
      "Step 487, Action: [[0 1]\n",
      " [2 0]\n",
      " [3 1]\n",
      " [0 2]\n",
      " [0 0]], Reward: 0.04409830056250527\n",
      "Step 488, Action: [[1 1]\n",
      " [0 2]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.04409830056250527\n",
      "Step 489, Action: [[3 1]\n",
      " [0 2]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 1]], Reward: 0.04409830056250527\n",
      "Step 490, Action: [[1 0]\n",
      " [3 0]\n",
      " [1 0]\n",
      " [3 2]\n",
      " [1 2]], Reward: 0.04409830056250527\n",
      "Step 491, Action: [[3 0]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [0 0]], Reward: 0.06409830056250528\n",
      "Step 492, Action: [[1 2]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [3 1]], Reward: 0.054098300562505273\n",
      "Step 493, Action: [[3 1]\n",
      " [1 2]\n",
      " [2 2]\n",
      " [3 1]\n",
      " [2 1]], Reward: 0.021810888857544175\n",
      "Step 494, Action: [[1 1]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [3 1]], Reward: 0.034098300562505277\n",
      "Step 495, Action: [[0 0]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [1 0]\n",
      " [2 0]], Reward: 0.024098300562505268\n",
      "Step 496, Action: [[1 2]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [0 0]\n",
      " [2 1]], Reward: 0.04409830056250527\n",
      "Step 497, Action: [[1 1]\n",
      " [2 0]\n",
      " [2 2]\n",
      " [3 1]\n",
      " [1 1]], Reward: 0.013592855026564765\n",
      "Step 498, Action: [[1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.02409830056250527\n",
      "Step 499, Action: [[1 1]\n",
      " [2 0]\n",
      " [2 1]\n",
      " [3 0]\n",
      " [0 1]], Reward: 0.023592855026564764\n",
      "Step 500, Action: [[1 1]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [0 2]], Reward: 0.03359285502656476\n",
      "Step 501, Action: [[1 2]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [0 2]], Reward: 0.053592855026564766\n",
      "Step 502, Action: [[1 0]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [0 2]\n",
      " [0 1]], Reward: 0.03359285502656476\n",
      "Step 503, Action: [[1 1]\n",
      " [3 0]\n",
      " [2 2]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.053592855026564766\n",
      "Step 504, Action: [[3 1]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [0 0]\n",
      " [0 1]], Reward: 0.053592855026564766\n",
      "Step 505, Action: [[1 1]\n",
      " [2 0]\n",
      " [2 2]\n",
      " [0 1]\n",
      " [0 2]], Reward: 0.03359285502656476\n",
      "Step 506, Action: [[1 2]\n",
      " [2 2]\n",
      " [1 0]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.043592855026564764\n",
      "Step 507, Action: [[1 1]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [0 0]], Reward: 0.053592855026564766\n",
      "Step 508, Action: [[1 0]\n",
      " [3 0]\n",
      " [2 2]\n",
      " [3 0]\n",
      " [1 2]], Reward: 0.043592855026564764\n",
      "Step 509, Action: [[1 2]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [2 1]], Reward: 0.03359285502656476\n",
      "Step 510, Action: [[1 0]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [0 2]\n",
      " [0 1]], Reward: 0.03359285502656476\n",
      "Step 511, Action: [[0 1]\n",
      " [0 0]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [0 1]], Reward: 0.043592855026564764\n",
      "Step 512, Action: [[1 2]\n",
      " [3 0]\n",
      " [2 2]\n",
      " [0 0]\n",
      " [3 1]], Reward: 0.053592855026564766\n",
      "Step 513, Action: [[1 1]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [0 1]], Reward: 0.053592855026564766\n",
      "Step 514, Action: [[1 2]\n",
      " [3 2]\n",
      " [2 2]\n",
      " [3 1]\n",
      " [0 0]], Reward: 0.053592855026564766\n",
      "Step 515, Action: [[1 0]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [1 0]], Reward: 0.043592855026564764\n",
      "Step 516, Action: [[0 1]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [2 1]], Reward: 0.03359285502656476\n",
      "Step 517, Action: [[1 1]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [0 2]\n",
      " [0 1]], Reward: 0.043592855026564764\n",
      "Step 518, Action: [[1 0]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [0 0]], Reward: 0.043592855026564764\n",
      "Step 519, Action: [[1 1]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [1 2]\n",
      " [2 1]], Reward: 0.03359285502656476\n",
      "Step 520, Action: [[1 1]\n",
      " [2 2]\n",
      " [3 2]\n",
      " [0 0]\n",
      " [2 1]], Reward: 0.043592855026564764\n",
      "Step 521, Action: [[1 0]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [1 2]], Reward: 0.03359285502656476\n",
      "Step 522, Action: [[1 1]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [1 1]], Reward: 0.043592855026564764\n",
      "Step 523, Action: [[0 1]\n",
      " [3 2]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [1 1]], Reward: 0.043592855026564764\n",
      "Step 524, Action: [[1 1]\n",
      " [2 0]\n",
      " [2 2]\n",
      " [0 1]\n",
      " [0 2]], Reward: 0.03359285502656476\n",
      "Step 525, Action: [[1 0]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [0 1]\n",
      " [0 2]], Reward: 0.03359285502656476\n",
      "Step 526, Action: [[1 2]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [3 1]\n",
      " [2 1]], Reward: 0.03359285502656476\n",
      "Step 527, Action: [[3 0]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [0 1]], Reward: 0.053592855026564766\n",
      "Step 528, Action: [[1 0]\n",
      " [3 0]\n",
      " [2 2]\n",
      " [3 1]\n",
      " [2 1]], Reward: 0.043592855026564764\n",
      "Step 529, Action: [[0 1]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [0 0]], Reward: 0.043592855026564764\n",
      "Step 530, Action: [[1 1]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [0 2]], Reward: 0.043592855026564764\n",
      "Step 531, Action: [[1 1]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [0 0]], Reward: 0.043592855026564764\n",
      "Step 532, Action: [[1 0]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [1 1]], Reward: 0.03359285502656476\n",
      "Step 533, Action: [[3 0]\n",
      " [0 2]\n",
      " [1 0]\n",
      " [3 1]\n",
      " [0 0]], Reward: 0.053592855026564766\n",
      "Step 534, Action: [[1 1]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [3 0]], Reward: 0.053592855026564766\n",
      "Step 535, Action: [[1 2]\n",
      " [2 0]\n",
      " [0 2]\n",
      " [3 0]\n",
      " [0 1]], Reward: 0.043592855026564764\n",
      "Step 536, Action: [[0 1]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [2 2]], Reward: 0.03359285502656476\n",
      "Step 537, Action: [[1 1]\n",
      " [0 0]\n",
      " [3 1]\n",
      " [0 1]\n",
      " [2 0]], Reward: 0.043592855026564764\n",
      "Step 538, Action: [[1 0]\n",
      " [2 2]\n",
      " [1 0]\n",
      " [0 2]\n",
      " [1 0]], Reward: 0.02359285502656476\n",
      "Step 539, Action: [[1 2]\n",
      " [3 0]\n",
      " [1 0]\n",
      " [3 2]\n",
      " [3 0]], Reward: 0.06359285502656477\n",
      "Step 540, Action: [[3 1]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [1 0]], Reward: 0.053592855026564766\n",
      "Step 541, Action: [[1 0]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.053592855026564766\n",
      "Step 542, Action: [[1 1]\n",
      " [3 0]\n",
      " [2 2]\n",
      " [0 1]\n",
      " [1 0]], Reward: 0.043592855026564764\n",
      "Step 543, Action: [[1 1]\n",
      " [1 0]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [3 0]], Reward: 0.034098300562505277\n",
      "Step 544, Action: [[1 1]\n",
      " [2 2]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]], Reward: 0.03409830056250527\n",
      "Step 545, Action: [[1 1]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.04409830056250527\n",
      "Step 546, Action: [[3 2]\n",
      " [0 2]\n",
      " [1 2]\n",
      " [0 2]\n",
      " [0 1]], Reward: 0.04409830056250527\n",
      "Step 547, Action: [[1 2]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [0 1]], Reward: 0.03409830056250527\n",
      "Step 548, Action: [[1 0]\n",
      " [3 2]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [0 0]], Reward: 0.04409830056250527\n",
      "Step 549, Action: [[1 0]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [3 1]], Reward: 0.06409830056250528\n",
      "Step 550, Action: [[1 1]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.054098300562505273\n",
      "Step 551, Action: [[1 2]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [0 2]\n",
      " [3 1]], Reward: 0.04409830056250527\n",
      "Step 552, Action: [[1 2]\n",
      " [2 2]\n",
      " [0 2]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.04409830056250527\n",
      "Step 553, Action: [[1 1]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [3 0]], Reward: 0.06409830056250528\n",
      "Step 554, Action: [[1 1]\n",
      " [2 0]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [3 1]], Reward: 0.023592855026564764\n",
      "Step 555, Action: [[1 0]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]], Reward: 0.03359285502656476\n",
      "Step 556, Action: [[0 1]\n",
      " [3 0]\n",
      " [0 2]\n",
      " [0 1]\n",
      " [0 1]], Reward: 0.043592855026564764\n",
      "Step 557, Action: [[1 1]\n",
      " [2 2]\n",
      " [2 2]\n",
      " [0 1]\n",
      " [2 1]], Reward: 0.02359285502656476\n",
      "Step 558, Action: [[1 1]\n",
      " [3 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [0 1]], Reward: 0.043592855026564764\n",
      "Step 559, Action: [[0 1]\n",
      " [3 0]\n",
      " [0 1]\n",
      " [1 1]\n",
      " [1 1]], Reward: 0.03359285502656476\n",
      "Step 560, Action: [[1 2]\n",
      " [2 2]\n",
      " [1 0]\n",
      " [0 2]\n",
      " [0 0]], Reward: 0.03359285502656476\n",
      "Step 561, Action: [[1 1]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [0 0]\n",
      " [3 0]], Reward: 0.043592855026564764\n",
      "Step 562, Action: [[1 2]\n",
      " [1 2]\n",
      " [2 0]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.03291796067500632\n",
      "Step 563, Action: [[1 2]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [3 2]\n",
      " [3 2]], Reward: 0.051810888857544174\n",
      "Step 564, Action: [[1 0]\n",
      " [3 2]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [1 1]], Reward: 0.04299122874504312\n",
      "Step 565, Action: [[3 0]\n",
      " [2 2]\n",
      " [0 1]\n",
      " [0 2]\n",
      " [1 0]], Reward: 0.03409830056250527\n",
      "Step 566, Action: [[1 0]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [3 0]], Reward: 0.054098300562505273\n",
      "Step 567, Action: [[1 2]\n",
      " [2 2]\n",
      " [0 2]\n",
      " [3 2]\n",
      " [3 1]], Reward: 0.054098300562505273\n",
      "Step 568, Action: [[1 2]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [0 1]\n",
      " [2 0]], Reward: 0.03409830056250527\n",
      "Step 569, Action: [[1 0]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [3 1]], Reward: 0.06409830056250528\n",
      "Step 570, Action: [[1 0]\n",
      " [2 2]\n",
      " [0 2]\n",
      " [0 0]\n",
      " [3 1]], Reward: 0.04409830056250527\n",
      "Step 571, Action: [[1 2]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [3 1]], Reward: 0.054098300562505273\n",
      "Step 572, Action: [[3 2]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [0 2]\n",
      " [0 1]], Reward: 0.054098300562505273\n",
      "Step 573, Action: [[3 1]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [0 0]], Reward: 0.054098300562505273\n",
      "Step 574, Action: [[1 0]\n",
      " [3 0]\n",
      " [3 1]\n",
      " [0 0]\n",
      " [0 0]], Reward: 0.054098300562505273\n",
      "Step 575, Action: [[1 1]\n",
      " [3 2]\n",
      " [2 1]\n",
      " [1 2]\n",
      " [1 2]], Reward: 0.02291796067500632\n",
      "Step 576, Action: [[0 0]\n",
      " [3 0]\n",
      " [2 2]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.05291796067500632\n",
      "Step 577, Action: [[0 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.03409830056250527\n",
      "Step 578, Action: [[1 1]\n",
      " [2 0]\n",
      " [0 2]\n",
      " [3 1]\n",
      " [0 0]], Reward: 0.04409830056250527\n",
      "Step 579, Action: [[1 2]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [3 0]\n",
      " [0 1]], Reward: 0.054098300562505273\n",
      "Step 580, Action: [[1 1]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [0 1]], Reward: 0.04409830056250527\n",
      "Step 581, Action: [[1 0]\n",
      " [2 2]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [0 1]], Reward: 0.013592855026564762\n",
      "Step 582, Action: [[3 1]\n",
      " [2 0]\n",
      " [2 2]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.053592855026564766\n",
      "Step 583, Action: [[1 1]\n",
      " [2 0]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [2 2]], Reward: 0.02359285502656476\n",
      "Step 584, Action: [[0 1]\n",
      " [2 0]\n",
      " [3 2]\n",
      " [3 0]\n",
      " [0 1]], Reward: 0.053592855026564766\n",
      "Step 585, Action: [[1 2]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [0 0]], Reward: 0.01409830056250527\n",
      "Step 586, Action: [[1 2]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.054098300562505273\n",
      "Step 587, Action: [[1 1]\n",
      " [3 0]\n",
      " [0 2]\n",
      " [3 1]\n",
      " [1 2]], Reward: 0.04409830056250527\n",
      "Step 588, Action: [[1 0]\n",
      " [0 2]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [1 0]], Reward: 0.04409830056250527\n",
      "Step 589, Action: [[1 2]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [0 2]\n",
      " [0 2]], Reward: 0.03409830056250527\n",
      "Step 590, Action: [[1 0]\n",
      " [2 0]\n",
      " [0 2]\n",
      " [1 1]\n",
      " [3 0]], Reward: 0.04409830056250527\n",
      "Step 591, Action: [[1 0]\n",
      " [2 2]\n",
      " [0 2]\n",
      " [0 2]\n",
      " [0 1]], Reward: 0.03409830056250527\n",
      "Step 592, Action: [[1 1]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [0 1]\n",
      " [2 1]], Reward: 0.03409830056250527\n",
      "Step 593, Action: [[1 0]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [0 0]\n",
      " [0 0]], Reward: 0.04409830056250527\n",
      "Step 594, Action: [[1 1]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [0 0]], Reward: 0.04409830056250527\n",
      "Step 595, Action: [[1 0]\n",
      " [1 0]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [2 1]], Reward: 0.011810888857544173\n",
      "Step 596, Action: [[1 0]\n",
      " [2 2]\n",
      " [3 2]\n",
      " [1 0]\n",
      " [2 1]], Reward: 0.04181088885754417\n",
      "Step 597, Action: [[1 0]\n",
      " [3 0]\n",
      " [2 0]\n",
      " [3 1]\n",
      " [1 1]], Reward: 0.03299122874504312\n",
      "Step 598, Action: [[1 0]\n",
      " [3 2]\n",
      " [0 1]\n",
      " [3 2]\n",
      " [3 1]], Reward: 0.06299122874504312\n",
      "Step 599, Action: [[1 1]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [1 0]\n",
      " [0 1]], Reward: 0.024098300562505268\n",
      "Step 600, Action: [[1 1]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [0 1]], Reward: 0.01181088885754417\n",
      "Step 601, Action: [[1 1]\n",
      " [1 0]\n",
      " [0 2]\n",
      " [3 1]\n",
      " [0 0]], Reward: 0.030296160558902926\n",
      "Step 602, Action: [[1 0]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [3 1]], Reward: 0.047334364822670935\n",
      "Step 603, Action: [[1 1]\n",
      " [3 0]\n",
      " [1 0]\n",
      " [3 0]\n",
      " [0 2]], Reward: 0.047334364822670935\n",
      "Step 604, Action: [[1 1]\n",
      " [1 0]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [2 0]], Reward: 0.0015570137547071816\n",
      "Step 605, Action: [[3 1]\n",
      " [1 0]\n",
      " [1 2]\n",
      " [0 2]\n",
      " [0 1]], Reward: 0.03155701375470718\n",
      "Step 606, Action: [[1 2]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [0 1]], Reward: 0.04155701375470718\n",
      "Step 607, Action: [[1 1]\n",
      " [0 0]\n",
      " [2 2]\n",
      " [1 0]\n",
      " [0 0]], Reward: 0.015442634508296188\n",
      "Step 608, Action: [[1 0]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [0 0]], Reward: 0.02733436482267093\n",
      "Step 609, Action: [[1 1]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [1 1]], Reward: 0.03733436482267093\n",
      "Step 610, Action: [[1 1]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.03733436482267093\n",
      "Step 611, Action: [[1 2]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [0 2]\n",
      " [0 0]], Reward: 0.01181088885754417\n",
      "Step 612, Action: [[1 0]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [3 0]\n",
      " [0 0]], Reward: 0.04181088885754417\n",
      "Step 613, Action: [[1 0]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [3 1]], Reward: 0.04181088885754417\n",
      "Step 614, Action: [[1 0]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [0 1]], Reward: 0.04181088885754417\n",
      "Step 615, Action: [[1 1]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.04181088885754417\n",
      "Step 616, Action: [[3 1]\n",
      " [1 0]\n",
      " [0 2]\n",
      " [3 2]\n",
      " [1 1]], Reward: 0.03029616055890293\n",
      "Step 617, Action: [[1 0]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [0 0]], Reward: 0.03733436482267093\n",
      "Step 618, Action: [[1 1]\n",
      " [3 2]\n",
      " [2 2]\n",
      " [0 1]\n",
      " [0 1]], Reward: 0.030296160558902926\n",
      "Step 619, Action: [[3 1]\n",
      " [3 0]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [1 1]], Reward: 0.06029616055890293\n",
      "Step 620, Action: [[1 0]\n",
      " [2 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 0]], Reward: 0.021810888857544168\n",
      "Step 621, Action: [[1 0]\n",
      " [3 0]\n",
      " [3 2]\n",
      " [0 1]\n",
      " [3 0]], Reward: 0.061810888857544176\n",
      "Step 622, Action: [[1 1]\n",
      " [1 2]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [1 1]], Reward: 0.017334364822670933\n",
      "Step 623, Action: [[1 1]\n",
      " [3 2]\n",
      " [2 2]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.04029616055890293\n",
      "Step 624, Action: [[1 0]\n",
      " [3 0]\n",
      " [2 1]\n",
      " [0 0]\n",
      " [0 0]], Reward: 0.03147650044640189\n",
      "Step 625, Action: [[3 0]\n",
      " [1 0]\n",
      " [3 0]\n",
      " [3 2]\n",
      " [2 1]], Reward: 0.039584770132027144\n",
      "Step 626, Action: [[1 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.020296160558902928\n",
      "Step 627, Action: [[1 2]\n",
      " [2 2]\n",
      " [0 2]\n",
      " [0 0]\n",
      " [3 1]], Reward: 0.03181088885754417\n",
      "Step 628, Action: [[1 1]\n",
      " [3 0]\n",
      " [3 2]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.061810888857544176\n",
      "Step 629, Action: [[1 1]\n",
      " [0 2]\n",
      " [1 0]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.032991228745043115\n",
      "Step 630, Action: [[1 2]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [3 1]\n",
      " [1 0]], Reward: 0.02409830056250527\n",
      "Step 631, Action: [[1 2]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.04409830056250527\n",
      "Step 632, Action: [[1 1]\n",
      " [3 0]\n",
      " [0 2]\n",
      " [3 1]\n",
      " [0 0]], Reward: 0.054098300562505273\n",
      "Step 633, Action: [[1 1]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [0 2]\n",
      " [0 1]], Reward: 0.04409830056250527\n",
      "Step 634, Action: [[1 1]\n",
      " [3 0]\n",
      " [2 0]\n",
      " [3 1]\n",
      " [2 2]], Reward: 0.032917960675006325\n",
      "Step 635, Action: [[1 1]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [0 1]], Reward: 0.04409830056250527\n",
      "Step 636, Action: [[1 2]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [2 1]], Reward: 0.04409830056250527\n",
      "Step 637, Action: [[1 0]\n",
      " [1 2]\n",
      " [0 2]\n",
      " [3 0]\n",
      " [0 1]], Reward: 0.032991228745043115\n",
      "Step 638, Action: [[1 1]\n",
      " [3 0]\n",
      " [1 0]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.04181088885754417\n",
      "Step 639, Action: [[3 2]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.061810888857544176\n",
      "Step 640, Action: [[1 0]\n",
      " [1 0]\n",
      " [1 2]\n",
      " [1 0]\n",
      " [3 1]], Reward: 0.017334364822670933\n",
      "Step 641, Action: [[1 1]\n",
      " [2 0]\n",
      " [3 2]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.047334364822670935\n",
      "Step 642, Action: [[1 0]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [0 2]\n",
      " [0 2]], Reward: 0.02733436482267093\n",
      "Step 643, Action: [[1 1]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [2 0]], Reward: 0.03733436482267093\n",
      "Step 644, Action: [[1 2]\n",
      " [3 0]\n",
      " [1 0]\n",
      " [3 0]\n",
      " [3 1]], Reward: 0.05733436482267094\n",
      "Step 645, Action: [[1 0]\n",
      " [3 0]\n",
      " [3 2]\n",
      " [0 1]\n",
      " [0 0]], Reward: 0.047334364822670935\n",
      "Step 646, Action: [[1 1]\n",
      " [2 1]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.03733436482267093\n",
      "Step 647, Action: [[0 0]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [3 1]], Reward: 0.047334364822670935\n",
      "Step 648, Action: [[3 1]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [0 1]], Reward: 0.03733436482267093\n",
      "Step 649, Action: [[1 2]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [0 1]], Reward: 0.047334364822670935\n",
      "Step 650, Action: [[3 2]\n",
      " [3 0]\n",
      " [2 2]\n",
      " [3 2]\n",
      " [1 0]], Reward: 0.040296160558902935\n",
      "Step 651, Action: [[1 0]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [3 1]], Reward: 0.02733436482267093\n",
      "Step 652, Action: [[1 0]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [1 0]\n",
      " [0 1]], Reward: 0.02733436482267093\n",
      "Step 653, Action: [[1 1]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [0 2]], Reward: 0.047334364822670935\n",
      "Step 654, Action: [[1 2]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [3 0]], Reward: 0.047334364822670935\n",
      "Step 655, Action: [[1 0]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.047334364822670935\n",
      "Step 656, Action: [[1 1]\n",
      " [2 0]\n",
      " [3 0]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.047334364822670935\n",
      "Step 657, Action: [[1 1]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [0 2]\n",
      " [0 0]], Reward: 0.02733436482267093\n",
      "Step 658, Action: [[1 1]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [0 0]], Reward: 0.047334364822670935\n",
      "Step 659, Action: [[1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.03733436482267093\n",
      "Step 660, Action: [[0 1]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]], Reward: 0.02733436482267093\n",
      "Step 661, Action: [[0 0]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [1 0]], Reward: 0.02733436482267093\n",
      "Step 662, Action: [[1 1]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [0 0]], Reward: 0.047334364822670935\n",
      "Step 663, Action: [[1 2]\n",
      " [0 0]\n",
      " [0 1]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.03733436482267093\n",
      "Step 664, Action: [[3 1]\n",
      " [1 0]\n",
      " [2 2]\n",
      " [3 2]\n",
      " [0 0]], Reward: 0.0284044302445282\n",
      "Step 665, Action: [[1 1]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.03544263450829619\n",
      "Step 666, Action: [[1 1]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [0 2]\n",
      " [0 1]], Reward: 0.01733436482267093\n",
      "Step 667, Action: [[1 1]\n",
      " [3 2]\n",
      " [0 1]\n",
      " [0 0]\n",
      " [0 2]], Reward: 0.03733436482267093\n",
      "Step 668, Action: [[1 1]\n",
      " [2 1]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [1 1]], Reward: 0.03733436482267093\n",
      "Step 669, Action: [[1 2]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [0 0]\n",
      " [2 0]], Reward: 0.01733436482267093\n",
      "Step 670, Action: [[1 1]\n",
      " [1 0]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [0 1]], Reward: 0.01155701375470718\n",
      "Step 671, Action: [[1 2]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [3 1]\n",
      " [2 1]], Reward: 0.02155701375470718\n",
      "Step 672, Action: [[1 2]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [3 2]], Reward: 0.04155701375470718\n",
      "Step 673, Action: [[1 2]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [0 1]\n",
      " [0 1]], Reward: 0.03155701375470718\n",
      "Step 674, Action: [[1 0]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [0 2]], Reward: 0.03155701375470718\n",
      "Step 675, Action: [[1 1]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [3 1]\n",
      " [0 0]], Reward: 0.02544263450829619\n",
      "Step 676, Action: [[0 0]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.02733436482267093\n",
      "Step 677, Action: [[1 1]\n",
      " [0 2]\n",
      " [0 0]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.03733436482267093\n",
      "Step 678, Action: [[0 1]\n",
      " [3 0]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [0 1]], Reward: 0.030296160558902926\n",
      "Step 679, Action: [[3 0]\n",
      " [3 2]\n",
      " [0 1]\n",
      " [0 2]\n",
      " [3 1]], Reward: 0.06029616055890293\n",
      "Step 680, Action: [[3 1]\n",
      " [1 2]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.025442634508296192\n",
      "Step 681, Action: [[0 2]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [0 1]], Reward: 0.01733436482267093\n",
      "Step 682, Action: [[0 1]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [0 0]], Reward: 0.03733436482267093\n",
      "Step 683, Action: [[1 2]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.03733436482267093\n",
      "Step 684, Action: [[1 1]\n",
      " [3 0]\n",
      " [1 0]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.047334364822670935\n",
      "Step 685, Action: [[1 1]\n",
      " [2 2]\n",
      " [2 2]\n",
      " [3 2]\n",
      " [1 1]], Reward: 0.011810888857544173\n",
      "Step 686, Action: [[1 2]\n",
      " [2 2]\n",
      " [2 2]\n",
      " [3 1]\n",
      " [0 2]], Reward: 0.02409830056250527\n",
      "Step 687, Action: [[0 0]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [0 2]\n",
      " [0 1]], Reward: 0.03409830056250527\n",
      "Step 688, Action: [[3 0]\n",
      " [3 0]\n",
      " [2 1]\n",
      " [3 1]\n",
      " [0 0]], Reward: 0.05291796067500632\n",
      "Step 689, Action: [[1 0]\n",
      " [2 0]\n",
      " [0 2]\n",
      " [0 0]\n",
      " [0 1]], Reward: 0.02359285502656476\n",
      "Step 690, Action: [[1 0]\n",
      " [0 2]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.043592855026564764\n",
      "Step 691, Action: [[1 1]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [0 2]\n",
      " [0 1]], Reward: 0.03359285502656476\n",
      "Step 692, Action: [[0 2]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [0 0]], Reward: 0.043592855026564764\n",
      "Step 693, Action: [[1 2]\n",
      " [3 0]\n",
      " [3 2]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.06359285502656477\n",
      "Step 694, Action: [[3 1]\n",
      " [3 2]\n",
      " [1 1]\n",
      " [0 2]\n",
      " [0 2]], Reward: 0.053592855026564766\n",
      "Step 695, Action: [[1 0]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [0 0]], Reward: 0.043592855026564764\n",
      "Step 696, Action: [[1 1]\n",
      " [1 0]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [0 0]], Reward: 0.02409830056250527\n",
      "Step 697, Action: [[1 2]\n",
      " [3 2]\n",
      " [1 0]\n",
      " [0 2]\n",
      " [1 1]], Reward: 0.03409830056250527\n",
      "Step 698, Action: [[1 1]\n",
      " [2 2]\n",
      " [3 1]\n",
      " [3 2]\n",
      " [1 1]], Reward: 0.054098300562505273\n",
      "Step 699, Action: [[1 2]\n",
      " [0 2]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.04409830056250527\n",
      "Step 700, Action: [[1 1]\n",
      " [3 2]\n",
      " [2 2]\n",
      " [3 2]\n",
      " [1 2]], Reward: 0.04291796067500632\n",
      "Step 701, Action: [[1 2]\n",
      " [2 1]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [0 1]], Reward: 0.02359285502656476\n",
      "Step 702, Action: [[1 1]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.043592855026564764\n",
      "Step 703, Action: [[1 2]\n",
      " [2 2]\n",
      " [0 1]\n",
      " [3 0]\n",
      " [0 1]], Reward: 0.043592855026564764\n",
      "Step 704, Action: [[1 1]\n",
      " [1 2]\n",
      " [1 2]\n",
      " [0 1]\n",
      " [0 1]], Reward: 0.01409830056250527\n",
      "Step 705, Action: [[0 1]\n",
      " [0 2]\n",
      " [0 1]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.04409830056250527\n",
      "Step 706, Action: [[1 1]\n",
      " [3 2]\n",
      " [2 1]\n",
      " [0 1]\n",
      " [0 0]], Reward: 0.03291796067500632\n",
      "Step 707, Action: [[1 1]\n",
      " [1 0]\n",
      " [0 2]\n",
      " [3 2]\n",
      " [2 1]], Reward: 0.02181088885754417\n",
      "Step 708, Action: [[0 2]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [3 1]], Reward: 0.034098300562505277\n",
      "Step 709, Action: [[1 2]\n",
      " [3 2]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]], Reward: 0.04409830056250527\n",
      "Step 710, Action: [[1 2]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [1 1]], Reward: 0.03409830056250527\n",
      "Step 711, Action: [[1 1]\n",
      " [3 2]\n",
      " [1 1]\n",
      " [3 0]\n",
      " [2 0]], Reward: 0.04409830056250527\n",
      "Step 712, Action: [[1 1]\n",
      " [3 2]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [0 2]], Reward: 0.054098300562505273\n",
      "Step 713, Action: [[1 1]\n",
      " [3 2]\n",
      " [1 1]\n",
      " [0 2]\n",
      " [0 1]], Reward: 0.04409830056250527\n",
      "Step 714, Action: [[1 1]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [1 1]], Reward: 0.04409830056250527\n",
      "Step 715, Action: [[1 1]\n",
      " [2 2]\n",
      " [1 0]\n",
      " [1 2]\n",
      " [3 2]], Reward: 0.04409830056250527\n",
      "Step 716, Action: [[3 0]\n",
      " [2 2]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [2 1]], Reward: 0.013592855026564765\n",
      "Step 717, Action: [[0 0]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.043592855026564764\n",
      "Step 718, Action: [[1 1]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [2 1]], Reward: 0.03359285502656476\n",
      "Step 719, Action: [[1 2]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [3 0]\n",
      " [3 1]], Reward: 0.053592855026564766\n",
      "Step 720, Action: [[0 2]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [1 1]], Reward: 0.014098300562505273\n",
      "Step 721, Action: [[0 1]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [0 2]\n",
      " [0 1]], Reward: 0.03409830056250527\n",
      "Step 722, Action: [[1 1]\n",
      " [2 2]\n",
      " [2 2]\n",
      " [0 1]\n",
      " [1 2]], Reward: 0.0035928550265647632\n",
      "Step 723, Action: [[1 1]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [3 1]], Reward: 0.053592855026564766\n",
      "Step 724, Action: [[1 2]\n",
      " [0 2]\n",
      " [2 2]\n",
      " [3 2]\n",
      " [0 0]], Reward: 0.043592855026564764\n",
      "Step 725, Action: [[1 0]\n",
      " [0 0]\n",
      " [3 2]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.053592855026564766\n",
      "Step 726, Action: [[1 1]\n",
      " [0 2]\n",
      " [1 1]\n",
      " [1 2]\n",
      " [0 1]], Reward: 0.03359285502656476\n",
      "Step 727, Action: [[0 0]\n",
      " [0 2]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [0 0]], Reward: 0.043592855026564764\n",
      "Step 728, Action: [[1 1]\n",
      " [3 2]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [1 1]], Reward: 0.053592855026564766\n",
      "Step 729, Action: [[1 0]\n",
      " [3 2]\n",
      " [1 1]\n",
      " [3 0]\n",
      " [0 1]], Reward: 0.053592855026564766\n",
      "Step 730, Action: [[1 2]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [0 2]\n",
      " [2 1]], Reward: 0.02359285502656476\n",
      "Step 731, Action: [[1 2]\n",
      " [2 2]\n",
      " [2 1]\n",
      " [3 1]\n",
      " [3 1]], Reward: 0.053592855026564766\n",
      "Step 732, Action: [[0 0]\n",
      " [3 2]\n",
      " [3 1]\n",
      " [3 1]\n",
      " [1 1]], Reward: 0.053592855026564766\n",
      "Step 733, Action: [[1 2]\n",
      " [1 2]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.02409830056250527\n",
      "Step 734, Action: [[1 2]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.04409830056250527\n",
      "Step 735, Action: [[1 1]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.04409830056250527\n",
      "Step 736, Action: [[1 1]\n",
      " [3 2]\n",
      " [1 0]\n",
      " [0 2]\n",
      " [0 1]], Reward: 0.04409830056250527\n",
      "Step 737, Action: [[1 1]\n",
      " [2 1]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [0 0]], Reward: 0.04409830056250527\n",
      "Step 738, Action: [[1 1]\n",
      " [0 2]\n",
      " [1 2]\n",
      " [0 1]\n",
      " [0 1]], Reward: 0.03409830056250527\n",
      "Step 739, Action: [[1 2]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [0 0]], Reward: 0.04409830056250527\n",
      "Step 740, Action: [[1 0]\n",
      " [3 0]\n",
      " [2 2]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.04291796067500632\n",
      "Step 741, Action: [[1 1]\n",
      " [3 2]\n",
      " [1 1]\n",
      " [1 2]\n",
      " [1 0]], Reward: 0.03409830056250527\n",
      "Step 742, Action: [[1 2]\n",
      " [2 0]\n",
      " [3 1]\n",
      " [3 2]\n",
      " [0 0]], Reward: 0.054098300562505273\n",
      "Step 743, Action: [[1 0]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [0 2]\n",
      " [0 1]], Reward: 0.01181088885754417\n",
      "Step 744, Action: [[0 1]\n",
      " [2 2]\n",
      " [2 2]\n",
      " [3 1]\n",
      " [0 2]], Reward: 0.02409830056250527\n",
      "Step 745, Action: [[1 1]\n",
      " [0 2]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.04409830056250527\n",
      "Step 746, Action: [[3 1]\n",
      " [3 2]\n",
      " [1 1]\n",
      " [3 0]\n",
      " [0 1]], Reward: 0.06409830056250528\n",
      "Step 747, Action: [[0 2]\n",
      " [3 2]\n",
      " [3 2]\n",
      " [0 2]\n",
      " [0 1]], Reward: 0.054098300562505273\n",
      "Step 748, Action: [[1 1]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.04409830056250527\n",
      "Step 749, Action: [[1 1]\n",
      " [2 2]\n",
      " [0 2]\n",
      " [0 1]\n",
      " [0 1]], Reward: 0.03409830056250527\n",
      "Step 750, Action: [[1 2]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [0 0]], Reward: 0.054098300562505273\n",
      "Step 751, Action: [[1 1]\n",
      " [3 2]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [0 0]], Reward: 0.054098300562505273\n",
      "Step 752, Action: [[1 2]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [0 1]\n",
      " [3 1]], Reward: 0.054098300562505273\n",
      "Step 753, Action: [[1 1]\n",
      " [3 2]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.054098300562505273\n",
      "Step 754, Action: [[3 0]\n",
      " [2 2]\n",
      " [0 1]\n",
      " [0 2]\n",
      " [0 2]], Reward: 0.04409830056250527\n",
      "Step 755, Action: [[1 2]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.04409830056250527\n",
      "Step 756, Action: [[1 0]\n",
      " [3 0]\n",
      " [0 2]\n",
      " [3 1]\n",
      " [0 0]], Reward: 0.054098300562505273\n",
      "Step 757, Action: [[1 1]\n",
      " [3 0]\n",
      " [2 2]\n",
      " [3 2]\n",
      " [0 0]], Reward: 0.04291796067500632\n",
      "Step 758, Action: [[1 2]\n",
      " [3 0]\n",
      " [1 0]\n",
      " [3 1]\n",
      " [3 1]], Reward: 0.09409830056250529\n",
      "Step 759, Action: [[1 1]\n",
      " [3 2]\n",
      " [2 1]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.07791796067500634\n",
      "Step 760, Action: [[1 0]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [1 1]], Reward: 0.0840983005625053\n",
      "Step 761, Action: [[1 1]\n",
      " [3 0]\n",
      " [1 0]\n",
      " [0 2]\n",
      " [1 0]], Reward: 0.0840983005625053\n",
      "Step 762, Action: [[1 2]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [3 0]\n",
      " [0 2]], Reward: 0.0840983005625053\n",
      "Step 763, Action: [[1 2]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [0 2]], Reward: 0.0840983005625053\n",
      "Step 764, Action: [[1 0]\n",
      " [2 2]\n",
      " [0 1]\n",
      " [0 2]\n",
      " [3 1]], Reward: 0.0840983005625053\n",
      "Step 765, Action: [[1 0]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [3 0]\n",
      " [2 2]], Reward: 0.0740983005625053\n",
      "Step 766, Action: [[1 1]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [0 2]\n",
      " [0 2]], Reward: 0.0840983005625053\n",
      "Step 767, Action: [[1 2]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.09409830056250529\n",
      "Step 768, Action: [[1 2]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [3 0]\n",
      " [0 1]], Reward: 0.0840983005625053\n",
      "Step 769, Action: [[1 1]\n",
      " [3 2]\n",
      " [1 0]\n",
      " [3 1]\n",
      " [3 0]], Reward: 0.10409830056250528\n",
      "Step 770, Action: [[1 2]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [0 0]], Reward: 0.0840983005625053\n",
      "Step 771, Action: [[1 2]\n",
      " [3 2]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [0 0]], Reward: 0.09409830056250529\n",
      "Step 772, Action: [[1 0]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.0840983005625053\n",
      "Step 773, Action: [[1 2]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [1 1]], Reward: 0.0840983005625053\n",
      "Step 774, Action: [[1 2]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.0840983005625053\n",
      "Step 775, Action: [[1 0]\n",
      " [2 2]\n",
      " [1 0]\n",
      " [1 2]\n",
      " [0 0]], Reward: 0.0740983005625053\n",
      "Step 776, Action: [[1 2]\n",
      " [1 2]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [1 2]], Reward: 0.0668108888575442\n",
      "Step 777, Action: [[1 2]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [0 2]], Reward: 0.0868108888575442\n",
      "Step 778, Action: [[1 2]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [3 0]\n",
      " [0 1]], Reward: 0.0968108888575442\n",
      "Step 779, Action: [[1 0]\n",
      " [3 2]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [1 0]], Reward: 0.0968108888575442\n",
      "Step 780, Action: [[1 0]\n",
      " [2 1]\n",
      " [3 2]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.0968108888575442\n",
      "Step 781, Action: [[1 0]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [0 0]], Reward: 0.0868108888575442\n",
      "Step 782, Action: [[1 2]\n",
      " [3 2]\n",
      " [1 0]\n",
      " [0 2]\n",
      " [1 2]], Reward: 0.0868108888575442\n",
      "Step 783, Action: [[1 2]\n",
      " [2 2]\n",
      " [3 0]\n",
      " [3 2]\n",
      " [0 2]], Reward: 0.0968108888575442\n",
      "Step 784, Action: [[1 0]\n",
      " [2 0]\n",
      " [0 2]\n",
      " [0 1]\n",
      " [0 1]], Reward: 0.07681088885754421\n",
      "Step 785, Action: [[1 2]\n",
      " [3 0]\n",
      " [2 0]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.08299122874504314\n",
      "Step 786, Action: [[1 1]\n",
      " [2 0]\n",
      " [3 2]\n",
      " [0 1]\n",
      " [1 1]], Reward: 0.0740983005625053\n",
      "Step 787, Action: [[1 1]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.0840983005625053\n",
      "Step 788, Action: [[3 1]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.0768108888575442\n",
      "Step 789, Action: [[1 1]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [0 1]], Reward: 0.0868108888575442\n",
      "Step 790, Action: [[1 2]\n",
      " [0 2]\n",
      " [2 2]\n",
      " [3 1]\n",
      " [2 1]], Reward: 0.06299122874504313\n",
      "Step 791, Action: [[3 0]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [1 1]], Reward: 0.0768108888575442\n",
      "Step 792, Action: [[1 0]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.0868108888575442\n",
      "Step 793, Action: [[1 0]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [3 2]\n",
      " [1 1]], Reward: 0.0868108888575442\n",
      "Step 794, Action: [[1 1]\n",
      " [3 2]\n",
      " [1 0]\n",
      " [3 0]\n",
      " [0 0]], Reward: 0.0968108888575442\n",
      "Step 795, Action: [[1 2]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [1 2]], Reward: 0.0968108888575442\n",
      "Step 796, Action: [[1 0]\n",
      " [2 0]\n",
      " [2 2]\n",
      " [3 2]\n",
      " [3 0]], Reward: 0.07409830056250528\n",
      "Step 797, Action: [[1 1]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [3 0]\n",
      " [0 1]], Reward: 0.0840983005625053\n",
      "Step 798, Action: [[1 1]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [3 2]\n",
      " [2 0]], Reward: 0.0740983005625053\n",
      "Step 799, Action: [[1 1]\n",
      " [2 2]\n",
      " [0 0]\n",
      " [3 2]\n",
      " [0 0]], Reward: 0.0840983005625053\n",
      "Step 800, Action: [[1 0]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [1 1]], Reward: 0.0840983005625053\n",
      "Step 801, Action: [[1 2]\n",
      " [3 2]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [2 2]], Reward: 0.0740983005625053\n",
      "Step 802, Action: [[1 0]\n",
      " [3 0]\n",
      " [3 1]\n",
      " [3 1]\n",
      " [2 1]], Reward: 0.09409830056250529\n",
      "Step 803, Action: [[1 1]\n",
      " [2 0]\n",
      " [2 1]\n",
      " [3 1]\n",
      " [1 1]], Reward: 0.04859285502656477\n",
      "Step 804, Action: [[1 2]\n",
      " [2 0]\n",
      " [0 2]\n",
      " [3 2]\n",
      " [2 2]], Reward: 0.06859285502656477\n",
      "Step 805, Action: [[1 1]\n",
      " [0 0]\n",
      " [2 0]\n",
      " [3 1]\n",
      " [2 2]], Reward: 0.07859285502656477\n",
      "Step 806, Action: [[1 2]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [0 2]\n",
      " [1 1]], Reward: 0.05859285502656477\n",
      "Step 807, Action: [[1 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [2 2]], Reward: 0.06859285502656477\n",
      "Step 808, Action: [[1 2]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [3 0]\n",
      " [0 1]], Reward: 0.07859285502656477\n",
      "Step 809, Action: [[1 1]\n",
      " [2 2]\n",
      " [0 2]\n",
      " [3 0]\n",
      " [0 1]], Reward: 0.07859285502656477\n",
      "Step 810, Action: [[1 0]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [0 2]], Reward: 0.08859285502656476\n",
      "Step 811, Action: [[0 2]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [0 2]\n",
      " [3 1]], Reward: 0.08859285502656476\n",
      "Step 812, Action: [[1 2]\n",
      " [2 2]\n",
      " [1 0]\n",
      " [3 2]\n",
      " [0 0]], Reward: 0.07859285502656477\n",
      "Step 813, Action: [[1 0]\n",
      " [2 0]\n",
      " [2 1]\n",
      " [3 2]\n",
      " [1 2]], Reward: 0.06859285502656477\n",
      "Step 814, Action: [[1 2]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]], Reward: 0.06859285502656477\n",
      "Step 815, Action: [[1 2]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [1 0]], Reward: 0.06859285502656477\n",
      "Step 816, Action: [[1 2]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [3 1]\n",
      " [0 2]], Reward: 0.07859285502656477\n",
      "Step 817, Action: [[1 1]\n",
      " [3 0]\n",
      " [0 0]\n",
      " [3 2]\n",
      " [1 1]], Reward: 0.08859285502656476\n",
      "Step 818, Action: [[1 2]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [0 0]], Reward: 0.07859285502656477\n",
      "Step 819, Action: [[0 0]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [1 0]], Reward: 0.08859285502656476\n",
      "Step 820, Action: [[3 2]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [2 1]], Reward: 0.08859285502656476\n",
      "Step 821, Action: [[3 2]\n",
      " [2 0]\n",
      " [2 2]\n",
      " [3 1]\n",
      " [0 2]], Reward: 0.08859285502656476\n",
      "Step 822, Action: [[1 1]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.07859285502656477\n",
      "Step 823, Action: [[3 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [0 1]], Reward: 0.07859285502656477\n",
      "Step 824, Action: [[1 2]\n",
      " [3 2]\n",
      " [1 0]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.08859285502656476\n",
      "Step 825, Action: [[3 1]\n",
      " [0 2]\n",
      " [1 0]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.08859285502656476\n",
      "Step 826, Action: [[1 2]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [3 0]\n",
      " [3 0]], Reward: 0.07409830056250528\n",
      "Step 827, Action: [[1 2]\n",
      " [2 0]\n",
      " [0 2]\n",
      " [3 2]\n",
      " [2 1]], Reward: 0.0740983005625053\n",
      "Step 828, Action: [[1 0]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [1 2]], Reward: 0.0640983005625053\n",
      "Step 829, Action: [[1 2]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.0840983005625053\n",
      "Step 830, Action: [[1 1]\n",
      " [2 2]\n",
      " [0 2]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.0840983005625053\n",
      "Step 831, Action: [[3 0]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [3 0]\n",
      " [1 0]], Reward: 0.0840983005625053\n",
      "Step 832, Action: [[1 1]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [0 0]], Reward: 0.09409830056250529\n",
      "Step 833, Action: [[1 0]\n",
      " [3 0]\n",
      " [3 1]\n",
      " [3 0]\n",
      " [3 1]], Reward: 0.11409830056250528\n",
      "Step 834, Action: [[1 2]\n",
      " [2 0]\n",
      " [3 0]\n",
      " [3 1]\n",
      " [3 0]], Reward: 0.10409830056250528\n",
      "Step 835, Action: [[1 0]\n",
      " [3 0]\n",
      " [0 2]\n",
      " [3 1]\n",
      " [1 1]], Reward: 0.09409830056250529\n",
      "Step 836, Action: [[1 0]\n",
      " [3 0]\n",
      " [1 0]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.09409830056250529\n",
      "Step 837, Action: [[3 2]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [0 0]], Reward: 0.10409830056250528\n",
      "Step 838, Action: [[1 1]\n",
      " [3 2]\n",
      " [3 2]\n",
      " [3 2]\n",
      " [1 1]], Reward: 0.10409830056250528\n",
      "Step 839, Action: [[1 0]\n",
      " [2 0]\n",
      " [0 1]\n",
      " [3 1]\n",
      " [2 1]], Reward: 0.0740983005625053\n",
      "Step 840, Action: [[0 0]\n",
      " [3 2]\n",
      " [2 1]\n",
      " [3 1]\n",
      " [0 0]], Reward: 0.07791796067500634\n",
      "Step 841, Action: [[3 0]\n",
      " [2 0]\n",
      " [0 1]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.07859285502656477\n",
      "Step 842, Action: [[1 2]\n",
      " [2 0]\n",
      " [2 2]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.07859285502656477\n",
      "Step 843, Action: [[1 2]\n",
      " [3 0]\n",
      " [2 2]\n",
      " [0 2]\n",
      " [0 1]], Reward: 0.07859285502656477\n",
      "Step 844, Action: [[3 1]\n",
      " [3 0]\n",
      " [2 1]\n",
      " [3 1]\n",
      " [2 0]], Reward: 0.08859285502656476\n",
      "Step 845, Action: [[1 2]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [1 2]], Reward: 0.06859285502656477\n",
      "Step 846, Action: [[3 2]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [0 2]], Reward: 0.08859285502656476\n",
      "Step 847, Action: [[1 2]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [0 0]], Reward: 0.07859285502656477\n",
      "Step 848, Action: [[1 1]\n",
      " [2 0]\n",
      " [2 2]\n",
      " [1 0]\n",
      " [1 2]], Reward: 0.05859285502656477\n",
      "Step 849, Action: [[1 1]\n",
      " [2 0]\n",
      " [2 1]\n",
      " [3 2]\n",
      " [0 0]], Reward: 0.07859285502656477\n",
      "Step 850, Action: [[1 0]\n",
      " [3 2]\n",
      " [3 1]\n",
      " [0 1]\n",
      " [1 1]], Reward: 0.08859285502656476\n",
      "Step 851, Action: [[1 2]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [0 0]], Reward: 0.08859285502656476\n",
      "Step 852, Action: [[1 2]\n",
      " [0 2]\n",
      " [2 2]\n",
      " [3 2]\n",
      " [2 0]], Reward: 0.06859285502656477\n",
      "Step 853, Action: [[1 0]\n",
      " [2 2]\n",
      " [0 1]\n",
      " [3 1]\n",
      " [0 0]], Reward: 0.07859285502656477\n",
      "Step 854, Action: [[0 2]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [0 2]], Reward: 0.07859285502656477\n",
      "Step 855, Action: [[1 0]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [1 1]], Reward: 0.06859285502656477\n",
      "Step 856, Action: [[1 2]\n",
      " [3 0]\n",
      " [1 0]\n",
      " [3 1]\n",
      " [0 0]], Reward: 0.08859285502656476\n",
      "Step 857, Action: [[1 0]\n",
      " [2 2]\n",
      " [1 0]\n",
      " [3 2]\n",
      " [0 0]], Reward: 0.07859285502656477\n",
      "Step 858, Action: [[1 0]\n",
      " [2 0]\n",
      " [3 1]\n",
      " [3 2]\n",
      " [3 1]], Reward: 0.09859285502656476\n",
      "Step 859, Action: [[1 2]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [1 1]\n",
      " [2 2]], Reward: 0.05859285502656477\n",
      "Step 860, Action: [[1 1]\n",
      " [3 2]\n",
      " [0 2]\n",
      " [3 1]\n",
      " [3 0]], Reward: 0.09859285502656476\n",
      "Step 861, Action: [[1 1]\n",
      " [3 1]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [0 1]], Reward: 0.08859285502656476\n",
      "Step 862, Action: [[1 1]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [0 0]], Reward: 0.07859285502656477\n",
      "Step 863, Action: [[1 0]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [2 2]], Reward: 0.06859285502656477\n",
      "Step 864, Action: [[3 2]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [0 0]], Reward: 0.09859285502656476\n",
      "Step 865, Action: [[0 2]\n",
      " [2 2]\n",
      " [3 2]\n",
      " [3 0]\n",
      " [0 0]], Reward: 0.08859285502656476\n",
      "Step 866, Action: [[1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [3 1]], Reward: 0.08859285502656476\n",
      "Step 867, Action: [[1 0]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.07859285502656477\n",
      "Step 868, Action: [[1 1]\n",
      " [2 2]\n",
      " [3 0]\n",
      " [3 1]\n",
      " [1 2]], Reward: 0.07859285502656477\n",
      "Step 869, Action: [[3 2]\n",
      " [3 0]\n",
      " [1 0]\n",
      " [3 2]\n",
      " [0 2]], Reward: 0.09859285502656476\n",
      "Step 870, Action: [[0 1]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [1 0]\n",
      " [0 0]], Reward: 0.07859285502656477\n",
      "Step 871, Action: [[1 1]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [1 0]], Reward: 0.06859285502656477\n",
      "Step 872, Action: [[1 1]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [1 0]], Reward: 0.07859285502656477\n",
      "Step 873, Action: [[3 1]\n",
      " [3 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [3 1]], Reward: 0.09859285502656476\n",
      "Step 874, Action: [[1 1]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [3 1]\n",
      " [1 0]], Reward: 0.07859285502656477\n",
      "Step 875, Action: [[1 2]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [0 2]\n",
      " [0 1]], Reward: 0.06859285502656477\n",
      "Step 876, Action: [[1 2]\n",
      " [1 0]\n",
      " [2 0]\n",
      " [3 0]\n",
      " [0 2]], Reward: 0.06791796067500634\n",
      "Step 877, Action: [[1 0]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [3 0]\n",
      " [0 2]], Reward: 0.06859285502656477\n",
      "Step 878, Action: [[1 2]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.07859285502656477\n",
      "Step 879, Action: [[1 2]\n",
      " [3 2]\n",
      " [1 0]\n",
      " [1 2]\n",
      " [2 2]], Reward: 0.06859285502656477\n",
      "Step 880, Action: [[0 1]\n",
      " [2 2]\n",
      " [3 1]\n",
      " [3 2]\n",
      " [1 2]], Reward: 0.07859285502656477\n",
      "Step 881, Action: [[3 1]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [1 1]], Reward: 0.09859285502656476\n",
      "Step 882, Action: [[3 0]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.09859285502656476\n",
      "Step 883, Action: [[0 0]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [1 0]], Reward: 0.08859285502656476\n",
      "Step 884, Action: [[1 2]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [0 0]], Reward: 0.08859285502656476\n",
      "Step 885, Action: [[1 0]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [1 2]\n",
      " [3 2]], Reward: 0.07859285502656477\n",
      "Step 886, Action: [[1 1]\n",
      " [2 0]\n",
      " [0 2]\n",
      " [3 1]\n",
      " [1 2]], Reward: 0.07859285502656477\n",
      "Step 887, Action: [[1 1]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [1 0]], Reward: 0.07859285502656477\n",
      "Step 888, Action: [[1 1]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [2 2]], Reward: 0.06859285502656477\n",
      "Step 889, Action: [[1 0]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [3 0]\n",
      " [0 1]], Reward: 0.08859285502656476\n",
      "Step 890, Action: [[1 1]\n",
      " [3 2]\n",
      " [3 2]\n",
      " [3 1]\n",
      " [0 2]], Reward: 0.09859285502656476\n",
      "Step 891, Action: [[1 2]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [2 0]], Reward: 0.06859285502656477\n",
      "Step 892, Action: [[1 1]\n",
      " [2 2]\n",
      " [1 0]\n",
      " [3 2]\n",
      " [2 2]], Reward: 0.07859285502656477\n",
      "Step 893, Action: [[1 2]\n",
      " [2 0]\n",
      " [2 1]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.07859285502656477\n",
      "Step 894, Action: [[1 2]\n",
      " [3 0]\n",
      " [1 0]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.08859285502656476\n",
      "Step 895, Action: [[1 2]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [1 1]], Reward: 0.07859285502656477\n",
      "Step 896, Action: [[1 0]\n",
      " [3 2]\n",
      " [2 1]\n",
      " [3 0]\n",
      " [1 1]], Reward: 0.07859285502656477\n",
      "Step 897, Action: [[1 0]\n",
      " [2 0]\n",
      " [3 2]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.08859285502656476\n",
      "Step 898, Action: [[1 0]\n",
      " [2 0]\n",
      " [3 1]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.08859285502656476\n",
      "Step 899, Action: [[1 0]\n",
      " [3 2]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [0 1]], Reward: 0.07859285502656477\n",
      "Step 900, Action: [[1 1]\n",
      " [3 2]\n",
      " [3 1]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.09859285502656476\n",
      "Step 901, Action: [[1 0]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [3 1]\n",
      " [3 0]], Reward: 0.08859285502656476\n",
      "Step 902, Action: [[1 2]\n",
      " [3 2]\n",
      " [2 1]\n",
      " [0 2]\n",
      " [0 1]], Reward: 0.07859285502656477\n",
      "Step 903, Action: [[1 2]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [1 2]], Reward: 0.07859285502656477\n",
      "Step 904, Action: [[1 0]\n",
      " [2 0]\n",
      " [0 2]\n",
      " [0 1]\n",
      " [0 2]], Reward: 0.06859285502656477\n",
      "Step 905, Action: [[1 2]\n",
      " [2 2]\n",
      " [1 0]\n",
      " [3 0]\n",
      " [1 0]], Reward: 0.07859285502656477\n",
      "Step 906, Action: [[1 2]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [0 0]], Reward: 0.07859285502656477\n",
      "Step 907, Action: [[1 2]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [1 1]], Reward: 0.08859285502656476\n",
      "Step 908, Action: [[1 2]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [3 0]], Reward: 0.09859285502656476\n",
      "Step 909, Action: [[1 2]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [3 0]], Reward: 0.09859285502656476\n",
      "Step 910, Action: [[1 2]\n",
      " [2 1]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.07859285502656477\n",
      "Step 911, Action: [[3 2]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [0 2]], Reward: 0.08859285502656476\n",
      "Step 912, Action: [[1 1]\n",
      " [3 1]\n",
      " [1 0]\n",
      " [3 1]\n",
      " [0 2]], Reward: 0.08859285502656476\n",
      "Step 913, Action: [[3 1]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [0 0]], Reward: 0.09859285502656476\n",
      "Step 914, Action: [[1 0]\n",
      " [0 0]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [0 0]], Reward: 0.07859285502656477\n",
      "Step 915, Action: [[1 1]\n",
      " [2 2]\n",
      " [1 0]\n",
      " [3 1]\n",
      " [1 2]], Reward: 0.07859285502656477\n",
      "Step 916, Action: [[1 2]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [1 1]], Reward: 0.07859285502656477\n",
      "Step 917, Action: [[1 0]\n",
      " [3 2]\n",
      " [2 1]\n",
      " [3 2]\n",
      " [2 2]], Reward: 0.07859285502656477\n",
      "Step 918, Action: [[1 2]\n",
      " [1 0]\n",
      " [1 2]\n",
      " [0 1]\n",
      " [0 0]], Reward: 0.054098300562505294\n",
      "Step 919, Action: [[1 2]\n",
      " [2 0]\n",
      " [2 2]\n",
      " [3 1]\n",
      " [1 2]], Reward: 0.04859285502656477\n",
      "Step 920, Action: [[1 2]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [2 1]], Reward: 0.06859285502656477\n",
      "Step 921, Action: [[1 0]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.07859285502656477\n",
      "Step 922, Action: [[1 1]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [2 1]], Reward: 0.06859285502656477\n",
      "Step 923, Action: [[1 1]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [0 0]\n",
      " [1 1]], Reward: 0.05859285502656477\n",
      "Step 924, Action: [[1 2]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [1 2]\n",
      " [0 1]], Reward: 0.07859285502656477\n",
      "Step 925, Action: [[1 1]\n",
      " [3 2]\n",
      " [1 0]\n",
      " [3 1]\n",
      " [1 2]], Reward: 0.07859285502656477\n",
      "Step 926, Action: [[1 0]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.08859285502656476\n",
      "Step 927, Action: [[1 2]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [0 1]], Reward: 0.08859285502656476\n",
      "Step 928, Action: [[1 2]\n",
      " [3 2]\n",
      " [1 1]\n",
      " [0 2]\n",
      " [1 1]], Reward: 0.07859285502656477\n",
      "Step 929, Action: [[1 2]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [2 0]], Reward: 0.06859285502656477\n",
      "Step 930, Action: [[1 2]\n",
      " [3 0]\n",
      " [3 1]\n",
      " [3 1]\n",
      " [0 0]], Reward: 0.09859285502656476\n",
      "Step 931, Action: [[1 1]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [1 1]], Reward: 0.06859285502656477\n",
      "Step 932, Action: [[1 1]\n",
      " [2 0]\n",
      " [2 2]\n",
      " [0 2]\n",
      " [0 0]], Reward: 0.06859285502656477\n",
      "Step 933, Action: [[1 2]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [0 1]], Reward: 0.06859285502656477\n",
      "Step 934, Action: [[1 1]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [0 0]], Reward: 0.08859285502656476\n",
      "Step 935, Action: [[1 0]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [0 2]], Reward: 0.08859285502656476\n",
      "Step 936, Action: [[1 2]\n",
      " [3 1]\n",
      " [1 1]\n",
      " [3 0]\n",
      " [0 1]], Reward: 0.08859285502656476\n",
      "Step 937, Action: [[1 2]\n",
      " [2 0]\n",
      " [2 0]\n",
      " [3 1]\n",
      " [0 0]], Reward: 0.07859285502656477\n",
      "Step 938, Action: [[1 1]\n",
      " [3 0]\n",
      " [1 0]\n",
      " [3 2]\n",
      " [1 0]], Reward: 0.08859285502656476\n",
      "Step 939, Action: [[1 2]\n",
      " [2 2]\n",
      " [2 0]\n",
      " [0 1]\n",
      " [0 2]], Reward: 0.06859285502656477\n",
      "Step 940, Action: [[1 2]\n",
      " [3 2]\n",
      " [1 0]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.08859285502656476\n",
      "Step 941, Action: [[1 2]\n",
      " [3 0]\n",
      " [0 2]\n",
      " [3 2]\n",
      " [1 1]], Reward: 0.08859285502656476\n",
      "Step 942, Action: [[1 2]\n",
      " [2 0]\n",
      " [3 1]\n",
      " [3 2]\n",
      " [0 0]], Reward: 0.08859285502656476\n",
      "Step 943, Action: [[1 2]\n",
      " [0 0]\n",
      " [2 1]\n",
      " [3 2]\n",
      " [0 0]], Reward: 0.07859285502656477\n",
      "Step 944, Action: [[1 2]\n",
      " [3 0]\n",
      " [1 0]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.08859285502656476\n",
      "Step 945, Action: [[1 2]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [3 0]\n",
      " [0 1]], Reward: 0.07859285502656477\n",
      "Step 946, Action: [[1 0]\n",
      " [2 2]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [1 1]], Reward: 0.07859285502656477\n",
      "Step 947, Action: [[1 1]\n",
      " [3 0]\n",
      " [3 0]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.09859285502656476\n",
      "Step 948, Action: [[1 2]\n",
      " [2 0]\n",
      " [2 2]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.07859285502656477\n",
      "Step 949, Action: [[1 2]\n",
      " [2 2]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [0 0]], Reward: 0.07859285502656477\n",
      "Step 950, Action: [[1 0]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [3 0]\n",
      " [0 1]], Reward: 0.08859285502656476\n",
      "Step 951, Action: [[1 2]\n",
      " [0 0]\n",
      " [2 1]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.07859285502656477\n",
      "Step 952, Action: [[1 2]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [0 2]], Reward: 0.06409830056250529\n",
      "Step 953, Action: [[1 0]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [3 0]\n",
      " [1 1]], Reward: 0.0840983005625053\n",
      "Step 954, Action: [[1 1]\n",
      " [3 2]\n",
      " [2 1]\n",
      " [3 1]\n",
      " [2 1]], Reward: 0.06791796067500633\n",
      "Step 955, Action: [[1 1]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [2 0]], Reward: 0.058592855026564764\n",
      "Step 956, Action: [[1 0]\n",
      " [0 0]\n",
      " [3 0]\n",
      " [3 1]\n",
      " [3 1]], Reward: 0.09859285502656476\n",
      "Step 957, Action: [[1 2]\n",
      " [3 2]\n",
      " [1 0]\n",
      " [3 0]\n",
      " [3 0]], Reward: 0.09859285502656476\n",
      "Step 958, Action: [[1 1]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [3 0]\n",
      " [0 1]], Reward: 0.07859285502656477\n",
      "Step 959, Action: [[3 1]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [2 2]], Reward: 0.09859285502656476\n",
      "Step 960, Action: [[1 2]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [0 0]], Reward: 0.08859285502656476\n",
      "Step 961, Action: [[1 0]\n",
      " [3 0]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [0 2]], Reward: 0.08859285502656476\n",
      "Step 962, Action: [[1 1]\n",
      " [2 2]\n",
      " [2 2]\n",
      " [3 2]\n",
      " [0 2]], Reward: 0.07859285502656477\n",
      "Step 963, Action: [[1 2]\n",
      " [3 0]\n",
      " [3 2]\n",
      " [0 2]\n",
      " [0 0]], Reward: 0.08859285502656476\n",
      "Step 964, Action: [[3 2]\n",
      " [3 0]\n",
      " [1 0]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.09859285502656476\n",
      "Step 965, Action: [[1 2]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [0 2]], Reward: 0.08859285502656476\n",
      "Step 966, Action: [[1 0]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [0 1]\n",
      " [1 0]], Reward: 0.05859285502656477\n",
      "Step 967, Action: [[1 1]\n",
      " [2 2]\n",
      " [3 1]\n",
      " [3 1]\n",
      " [1 0]], Reward: 0.07859285502656477\n",
      "Step 968, Action: [[1 0]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [3 0]], Reward: 0.08859285502656476\n",
      "Step 969, Action: [[1 0]\n",
      " [2 0]\n",
      " [0 2]\n",
      " [3 0]\n",
      " [0 0]], Reward: 0.07859285502656477\n",
      "Step 970, Action: [[0 1]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [3 0]\n",
      " [1 0]], Reward: 0.08859285502656476\n",
      "Step 971, Action: [[1 0]\n",
      " [3 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [2 1]], Reward: 0.06859285502656477\n",
      "Step 972, Action: [[0 0]\n",
      " [2 0]\n",
      " [3 0]\n",
      " [3 2]\n",
      " [2 0]], Reward: 0.07859285502656477\n",
      "Step 973, Action: [[1 2]\n",
      " [2 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 1]], Reward: 0.05859285502656477\n",
      "Step 974, Action: [[1 0]\n",
      " [0 2]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [0 0]], Reward: 0.07859285502656477\n",
      "Step 975, Action: [[1 1]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [3 1]\n",
      " [0 0]], Reward: 0.08859285502656476\n",
      "Step 976, Action: [[3 0]\n",
      " [3 0]\n",
      " [2 2]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.09859285502656476\n",
      "Step 977, Action: [[1 2]\n",
      " [2 2]\n",
      " [2 1]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.07859285502656477\n",
      "Step 978, Action: [[1 0]\n",
      " [3 2]\n",
      " [0 0]\n",
      " [3 0]\n",
      " [2 2]], Reward: 0.07859285502656477\n",
      "Step 979, Action: [[1 0]\n",
      " [3 2]\n",
      " [2 1]\n",
      " [3 0]\n",
      " [0 0]], Reward: 0.08859285502656476\n",
      "Step 980, Action: [[1 1]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [0 2]\n",
      " [0 0]], Reward: 0.07859285502656477\n",
      "Step 981, Action: [[1 0]\n",
      " [3 2]\n",
      " [1 1]\n",
      " [0 2]\n",
      " [0 1]], Reward: 0.07859285502656477\n",
      "Step 982, Action: [[3 2]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.09859285502656476\n",
      "Step 983, Action: [[3 2]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [2 1]], Reward: 0.06409830056250529\n",
      "Step 984, Action: [[3 0]\n",
      " [2 0]\n",
      " [3 1]\n",
      " [3 2]\n",
      " [1 1]], Reward: 0.09409830056250529\n",
      "Step 985, Action: [[1 2]\n",
      " [3 0]\n",
      " [0 0]\n",
      " [3 2]\n",
      " [1 1]], Reward: 0.0840983005625053\n",
      "Step 986, Action: [[1 0]\n",
      " [2 2]\n",
      " [0 1]\n",
      " [3 0]\n",
      " [1 0]], Reward: 0.0840983005625053\n",
      "Step 987, Action: [[1 0]\n",
      " [3 0]\n",
      " [3 0]\n",
      " [3 1]\n",
      " [1 1]], Reward: 0.10409830056250528\n",
      "Step 988, Action: [[1 1]\n",
      " [2 2]\n",
      " [1 0]\n",
      " [3 1]\n",
      " [0 0]], Reward: 0.0840983005625053\n",
      "Step 989, Action: [[1 2]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [0 1]], Reward: 0.0840983005625053\n",
      "Step 990, Action: [[1 2]\n",
      " [2 2]\n",
      " [0 2]\n",
      " [3 1]\n",
      " [1 1]], Reward: 0.0840983005625053\n",
      "Step 991, Action: [[1 0]\n",
      " [2 2]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [0 0]], Reward: 0.0840983005625053\n",
      "Step 992, Action: [[1 1]\n",
      " [2 0]\n",
      " [1 1]\n",
      " [3 2]\n",
      " [1 2]], Reward: 0.0840983005625053\n",
      "Step 993, Action: [[1 0]\n",
      " [3 0]\n",
      " [1 1]\n",
      " [3 1]\n",
      " [0 1]], Reward: 0.09409830056250529\n",
      "Step 994, Action: [[1 2]\n",
      " [2 2]\n",
      " [2 0]\n",
      " [3 1]\n",
      " [2 1]], Reward: 0.04859285502656477\n",
      "Step 995, Action: [[1 0]\n",
      " [2 0]\n",
      " [3 2]\n",
      " [3 0]\n",
      " [3 0]], Reward: 0.09859285502656476\n",
      "Step 996, Action: [[1 2]\n",
      " [2 0]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [1 0]], Reward: 0.06859285502656477\n",
      "Step 997, Action: [[1 0]\n",
      " [3 0]\n",
      " [2 2]\n",
      " [3 2]\n",
      " [1 1]], Reward: 0.08859285502656476\n",
      "Step 998, Action: [[1 0]\n",
      " [3 2]\n",
      " [1 2]\n",
      " [3 2]\n",
      " [1 0]], Reward: 0.08859285502656476\n",
      "Step 999, Action: [[1 2]\n",
      " [3 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [0 0]], Reward: 0.07859285502656477\n",
      "Step 1000, Action: [[1 1]\n",
      " [3 2]\n",
      " [0 1]\n",
      " [3 2]\n",
      " [1 1]], Reward: 0.08859285502656476\n",
      "Episode 0, Total Reward: 30.9402439098713, Steps: 1000\n"
     ]
    }
   ],
   "source": [
    "# Tạo môi trường\n",
    "example_env = Env('map2.txt', 1000, 5, 20, -0.01, 10., 1., 10)\n",
    "eval_env = Env('map1.txt', 1000, 5, 100, -0.01, 10., 1., 10)\n",
    "# Kiểm tra môi trường\n",
    "(obs, _) = example_env.reset()\n",
    "map_tensor, feature_vector = obs\n",
    "print(\"Map tensor shape:\", map_tensor.shape)\n",
    "print(\"Feature vector shape:\", feature_vector.shape)\n",
    "\n",
    "action = example_env.action_space.sample()\n",
    "print(\"Sampled action:\", action)\n",
    "\n",
    "(next_obs, reward, terminated, truncated, info) = example_env.step(action)\n",
    "next_map_tensor, next_feature_vector = next_obs\n",
    "print(\"Reward:\", reward)\n",
    "print(\"Next map tensor shape:\", next_map_tensor.shape)\n",
    "print(\"Next feature vector shape:\", next_feature_vector.shape)\n",
    "\n",
    "# Tạo agent\n",
    "map_size = map_tensor.shape[0]\n",
    "feature_dim = feature_vector.shape[0]\n",
    "n_actions = 4  # up, right, down, left\n",
    "n_status_actions = 3  # S, L, R\n",
    "n_agents = 5\n",
    "\n",
    "agent = MAPPOAgent(map_size, feature_dim, n_actions,n_status_actions, n_agents=n_agents)\n",
    "\n",
    "# Huấn luyện\n",
    "print(\"\\nBắt đầu huấn luyện...\")\n",
    "rewards = train(example_env, agent, num_episodes=3, max_steps=1000, update_interval=200)\n",
    "\n",
    "# Chạy mô hình đã huấn luyện\n",
    "print(\"\\nChạy mô hình đã huấn luyện...\")\n",
    "run_trained_model(example_env, agent, num_episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x1600 and 6400x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrun_trained_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 343\u001b[39m, in \u001b[36mrun_trained_model\u001b[39m\u001b[34m(env, agent, num_episodes)\u001b[39m\n\u001b[32m    339\u001b[39m step = \u001b[32m0\u001b[39m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    342\u001b[39m     \u001b[38;5;66;03m# Chọn hành động\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m     actions, _, _ = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmap_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_vector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    345\u001b[39m     \u001b[38;5;66;03m# Thực hiện hành động\u001b[39;00m\n\u001b[32m    346\u001b[39m     next_obs, reward, terminated, truncated, info = env.step(actions)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 122\u001b[39m, in \u001b[36mMAPPOAgent.get_action\u001b[39m\u001b[34m(self, map_tensor, feature_vector)\u001b[39m\n\u001b[32m    120\u001b[39m feature_vector = torch.FloatTensor(feature_vector).unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     move_probs, status_probs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmap_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_vector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m     value = \u001b[38;5;28mself\u001b[39m.critic(map_tensor, feature_vector)\n\u001b[32m    124\u001b[39m actions = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/home_work/RL/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/home_work/RL/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mActor.forward\u001b[39m\u001b[34m(self, map_tensor, feature_vector)\u001b[39m\n\u001b[32m     56\u001b[39m batch_size = map_tensor.size(\u001b[32m0\u001b[39m)\n\u001b[32m     57\u001b[39m map_tensor = map_tensor.unsqueeze(\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m map_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcnn_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmap_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m feature_features = \u001b[38;5;28mself\u001b[39m.mlp_encoder(feature_vector)\n\u001b[32m     60\u001b[39m combined = torch.cat([map_features, feature_features], dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/home_work/RL/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/home_work/RL/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mCNNEncoder.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     26\u001b[39m x = F.relu(\u001b[38;5;28mself\u001b[39m.conv3(x))\n\u001b[32m     27\u001b[39m x = x.view(x.size(\u001b[32m0\u001b[39m), -\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# Flatten\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m x = F.relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/home_work/RL/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/home_work/RL/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/home_work/RL/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (1x1600 and 6400x256)"
     ]
    }
   ],
   "source": [
    "run_trained_model(eval_env, agent, num_episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(env, agent, max_steps):\n",
    "    obs, _ = env.reset()\n",
    "    map_tensor, feature_vector = obs\n",
    "\n",
    "    print(\"Map tensor shape:\", map_tensor.shape)\n",
    "    print(\"Feature vector shape:\", feature_vector.shape)\n",
    "\n",
    "    total_test_reward = 0\n",
    "    step = 0\n",
    "    done = False\n",
    "    truncated = False\n",
    "\n",
    "    while not done and not truncated and step < max_steps:\n",
    "        actions, _, _ = agent.get_action(map_tensor, feature_vector)\n",
    "        print(f\"Actions: {actions}\")\n",
    "        obs, reward, done, truncated, info = env.step(actions)\n",
    "        total_test_reward += reward\n",
    "        step += 1\n",
    "\n",
    "    print(f\" Tổng phần thưởng :{total_test_reward:.2f} sau {step} bước.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m env = Env(\u001b[33m'\u001b[39m\u001b[33mmap5.txt\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m100\u001b[39m, \u001b[32m10\u001b[39m, \u001b[32m1000\u001b[39m, -\u001b[32m0.01\u001b[39m, \u001b[32m10.0\u001b[39m, \u001b[32m1.0\u001b[39m, \u001b[32m10\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m test(env, \u001b[43magent\u001b[49m, max_steps=\u001b[32m100\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'agent' is not defined"
     ]
    }
   ],
   "source": [
    "env = Env('map5.txt', 100, 10, 1000, -0.01, 10.0, 1.0, 10)\n",
    "test(env, agent, max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.status.busy": "2025-05-14T14:12:10.947589Z",
     "iopub.status.idle": "2025-05-14T14:12:10.947977Z",
     "shell.execute_reply": "2025-05-14T14:12:10.947825Z",
     "shell.execute_reply.started": "2025-05-14T14:12:10.947809Z"
    },
    "id": "WVpXVAz8Kn9C",
    "outputId": "da85df6f-1219-444b-eeac-0bcf9bd6bf83",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip freeze | grep stable_baselines3"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
