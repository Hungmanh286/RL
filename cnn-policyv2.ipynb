{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AM3r0wPe5N0K"
   },
   "source": [
    "Solving Package delivery using single-agent PPO with a naive feature representation learning: concatenante all the feature in to a single state vector, and multiple robot actions as a multi discrete distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T14:24:00.824019Z",
     "iopub.status.busy": "2025-05-15T14:24:00.823838Z",
     "iopub.status.idle": "2025-05-15T14:24:01.473592Z",
     "shell.execute_reply": "2025-05-15T14:24:01.472651Z",
     "shell.execute_reply.started": "2025-05-15T14:24:00.824001Z"
    },
    "id": "9Ro5mHQ3GnN8",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'marl-delivery'\n",
      "/home/hungmanh/home_work/RL/marl-delivery\n"
     ]
    }
   ],
   "source": [
    "# %%capture\n",
    "# !git clone https://github.com/cuongtv312/marl-delivery.git\n",
    "%cd marl-delivery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T14:24:47.290464Z",
     "iopub.status.busy": "2025-05-15T14:24:47.289639Z",
     "iopub.status.idle": "2025-05-15T14:24:47.293858Z",
     "shell.execute_reply": "2025-05-15T14:24:47.293254Z",
     "shell.execute_reply.started": "2025-05-15T14:24:47.290437Z"
    },
    "id": "309nvG-V8Otr",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from env import Environment\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T14:24:49.799301Z",
     "iopub.status.busy": "2025-05-15T14:24:49.798637Z",
     "iopub.status.idle": "2025-05-15T14:24:49.806314Z",
     "shell.execute_reply": "2025-05-15T14:24:49.805561Z",
     "shell.execute_reply.started": "2025-05-15T14:24:49.799270Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 4, 0), (5, 3, 0)]\n",
      "[(1, 6, 5, 4, 3, 0, 26), (2, 2, 2, 2, 3, 0, 22), (3, 4, 5, 5, 6, 0, 22)]\n",
      "[[1, 1, 1, 1, 1, 1, 1], [1, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0, 1], [1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "env = Environment('map.txt', 10, 2, 5)\n",
    "state = env.reset()\n",
    "print(state[\"robots\"])\n",
    "print(state[\"packages\"])\n",
    "print(state[\"map\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T14:24:53.111866Z",
     "iopub.status.busy": "2025-05-15T14:24:53.111094Z",
     "iopub.status.idle": "2025-05-15T14:24:53.120962Z",
     "shell.execute_reply": "2025-05-15T14:24:53.120090Z",
     "shell.execute_reply.started": "2025-05-15T14:24:53.111834Z"
    },
    "id": "rq1hlk4b8Q37",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def convert_state(state):\n",
    "    \"\"\"\n",
    "    Chuyển đổi trạng thái đầu vào thành 2 phần: \n",
    "        - map_tensor: 2D numpy array (giữ nguyên spatial shape)\n",
    "        - feature_vector: vector phẳng từ robots, packages và time_step\n",
    "\n",
    "    Không normalize, không padding.\n",
    "\n",
    "    Args:\n",
    "        state (dict): Dictionary chứa:\n",
    "            \"map\": list 2D\n",
    "            \"robots\": list các tuple (x, y, status)\n",
    "            \"packages\": list các tuple (id, pickup_x, pickup_y, dropoff_x, dropoff_y, appear_time, deadline)\n",
    "            \"time_step\": int\n",
    "\n",
    "    Returns:\n",
    "        map_tensor (np.ndarray): shape (n, n)\n",
    "        feature_vector (np.ndarray): vector 1D float32\n",
    "    \"\"\"\n",
    "    current_time = state.get(\"time_step\", 0)\n",
    "\n",
    "    # Dữ liệu bản đồ giữ nguyên shape (n, n)\n",
    "    map_tensor = np.array(state[\"map\"], dtype=np.float32)\n",
    "\n",
    "    # Robot features: (x, y, status)\n",
    "    robot_features = []\n",
    "    for robot_x, robot_y, status in state[\"robots\"]:\n",
    "        robot_features.extend([robot_x, robot_y, float(status)])\n",
    "\n",
    "    # Package features: (pickup_x, pickup_y, dropoff_x, dropoff_y, appear_time, deadline, is_active)\n",
    "    package_features = []\n",
    "    for pkg in sorted(state[\"packages\"], key=lambda p: (p[3], p[0])):  # sort by dropoff_x then id\n",
    "        _, px, py, dx, dy, t_appear, t_deadline = pkg\n",
    "        is_active = 1.0 if (current_time >= t_appear and current_time < t_deadline) else 0.0\n",
    "        package_features.extend([px, py, dx, dy, t_appear, t_deadline, is_active])\n",
    "\n",
    "    # Gộp robot + package + time_step\n",
    "    feature_vector = np.array(\n",
    "        robot_features + package_features + [float(current_time)],\n",
    "        dtype=np.float32\n",
    "    )\n",
    "\n",
    "    return map_tensor, feature_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1.]]\n",
      "[ 5.  4.  0.  5.  3.  0.  2.  2.  2.  3.  0. 22.  1.  6.  5.  4.  3.  0.\n",
      " 26.  1.  4.  5.  5.  6.  0. 22.  1.  0.]\n"
     ]
    }
   ],
   "source": [
    "map_tensor, feature_vector = convert_state(state)\n",
    "print(map_tensor)\n",
    "print(feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T14:25:14.499419Z",
     "iopub.status.busy": "2025-05-15T14:25:14.499105Z",
     "iopub.status.idle": "2025-05-15T14:25:14.508636Z",
     "shell.execute_reply": "2025-05-15T14:25:14.507946Z",
     "shell.execute_reply.started": "2025-05-15T14:25:14.499372Z"
    },
    "id": "7SHRHHeF8SjO",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "def reward_shaping(original_reward, env, prev_state, actions):\n",
    "    shaped_reward = original_reward\n",
    "    shaping_factor = 0.5\n",
    "\n",
    "    map_grid = env.load_map()\n",
    "\n",
    "    for i, robot in enumerate(env.robots):\n",
    "        prev_pos = tuple(prev_state['robots'][i][:2])\n",
    "        curr_pos = robot.position\n",
    "\n",
    "        # --- 1. Thưởng/Penalty di chuyển hợp lý ---\n",
    "        if robot.carrying:\n",
    "            pkg = env.packages[robot.carrying - 1]\n",
    "            d_prev = bfs_distance(prev_pos, pkg.target, map_grid)\n",
    "            d_curr = bfs_distance(curr_pos, pkg.target, map_grid)\n",
    "            if d_curr < d_prev:\n",
    "                shaped_reward += shaping_factor\n",
    "            else:\n",
    "                shaped_reward -= 0.3  # đi xa mục tiêu\n",
    "        else:\n",
    "            # tìm gói hàng gần nhất đang chờ\n",
    "            waiting_pkgs = [p for p in env.packages if p.status == 'waiting']\n",
    "            if waiting_pkgs:\n",
    "                pkg = min(waiting_pkgs, key=lambda p: bfs_distance(prev_pos, p.start, map_grid))\n",
    "                d_prev = bfs_distance(prev_pos, pkg.start, map_grid)\n",
    "                d_curr = bfs_distance(curr_pos, pkg.start, map_grid)\n",
    "                if d_curr < d_prev:\n",
    "                    shaped_reward += shaping_factor * 0.3\n",
    "                else:\n",
    "                    shaped_reward -= 0.1\n",
    "\n",
    "        # --- 2. Phạt đứng yên ---\n",
    "        if curr_pos == prev_pos:\n",
    "            shaped_reward -= 0.2\n",
    "\n",
    "        # --- 3. Phạt va chạm tường hoặc bước không hợp lệ ---\n",
    "        if not (0 <= curr_pos[0] < len(map_grid) and 0 <= curr_pos[1] < len(map_grid[0])):\n",
    "            shaped_reward -= 1.0  # đi ra ngoài bản đồ\n",
    "        elif map_grid[curr_pos[0]][curr_pos[1]] == 1:\n",
    "            shaped_reward -= 0.8  # đụng tường\n",
    "\n",
    "        # --- 4. Giao hàng trễ bị phạt ---\n",
    "        for pkg in env.packages:\n",
    "            if pkg.status == 'delivering' and pkg.picked_by == i:\n",
    "                waiting_time = pkg.pick_time - pkg.start_time if pkg.pick_time else 0\n",
    "                shaped_reward += max(0.0, 1.0 - 0.01 * waiting_time)\n",
    "\n",
    "            if pkg.status == 'delivered' and pkg.delivered_by == i:\n",
    "                if env.time_step > pkg.deadline:\n",
    "                    shaped_reward -= 1.0  # giao trễ\n",
    "                else:\n",
    "                    shaped_reward += 2.0  # giao đúng hạn\n",
    "\n",
    "        # --- 5. Tránh va chạm giữa các robot ---\n",
    "        for j, other_robot in enumerate(env.robots):\n",
    "            if i != j and curr_pos == other_robot.position:\n",
    "                shaped_reward -= 0.5  # phạt va chạm\n",
    "\n",
    "        # --- 6. Hành động không hiệu quả ---\n",
    "        if not robot.carrying and actions[i] == 'WAIT':\n",
    "            shaped_reward -= 0.1\n",
    "\n",
    "    return shaped_reward\n",
    "\n",
    "\n",
    "def bfs_distance(start, goal, grid):\n",
    "    rows, cols = len(grid), len(grid[0])\n",
    "    visited = set()\n",
    "    queue = deque([(start, 0)])\n",
    "    while queue:\n",
    "        (x, y), dist = queue.popleft()\n",
    "        if (x, y) == goal:\n",
    "            return dist\n",
    "        for dx, dy in [(-1,0), (1,0), (0,-1), (0,1)]:\n",
    "            nx, ny = x + dx, y + dy\n",
    "            if 0 <= nx < rows and 0 <= ny < cols:\n",
    "                if grid[nx][ny] == 0 and (nx, ny) not in visited:\n",
    "                    visited.add((nx, ny))\n",
    "                    queue.append(((nx, ny), dist + 1))\n",
    "    return float('inf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T14:25:17.286288Z",
     "iopub.status.busy": "2025-05-15T14:25:17.285997Z",
     "iopub.status.idle": "2025-05-15T14:25:17.292187Z",
     "shell.execute_reply": "2025-05-15T14:25:17.291624Z",
     "shell.execute_reply.started": "2025-05-15T14:25:17.286263Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 4, 0), (5, 3, 0)]\n",
      "[(1, 6, 5, 4, 3, 0, 26), (2, 2, 2, 2, 3, 0, 22), (3, 4, 5, 5, 6, 0, 22)]\n",
      "({'time_step': 1, 'map': [[1, 1, 1, 1, 1, 1, 1], [1, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0, 1], [1, 1, 1, 1, 1, 1, 1]], 'robots': [(5, 3, 0), (4, 3, 0)], 'packages': [(1, 6, 5, 4, 3, 0, 26), (2, 2, 2, 2, 3, 0, 22), (3, 4, 5, 5, 6, 0, 22)]}, -0.02, False, {})\n"
     ]
    }
   ],
   "source": [
    "env = Environment('map.txt', 10, 2, 5)\n",
    "state = env.reset()\n",
    "print(state[\"robots\"])\n",
    "print(state[\"packages\"])\n",
    "# print(state[\"map\"])\n",
    "# print(env.load_map())\n",
    "print(env.step([('L', 1),('U',0)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T14:25:50.821304Z",
     "iopub.status.busy": "2025-05-15T14:25:50.820992Z",
     "iopub.status.idle": "2025-05-15T14:25:50.828833Z",
     "shell.execute_reply": "2025-05-15T14:25:50.828108Z",
     "shell.execute_reply.started": "2025-05-15T14:25:50.821281Z"
    },
    "id": "kfrZJa4jG6yE",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Avoid to modify the Env class,\n",
    "# If it is neccessary, you should describe those changes clearly in report and code\n",
    "class Env(gym.Env):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Env, self).__init__()\n",
    "        self.env = Environment(*args, **kwargs)\n",
    "\n",
    "        self.action_space = spaces.multi_discrete.MultiDiscrete([5, 3]*self.env.n_robots)\n",
    "\n",
    "\n",
    "        self.prev_state = self.env.reset()\n",
    "        first_state=convert_state(self.prev_state)\n",
    "        map_shape = first_state[0].shape\n",
    "        feature_shape = first_state[1].shape\n",
    "\n",
    "        # Define observation space as a dictionary\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"map\": spaces.Box(low=0, high=100, shape=map_shape, dtype=np.float32),\n",
    "            \"feature\": spaces.Box(low=0, high=100, shape=feature_shape, dtype=np.float32)\n",
    "        })\n",
    "\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        self.le1, self.le2= LabelEncoder(), LabelEncoder()\n",
    "        self.le1.fit(['S', 'L', 'R', 'U', 'D'])\n",
    "        self.le2.fit(['0','1', '2'])\n",
    "\n",
    "    def reset(self, *args, **kwargs):\n",
    "        self.prev_state = self.env.reset()\n",
    "        return convert_state(self.prev_state), {}\n",
    "\n",
    "    def render(self, *args, **kwargs):\n",
    "        return self.env.render()\n",
    "\n",
    "    def step(self, action):\n",
    "        ret = []\n",
    "        ret.append(self.le1.inverse_transform(action.reshape(-1, 2).T[0]))\n",
    "        ret.append(self.le2.inverse_transform(action.reshape(-1, 2).T[1]))\n",
    "        action = list(zip(*ret))\n",
    "\n",
    "        # You should not modify the infos object\n",
    "        s, r, done, infos = self.env.step(action)\n",
    "        new_r = reward_shaping(r, self.env, self.prev_state, action)\n",
    "        self.prev_state = s\n",
    "        return convert_state(s), new_r, \\\n",
    "            done, False, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[1., 1., 1., 1., 1., 1., 1.],\n",
      "       [1., 0., 0., 0., 0., 0., 1.],\n",
      "       [1., 0., 0., 0., 0., 0., 1.],\n",
      "       [1., 0., 0., 0., 0., 0., 1.],\n",
      "       [1., 0., 0., 0., 0., 0., 1.],\n",
      "       [1., 0., 0., 0., 0., 0., 1.],\n",
      "       [1., 1., 1., 1., 1., 1., 1.]], dtype=float32), array([ 5.,  4.,  0.,  5.,  3.,  0.,  2.,  2.,  2.,  3.,  0., 22.,  1.,\n",
      "        6.,  5.,  4.,  3.,  0., 26.,  1.,  4.,  5.,  5.,  6.,  0., 22.,\n",
      "        1.,  0.], dtype=float32))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m example_env = Env(\u001b[33m'\u001b[39m\u001b[33mmap.txt\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m10\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m5\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m x, y, done, infos = example_env.reset()\n",
      "\u001b[31mValueError\u001b[39m: not enough values to unpack (expected 4, got 2)"
     ]
    }
   ],
   "source": [
    "example_env = Env('map.txt', 10, 2, 5)\n",
    "x, y, done, infos = example_env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "LR = 1e-4\n",
    "GAMMA = 0.99\n",
    "CLIP_EPS = 0.2\n",
    "UPDATE_EPOCHS = 10\n",
    "VALUE_COEF = 0.5\n",
    "ENTROPY_COEF = 0.01\n",
    "MAX_TIMESTEPS = 1000\n",
    "# ==== MAPPO Hyperparameters ====\n",
    "MAX_ROBOTS = 20\n",
    "MAX_PACKAGES = 1000\n",
    "MAX_TIMESTEPS = 1000\n",
    "\n",
    "# ==== MAPPO Policy ====\n",
    "class MAPPOPolicy(nn.Module):\n",
    "    def __init__(self, state_dim, n_agents, move_action_dim=5, pkg_action_dim=3, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.n_agents = n_agents\n",
    "        self.move_action_dim = move_action_dim\n",
    "        self.pkg_action_dim = pkg_action_dim\n",
    "\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.move_heads = nn.ModuleList([nn.Linear(hidden_dim, move_action_dim) for _ in range(n_agents)])\n",
    "        self.pkg_heads = nn.ModuleList([nn.Linear(hidden_dim, pkg_action_dim) for _ in range(n_agents)])\n",
    "        self.value_head = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        # state: (batch, state_dim)\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        move_logits = [head(x) for head in self.move_heads]  # [(batch, move_action_dim)] * n_agents\n",
    "        pkg_logits = [head(x) for head in self.pkg_heads]    # [(batch, pkg_action_dim)] * n_agents\n",
    "        value = self.value_head(x)\n",
    "        move_logits = torch.stack(move_logits, dim=1)  # (batch, n_agents, move_action_dim)\n",
    "        pkg_logits = torch.stack(pkg_logits, dim=1)    # (batch, n_agents, pkg_action_dim)\n",
    "        return move_logits, pkg_logits, value\n",
    "\n",
    "# ==== MAPPO Buffer ====\n",
    "class MAPPOBuffer:\n",
    "    def __init__(self):\n",
    "        self.states, self.move_actions, self.pkg_actions = [], [], []\n",
    "        self.logprobs_move, self.logprobs_pkg = [], []\n",
    "        self.rewards, self.dones, self.values = [], [], []\n",
    "\n",
    "    def clear(self):\n",
    "        self.__init__()\n",
    "\n",
    "# ==== MAPPO Agent ====\n",
    "class MAPPOAgent:\n",
    "    def __init__(self, state_dim, n_agents, move_action_dim=5, pkg_action_dim=3):\n",
    "        self.policy = MAPPOPolicy(state_dim, n_agents, move_action_dim, pkg_action_dim).to(DEVICE)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=LR)\n",
    "        self.buffer = MAPPOBuffer()\n",
    "        self.n_agents = n_agents\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # state: (1, state_dim)\n",
    "        move_logits, pkg_logits, value = self.policy(state)\n",
    "        move_dists = [Categorical(logits=move_logits[0, i]) for i in range(self.n_agents)]\n",
    "        pkg_dists = [Categorical(logits=pkg_logits[0, i]) for i in range(self.n_agents)]\n",
    "        move_actions = [dist.sample() for dist in move_dists]\n",
    "        pkg_actions = [dist.sample() for dist in pkg_dists]\n",
    "        logprobs_move = [dist.log_prob(a) for dist, a in zip(move_dists, move_actions)]\n",
    "        logprobs_pkg = [dist.log_prob(a) for dist, a in zip(pkg_dists, pkg_actions)]\n",
    "\n",
    "        self.buffer.states.append(state)\n",
    "        self.buffer.move_actions.append(torch.stack(move_actions))\n",
    "        self.buffer.pkg_actions.append(torch.stack(pkg_actions))\n",
    "        self.buffer.logprobs_move.append(torch.stack(logprobs_move))\n",
    "        self.buffer.logprobs_pkg.append(torch.stack(logprobs_pkg))\n",
    "        self.buffer.values.append(value.squeeze())\n",
    "\n",
    "        # Return as numpy for env.step\n",
    "        actions = [(move.item(), pkg.item()) for move, pkg in zip(move_actions, pkg_actions)]\n",
    "        return np.array(actions)\n",
    "\n",
    "    def compute_returns_and_advantages(self, next_value):\n",
    "        returns, advs = [], []\n",
    "        gae = 0\n",
    "        values = self.buffer.values + [next_value]\n",
    "        for i in reversed(range(len(self.buffer.rewards))):\n",
    "            delta = self.buffer.rewards[i] + GAMMA * values[i + 1] * (1 - self.buffer.dones[i]) - values[i]\n",
    "            gae = delta + GAMMA * gae * (1 - self.buffer.dones[i])\n",
    "            advs.insert(0, gae)\n",
    "            returns.insert(0, gae + values[i])\n",
    "        return returns, advs\n",
    "\n",
    "    def update(self, next_value):\n",
    "        returns, advs = self.compute_returns_and_advantages(next_value)\n",
    "\n",
    "        states = torch.cat(self.buffer.states).to(DEVICE)\n",
    "        move_actions = torch.stack(self.buffer.move_actions).to(DEVICE)\n",
    "        pkg_actions = torch.stack(self.buffer.pkg_actions).to(DEVICE)\n",
    "        old_logprobs_move = torch.stack(self.buffer.logprobs_move).detach().to(DEVICE)\n",
    "        old_logprobs_pkg = torch.stack(self.buffer.logprobs_pkg).detach().to(DEVICE)\n",
    "        returns = torch.tensor(returns).detach().unsqueeze(1).to(DEVICE)\n",
    "        advs = torch.tensor(advs).detach().unsqueeze(1).to(DEVICE)\n",
    "\n",
    "        for _ in range(UPDATE_EPOCHS):\n",
    "            move_logits, pkg_logits, values = self.policy(states)\n",
    "            loss_actor, loss_critic, entropy = 0, 0, 0\n",
    "\n",
    "            for i in range(self.n_agents):\n",
    "                move_dist = Categorical(logits=move_logits[:, i])\n",
    "                pkg_dist = Categorical(logits=pkg_logits[:, i])\n",
    "                logprob_move = move_dist.log_prob(move_actions[:, i])\n",
    "                logprob_pkg = pkg_dist.log_prob(pkg_actions[:, i])\n",
    "                entropy += move_dist.entropy().mean() + pkg_dist.entropy().mean()\n",
    "\n",
    "                ratio_move = torch.exp(logprob_move - old_logprobs_move[:, i])\n",
    "                ratio_pkg = torch.exp(logprob_pkg - old_logprobs_pkg[:, i])\n",
    "                surr1 = (ratio_move + ratio_pkg) * advs.squeeze()\n",
    "                surr2 = torch.clamp(ratio_move + ratio_pkg, 1 - CLIP_EPS, 1 + CLIP_EPS) * advs.squeeze()\n",
    "                loss_actor += -torch.min(surr1, surr2).mean()\n",
    "\n",
    "            loss_critic = nn.MSELoss()(values, returns)\n",
    "            loss = loss_actor + VALUE_COEF * loss_critic - ENTROPY_COEF * entropy\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        self.buffer.clear()\n",
    "\n",
    "# ==== Training Loop ====\n",
    "def train_mappo(env, convert_state, n_agents, episodes=5):\n",
    "    # Get state_dim from convert_state\n",
    "    state, _ = env.reset()\n",
    "    state_tensor = convert_state(state)\n",
    "    state_dim = state_tensor.shape[1]\n",
    "    agent = MAPPOAgent(state_dim, n_agents)\n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        state_tensor = convert_state(state)\n",
    "        episode_reward = 0\n",
    "        for t in range(MAX_TIMESTEPS):\n",
    "            action = agent.select_action(state_tensor)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            agent.buffer.rewards.append(reward)\n",
    "            agent.buffer.dones.append(done)\n",
    "            state_tensor = convert_state(next_state)\n",
    "            episode_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        with torch.no_grad():\n",
    "            _, _, next_value = agent.policy(state_tensor)\n",
    "        agent.update(next_value.squeeze())\n",
    "        print(f\"Episode {episode}, Reward: {episode_reward}\")\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-15T14:26:03.000905Z",
     "iopub.status.busy": "2025-05-15T14:26:03.000419Z",
     "iopub.status.idle": "2025-05-15T14:26:27.471931Z",
     "shell.execute_reply": "2025-05-15T14:26:27.471254Z",
     "shell.execute_reply.started": "2025-05-15T14:26:03.000883Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m env = Env(\u001b[33m'\u001b[39m\u001b[33mmap2.txt\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m100\u001b[39m, \u001b[32m5\u001b[39m, \u001b[32m20\u001b[39m, -\u001b[32m0.01\u001b[39m, \u001b[32m10.\u001b[39m, \u001b[32m1.\u001b[39m, \u001b[32m10\u001b[39m)\n\u001b[32m      3\u001b[39m n_agents = env.env.n_robots  \u001b[38;5;66;03m# or set manually\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m trained_agent = \u001b[43mtrain_mappo\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_agents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Testing in another environment\u001b[39;00m\n\u001b[32m      7\u001b[39m test_env = Env(\u001b[33m'\u001b[39m\u001b[33mmap1.txt\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m100\u001b[39m, \u001b[32m5\u001b[39m, \u001b[32m20\u001b[39m, -\u001b[32m0.01\u001b[39m, \u001b[32m10.\u001b[39m, \u001b[32m1.\u001b[39m, \u001b[32m10\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 137\u001b[39m, in \u001b[36mtrain_mappo\u001b[39m\u001b[34m(env, convert_state, n_agents, episodes)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_mappo\u001b[39m(env, convert_state, n_agents, episodes=\u001b[32m5\u001b[39m):\n\u001b[32m    135\u001b[39m     \u001b[38;5;66;03m# Get state_dim from convert_state\u001b[39;00m\n\u001b[32m    136\u001b[39m     state, _ = env.reset()\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     state_tensor = \u001b[43mconvert_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m     state_dim = state_tensor.shape[\u001b[32m1\u001b[39m]\n\u001b[32m    139\u001b[39m     agent = MAPPOAgent(state_dim, n_agents)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mconvert_state\u001b[39m\u001b[34m(state, max_robots, max_packages, device)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mConvert raw state dict to input tensor dict for CNNPolicy.\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03mThe tensors are concatenated together as a single input for the model.\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Convert map to (1, 1, H, W)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m map_tensor = torch.tensor(\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmap\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, dtype=torch.float32, device=device)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Process robots (each tuple has 3 values)\u001b[39;00m\n\u001b[32m     10\u001b[39m robots = state[\u001b[33m\"\u001b[39m\u001b[33mrobots\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mIndexError\u001b[39m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "env = Env('map2.txt', 100, 5, 20, -0.01, 10., 1., 10)\n",
    "n_agents = env.env.n_robots  # or set manually\n",
    "trained_agent = train_mappo(env, convert_state, n_agents, episodes=5)\n",
    "\n",
    "# Testing in another environment\n",
    "test_env = Env('map1.txt', 100, 5, 20, -0.01, 10., 1., 10)\n",
    "state, _ = test_env.reset()\n",
    "state_tensor = convert_state(state)\n",
    "with torch.no_grad():\n",
    "    move_logits, pkg_logits, _ = trained_agent.policy(state_tensor)\n",
    "    move_actions = torch.argmax(move_logits, dim=-1)\n",
    "    pkg_actions = torch.argmax(pkg_logits, dim=-1)\n",
    "    actions = [(move.item(), pkg.item()) for move, pkg in zip(move_actions[0], pkg_actions[0])]\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T14:31:11.129029Z",
     "iopub.status.busy": "2025-05-14T14:31:11.128386Z",
     "iopub.status.idle": "2025-05-14T14:31:11.134733Z",
     "shell.execute_reply": "2025-05-14T14:31:11.133912Z",
     "shell.execute_reply.started": "2025-05-14T14:31:11.129006Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate(agent, env, episodes=10, render=False):\n",
    "    total_rewards = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for _ in range(MAX_TIMESTEPS):\n",
    "            # Không cần lưu logprob hay value\n",
    "            state_tensor = state.unsqueeze(0).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                action_probs, _ = agent.policy(state_tensor)\n",
    "            action = torch.argmax(action_probs, dim=-1).item()\n",
    "            action = np.unravel_index(action, env.action_space.nvec)\n",
    "            action = np.array(action)\n",
    "            print(action)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        total_rewards.append(episode_reward)\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    return avg_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-14T14:31:15.888079Z",
     "iopub.status.busy": "2025-05-14T14:31:15.887331Z",
     "iopub.status.idle": "2025-05-14T14:31:21.454162Z",
     "shell.execute_reply": "2025-05-14T14:31:21.453343Z",
     "shell.execute_reply.started": "2025-05-14T14:31:15.888052Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "[1 0 4 2 1 2 2 1 4 2]\n",
      "2.3760000000000088\n"
     ]
    }
   ],
   "source": [
    "eval_env = Env('map1.txt', 100, 5, 100, -0.01, 10., 1., 10)\n",
    "model.policy.eval()\n",
    "ev = evaluate(model, eval_env, render=False)\n",
    "print(ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.status.busy": "2025-05-14T14:12:10.947589Z",
     "iopub.status.idle": "2025-05-14T14:12:10.947977Z",
     "shell.execute_reply": "2025-05-14T14:12:10.947825Z",
     "shell.execute_reply.started": "2025-05-14T14:12:10.947809Z"
    },
    "id": "WVpXVAz8Kn9C",
    "outputId": "da85df6f-1219-444b-eeac-0bcf9bd6bf83",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip freeze | grep stable_baselines3"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
