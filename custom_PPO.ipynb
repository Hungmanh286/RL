{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM3r0wPe5N0K"
      },
      "source": [
        "Solving Package delivery using single-agent PPO with a naive feature representation learning: concatenante all the feature in to a single state vector, and multiple robot actions as a multi discrete distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9Ro5mHQ3GnN8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/hungmanh/home_work/RL/marl-delivery\n",
            "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mRequirements file `requirements.txt` does not contain any dependencies\u001b[0m\n",
            "\u001b[2mResolved \u001b[1m92 packages\u001b[0m \u001b[2min 6ms\u001b[0m\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m86 packages\u001b[0m \u001b[2min 0.16ms\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# %%capture\n",
        "# !git clone https://github.com/cuongtv312/marl-delivery.git\n",
        "%cd /home/hungmanh/home_work/RL/marl-delivery\n",
        "!uv add -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uWjMBXQoG4JL"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!uv add stable-baselines3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "309nvG-V8Otr"
      },
      "outputs": [],
      "source": [
        "from env import Environment\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rq1hlk4b8Q37"
      },
      "outputs": [],
      "source": [
        "N_MAX_SIZE = 20\n",
        "M_MAX_AGENTS = 20 \n",
        "P_MAX_PACKAGES = 1000 \n",
        "MAX_TIME_STEPS = 2000 \n",
        "\n",
        "def convert_state(state):\n",
        "    \"\"\"\n",
        "    Chuyển đổi trạng thái đầu vào thành một vector NumPy phẳng, cố định kích thước,\n",
        "    phù hợp để đưa vào thuật toán MAPPO (thường là cho Critic hoặc state toàn cục).\n",
        "\n",
        "    Args:\n",
        "        state (dict): Dictionary chứa thông tin trạng thái:\n",
        "            \"map\": list 2 chiều (nxn)\n",
        "            \"robots\": list các tuple (vị trí robot, trạng thái)\n",
        "            \"packages\": list các tuple (id, vị trí lấy, vị trí giao, t_xuất_hiện, deadline)\n",
        "            \"time_step\": int, bước thời gian hiện tại\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Vector trạng thái đã được làm phẳng và chuẩn hóa.\n",
        "    \"\"\"\n",
        "    ret_components = {} \n",
        "\n",
        "    current_time = state.get(\"time_step\", 0) \n",
        "\n",
        "    time_feature = np.array([current_time / MAX_TIME_STEPS], dtype=np.float32)\n",
        "    ret_components[\"time\"] = time_feature\n",
        "\n",
        "    game_map = np.array(state[\"map\"], dtype=np.float32)\n",
        "    n_map = game_map.shape[0]\n",
        "\n",
        "    padded_map = np.zeros((N_MAX_SIZE, N_MAX_SIZE), dtype=np.float32)\n",
        "    padded_map[:n_map, :n_map] = game_map\n",
        "    ret_components[\"map\"] = padded_map.flatten()\n",
        "\n",
        "    robot_features_dim = 3\n",
        "    robots_features = np.zeros(M_MAX_AGENTS * robot_features_dim, dtype=np.float32)\n",
        "    \n",
        "    pos_norm_factor = max(1, n_map - 1)\n",
        "\n",
        "    for i in range(len(state[\"robots\"])):\n",
        "        if i >= M_MAX_AGENTS:\n",
        "            break\n",
        "        robot_pos1, robot_pos2, robot_status = state[\"robots\"][i]\n",
        "        offset = i * robot_features_dim\n",
        "\n",
        "        robots_features[offset] = robot_pos1 / pos_norm_factor\n",
        "        robots_features[offset+1] = robot_pos2 / pos_norm_factor\n",
        "\n",
        "        if 0 <= robot_status <= 2: \n",
        "            robots_features[offset + 2 + int(robot_status)] = 1.0\n",
        "            \n",
        "    ret_components[\"robots\"] = robots_features\n",
        "\n",
        "    package_features_dim = 7\n",
        "    packages_features = np.zeros(P_MAX_PACKAGES * package_features_dim, dtype=np.float32)\n",
        "\n",
        "    sorted_packages = sorted(state[\"packages\"], key=lambda p: (p[3], p[0]))\n",
        "\n",
        "    for i in range(len(sorted_packages)):\n",
        "        if i >= P_MAX_PACKAGES: \n",
        "            break\n",
        "        \n",
        "        pkg_id, pickup_loc1, pickup_loc2, dropoff_loc1, dropoff_loc2, appear_time, deadline_time = sorted_packages[i]\n",
        "        offset = i * package_features_dim\n",
        "\n",
        "\n",
        "        packages_features[offset] = pickup_loc1 / pos_norm_factor\n",
        "        packages_features[offset+1] = pickup_loc2 / pos_norm_factor\n",
        "\n",
        "        packages_features[offset+2] = dropoff_loc1 / pos_norm_factor\n",
        "        packages_features[offset+3] = dropoff_loc2 / pos_norm_factor\n",
        "\n",
        "        packages_features[offset+4] = appear_time / MAX_TIME_STEPS\n",
        "        packages_features[offset+5] = deadline_time / MAX_TIME_STEPS\n",
        "        \n",
        "\n",
        "        is_active = 1.0 if (current_time >= appear_time and current_time < deadline_time) else 0.0\n",
        "        packages_features[offset+6] = is_active\n",
        "        \n",
        "    ret_components[\"packages\"] = packages_features\n",
        "    \n",
        "    final_vector = np.concatenate(\n",
        "        [ret_components[\"time\"], ret_components[\"map\"], ret_components[\"robots\"], ret_components[\"packages\"]]\n",
        "    )\n",
        "    return final_vector.astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(6, 5, 0), (5, 4, 0), (4, 4, 0), (2, 5, 0), (3, 5, 0)]\n",
            "[(1, 4, 6, 6, 2, 0, 23), (2, 4, 3, 2, 4, 0, 29), (3, 3, 3, 5, 2, 0, 25), (4, 6, 3, 4, 3, 0, 22), (5, 4, 5, 2, 5, 0, 17), (6, 2, 6, 4, 5, 0, 22)]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([0., 1., 1., ..., 0., 0., 0.], shape=(7461,), dtype=float32)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env = Environment('map.txt', 100, 5, 100)\n",
        "state = env.reset()\n",
        "print(state[\"robots\"])\n",
        "print(state[\"packages\"])\n",
        "convert_state(state)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7SHRHHeF8SjO"
      },
      "outputs": [],
      "source": [
        "\n",
        "from collections import deque \n",
        "def reward_shaping(original_reward, env: Environment, prev_state, actions):\n",
        "    shaped_reward = original_reward\n",
        "    shaping_factor = 0.5\n",
        "\n",
        "    for i, robot in enumerate(env.robots):\n",
        "        prev_pos = tuple(prev_state['robots'][i][:2])\n",
        "        curr_pos = robot.position\n",
        "        grid = env.load_map()  # Giả sử đây là 2D list (0: trống, 1: tường)\n",
        "\n",
        "        if robot.carrying:\n",
        "            pkg = env.packages[robot.carrying - 1]\n",
        "            d_prev = bfs_distance(prev_pos, pkg.target, grid)\n",
        "            d_curr = bfs_distance(curr_pos, pkg.target, grid)\n",
        "            if d_curr < d_prev:\n",
        "                shaped_reward += shaping_factor\n",
        "            else:\n",
        "                shaped_reward -= 0.2\n",
        "        else:\n",
        "            for pkg in env.packages:\n",
        "                if pkg.status == 'waiting':\n",
        "                    d_prev = bfs_distance(prev_pos, pkg.start, grid)\n",
        "                    d_curr = bfs_distance(curr_pos, pkg.start, grid)\n",
        "                    if d_curr < d_prev:\n",
        "                        shaped_reward += shaping_factor * 0.5\n",
        "                    break\n",
        "\n",
        "        if curr_pos == prev_pos:\n",
        "            shaped_reward -= 0.1\n",
        "\n",
        "        for pkg in env.packages:\n",
        "            if pkg.status == 'delivering' and pkg.picked_by == i:\n",
        "                waiting_time = pkg.pick_time - pkg.start_time if pkg.pick_time else 0\n",
        "                shaped_reward += max(0, 1.0 - 0.01 * waiting_time)\n",
        "\n",
        "    return shaped_reward\n",
        "def bfs_distance(start, goal, grid):\n",
        "    rows, cols = len(grid), len(grid[0])\n",
        "    visited = set()\n",
        "    queue = deque([(start, 0)])\n",
        "\n",
        "    while queue:\n",
        "        (x, y), dist = queue.popleft()\n",
        "        if (x, y) == goal:\n",
        "            return dist\n",
        "\n",
        "        for dx, dy in [(-1,0), (1,0), (0,-1), (0,1)]:\n",
        "            nx, ny = x + dx, y + dy\n",
        "            if 0 <= nx < rows and 0 <= ny < cols:\n",
        "                if grid[nx][ny] == 0 and (nx, ny) not in visited:  # 0 là ô trống\n",
        "                    visited.add((nx, ny))\n",
        "                    queue.append(((nx, ny), dist + 1))\n",
        "    \n",
        "    return float('inf')  # Không tìm được đường đi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kfrZJa4jG6yE"
      },
      "outputs": [],
      "source": [
        "# Avoid to modify the Env class,\n",
        "# If it is neccessary, you should describe those changes clearly in report and code\n",
        "class Env(gym.Env):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(Env, self).__init__()\n",
        "        self.env = Environment(*args, **kwargs)\n",
        "\n",
        "        self.action_space = spaces.multi_discrete.MultiDiscrete([5, 3]*self.env.n_robots)\n",
        "\n",
        "\n",
        "        self.prev_state = self.env.reset()\n",
        "        first_state=convert_state(self.prev_state)\n",
        "\n",
        "        # Define observation space as a dictionary\n",
        "\n",
        "        self.observation_space = spaces.Box(low=0, high=100, shape=first_state.shape, dtype=np.float32)\n",
        "\n",
        "\n",
        "        from sklearn.preprocessing import LabelEncoder\n",
        "        self.le1, self.le2= LabelEncoder(), LabelEncoder()\n",
        "        self.le1.fit(['S', 'L', 'R', 'U', 'D'])\n",
        "        self.le2.fit(['0','1', '2'])\n",
        "\n",
        "    def reset(self, *args, **kwargs):\n",
        "        self.prev_state = self.env.reset()\n",
        "        return convert_state(self.prev_state), {}\n",
        "\n",
        "    def render(self, *args, **kwargs):\n",
        "        return self.env.render()\n",
        "\n",
        "    def step(self, action):\n",
        "        ret = []\n",
        "        ret.append(self.le1.inverse_transform(action.reshape(-1, 2).T[0]))\n",
        "        ret.append(self.le2.inverse_transform(action.reshape(-1, 2).T[1]))\n",
        "        action = list(zip(*ret))\n",
        "\n",
        "        # You should not modify the infos object\n",
        "        s, r, done, infos = self.env.step(action)\n",
        "        new_r = reward_shaping(r, self.env, self.prev_state, action)\n",
        "        self.prev_state = s\n",
        "        return convert_state(s), new_r, \\\n",
        "            done, False, infos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2IQRlqoKl78",
        "outputId": "fc847fe0-d855-43d0-cfe4-159cb2fc43e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 100      |\n",
            "|    ep_rew_mean     | 66.7     |\n",
            "| time/              |          |\n",
            "|    fps             | 789      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 25       |\n",
            "|    total_timesteps | 20480    |\n",
            "---------------------------------\n"
          ]
        }
      ],
      "source": [
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "\n",
        "# Parallel environments\n",
        "\n",
        "vec_env = make_vec_env(lambda: Env('map2.txt', 100, 5, 20, -0.01, 10., 1., 10), n_envs=10)\n",
        "eval_env = Monitor(Env('map2.txt', 100, 5, 20, -0.01, 10., 1., 10), \"ppo_delivery\")\n",
        "\n",
        "eval_callback = EvalCallback(eval_env, best_model_save_path=\"best_model/\",\n",
        "                             log_path=\"logs/\", eval_freq=5000,\n",
        "                             deterministic=True, render=False)\n",
        "\n",
        "model = PPO(\"MlpPolicy\", vec_env, verbose=1)\n",
        "model.learn(total_timesteps=10000, callback=eval_callback)\n",
        "model.save(\"ppo_delivery\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "import numpy as np\n",
        "\n",
        "GAMMA = 0.99\n",
        "CLIP_EPS = 0.2\n",
        "LR = 3e-4\n",
        "UPDATE_EPOCHS = 4\n",
        "BATCH_SIZE = 64\n",
        "MAX_TIMESTEPS = 1000\n",
        "\n",
        "# Define CNN Policy\n",
        "\n",
        "class CNNPolicy(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim):\n",
        "        super(CNNPolicy, self).__init__()\n",
        "\n",
        "        # Adjusted for 1D convolution (since height is 1)\n",
        "        self.conv1 = nn.Conv1d(1, 32, kernel_size=3, stride=1, padding=1)  # 1D convolution\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)  # 1D convolution\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)  # Max pooling for 1D data\n",
        "\n",
        "        # Calculate the size after convolution and pooling for 1D\n",
        "        self.fc1 = nn.Linear(64 * (obs_dim[1] // 2), 256)  # Adjusted to reflect the reduced size\n",
        "        self.fc2 = nn.Linear(256, act_dim)\n",
        "        self.value_head = nn.Linear(256, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        # state is assumed to be in the shape (batch_size, 1, height, width)\n",
        "        # Squeeze out the height dimension (which is 1) to make it (batch_size, 1, width)\n",
        "        state = state.squeeze(2)  # This will remove the height dimension (1) from shape [batch_size, 1, 1, width] to [batch_size, 1, width]\n",
        "\n",
        "        # Now state is of shape (batch_size, 1, width)\n",
        "        x = torch.relu(self.conv1(state))  # Conv1d operation\n",
        "        x = torch.relu(self.conv2(x))  # Conv1d operation\n",
        "        x = self.pool(x)  # Apply max pooling to reduce size\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor for the fully connected layer\n",
        "        x = torch.relu(self.fc1(x))\n",
        "\n",
        "        # Action probabilities (policy)\n",
        "        action_probs = torch.softmax(self.fc2(x), dim=-1)\n",
        "\n",
        "        # Value estimation\n",
        "        state_value = self.value_head(x)\n",
        "\n",
        "        return action_probs, state_value\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Rollout Buffer to store PPO rollouts\n",
        "class RolloutBuffer:\n",
        "    def __init__(self):\n",
        "        self.states, self.actions, self.logprobs = [], [], []\n",
        "        self.rewards, self.dones, self.values = [], [], []\n",
        "\n",
        "    def clear(self):\n",
        "        self.__init__()\n",
        "\n",
        "# PPO Agent that uses the CNNPolicy\n",
        "class PPOAgent:\n",
        "    def __init__(self, obs_dim, act_dim):\n",
        "        self.policy = CNNPolicy(obs_dim, act_dim)\n",
        "        self.optimizer = optim.Adam(self.policy.parameters(), lr=LR)\n",
        "        self.buffer = RolloutBuffer()\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions (1, 1, H, W)\n",
        "        probs, value = self.policy(state)\n",
        "        dist = Categorical(probs)\n",
        "        action = dist.sample()\n",
        "        self.buffer.states.append(state)\n",
        "        self.buffer.actions.append(action)\n",
        "        self.buffer.logprobs.append(dist.log_prob(action))\n",
        "        self.buffer.values.append(value)\n",
        "        return action.item()\n",
        "\n",
        "    def compute_returns_and_advantages(self, next_value):\n",
        "        returns, advs = [], []\n",
        "        gae = 0\n",
        "        values = self.buffer.values + [next_value]\n",
        "        for i in reversed(range(len(self.buffer.rewards))):\n",
        "            delta = self.buffer.rewards[i] + GAMMA * values[i+1] * (1 - self.buffer.dones[i]) - values[i]\n",
        "            gae = delta + GAMMA * gae * (1 - self.buffer.dones[i])\n",
        "            advs.insert(0, gae)\n",
        "            returns.insert(0, gae + values[i])\n",
        "        return returns, advs\n",
        "\n",
        "    def update(self, next_value):\n",
        "        returns, advs = self.compute_returns_and_advantages(next_value)\n",
        "\n",
        "        states = torch.cat(self.buffer.states)\n",
        "        actions = torch.tensor(self.buffer.actions)\n",
        "        old_logprobs = torch.stack(self.buffer.logprobs).detach()\n",
        "        returns = torch.tensor(returns).detach().unsqueeze(1)\n",
        "        advs = torch.tensor(advs).detach().unsqueeze(1)\n",
        "\n",
        "        for _ in range(UPDATE_EPOCHS):\n",
        "            probs, values = self.policy(states)\n",
        "            dist = Categorical(probs)\n",
        "            logprobs = dist.log_prob(actions)\n",
        "            ratio = torch.exp(logprobs - old_logprobs)\n",
        "            \n",
        "            surr1 = ratio * advs\n",
        "            surr2 = torch.clamp(ratio, 1 - CLIP_EPS, 1 + CLIP_EPS) * advs\n",
        "            actor_loss = -torch.min(surr1, surr2).mean()\n",
        "            critic_loss = nn.MSELoss()(values, returns)\n",
        "\n",
        "            loss = actor_loss + 0.5 * critic_loss\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        self.buffer.clear()\n",
        "\n",
        "# Training Loop\n",
        "def train(env):\n",
        "    obs_dim = env.observation_space.shape  # (Height, Width)\n",
        "    act_dim = int(np.prod(env.action_space.nvec))  # Flatten MultiDiscrete\n",
        "\n",
        "    agent = PPOAgent(obs_dim, act_dim)\n",
        "\n",
        "    for episode in range(1000):\n",
        "        state, _ = env.reset()\n",
        "        episode_reward = 0\n",
        "\n",
        "        for t in range(MAX_TIMESTEPS):\n",
        "            action_flat = agent.select_action(state)\n",
        "\n",
        "            # Convert flat action back to multi-discrete\n",
        "            action = np.unravel_index(action_flat, env.action_space.nvec)\n",
        "            action = np.array(action)\n",
        "\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "            agent.buffer.rewards.append(reward)\n",
        "            agent.buffer.dones.append(done)\n",
        "\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0).unsqueeze(0)\n",
        "            _, next_value = agent.policy(next_state_tensor)\n",
        "        agent.update(next_value)\n",
        "        print(f\"Episode {episode}, Reward: {episode_reward}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "571\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m obs_dim = env.observation_space.shape\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(obs_dim[\u001b[32m1\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 150\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(env)\u001b[39m\n\u001b[32m    148\u001b[39m     next_state_tensor = torch.FloatTensor(next_state).unsqueeze(\u001b[32m0\u001b[39m).unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m    149\u001b[39m     _, next_value = agent.policy(next_state_tensor)\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 101\u001b[39m, in \u001b[36mPPOAgent.update\u001b[39m\u001b[34m(self, next_value)\u001b[39m\n\u001b[32m     98\u001b[39m advs = torch.tensor(advs).detach().unsqueeze(\u001b[32m1\u001b[39m)\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(UPDATE_EPOCHS):\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     probs, values = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m     dist = Categorical(probs)\n\u001b[32m    103\u001b[39m     logprobs = dist.log_prob(actions)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/home_work/RL/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/home_work/RL/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mCNNPolicy.forward\u001b[39m\u001b[34m(self, state)\u001b[39m\n\u001b[32m     40\u001b[39m x = torch.relu(\u001b[38;5;28mself\u001b[39m.fc1(x))\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Action probabilities (policy)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m action_probs = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Value estimation\u001b[39;00m\n\u001b[32m     46\u001b[39m state_value = \u001b[38;5;28mself\u001b[39m.value_head(x)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "env = Env('map2.txt', 100, 5, 20, -0.01, 10., 1., 10)\n",
        "obs_dim = env.observation_space.shape\n",
        "print(obs_dim[1])\n",
        "train(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuDtccMk3bXJ",
        "outputId": "3c32669b-9358-49ad-c715-c2ae4e84463c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'total_reward': -3.1199999999999988, 'total_time_steps': 100, 'episode': {'r': 42.03, 'l': 100, 't': 64.221242}}\n"
          ]
        }
      ],
      "source": [
        "obs,_ = eval_env.reset()\n",
        "while True:\n",
        "    action, _states = model.predict(obs)\n",
        "    obs, rewards, dones, _, info = eval_env.step(action)\n",
        "    # print('='*10)\n",
        "    # eval_env.unwrapped.env.render()\n",
        "    if dones:\n",
        "        break\n",
        "\n",
        "print(info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVpXVAz8Kn9C",
        "outputId": "da85df6f-1219-444b-eeac-0bcf9bd6bf83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "stable_baselines3==2.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip freeze | grep stable_baselines3"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
